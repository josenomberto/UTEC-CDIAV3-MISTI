{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TIecD0AZz4e0",
        "EV2Fhjg294h5",
        "D2_kM6YS92yl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day3_intro_to_feature_engineering_images_student_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSpDoDOj79I3"
      },
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Feature Engineering with PyTorch and MNIST Images\n",
        "\n",
        "In this exercise, we will explore image data using MNIST. [MNIST](http://yann.lecun.com/exdb/mnist/) is a data set of handwritten digits."
      ],
      "metadata": {
        "id": "w_B7z7DbrE23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary Python modules for various tasks\n",
        "\n",
        "# --- Data Access & File Handling ---\n",
        "from google.colab import drive  # Access Google Drive for file storage\n",
        "import io  # Handle I/O operations (e.g., reading files)\n",
        "import os  # Interact with the operating system (e.g., file paths)\n",
        "\n",
        "# --- Data Visualization ---\n",
        "from matplotlib import pyplot as plt  # Create plots and visualizations\n",
        "\n",
        "# --- Data Manipulation ---\n",
        "import pandas as pd  # Data analysis and manipulation (DataFrames)\n",
        "import numpy as np  # Numerical computations and array manipulations\n",
        "\n",
        "# --- Image Processing ---\n",
        "from scipy.ndimage import rotate  # Rotate images (used in image augmentation)\n",
        "\n",
        "# --- Machine Learning Models ---\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
        "from sklearn.decomposition import PCA  # Principal Component Analysis (PCA) for dimensionality reduction\n",
        "from sklearn.preprocessing import StandardScaler  # Standardize data before training\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n",
        "\n",
        "# --- Parallel Processing ---\n",
        "import multiprocessing  # Manage processes for parallel computing\n",
        "from joblib import delayed, Parallel  # Simplified parallel processing\n",
        "\n",
        "# --- Plotting Configuration ---\n",
        "import matplotlib.pyplot as plt  # Import for additional plotting capabilities\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Performance Measurement ---\n",
        "import time  # Measure execution time for performance benchmarking\n",
        "\n",
        "# --- Mathematical Functions ---\n",
        "import random  # Generate random numbers\n",
        "\n",
        "# --- Error Metrics ---\n",
        "from sklearn.metrics import accuracy_score  # Calculate model accuracy\n",
        "\n",
        "# --- Deep Learning (PyTorch) ---\n",
        "import torch  # PyTorch framework for building and training neural networks\n",
        "import torchvision  # Utility functions for computer vision tasks\n",
        "from torchvision import datasets, transforms  # Image datasets and transformations\n",
        "from torch.utils.data import DataLoader  # Load data efficiently in batches\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "AiZbCV7Fn9am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Explore Data"
      ],
      "metadata": {
        "id": "DT4VE6tqzin3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNDJz2Q9BZbv",
        "outputId": "1500a521-f11c-4c2b-a57b-25e53bacc1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xiF493PA5Aa",
        "outputId": "b1c7419d-e812-417e-f9ee-38949e277fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Training data shape: (60000, 28, 28)\n",
            "Test data shape: (10000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01lDZ6V1BQ0s",
        "outputId": "50c0cb3a-5a30-4646-c1b4-d2b691e3b976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transformation to load the dataset as tensors\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the training set\n",
        "dataset_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Download and load the test set\n",
        "dataset_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(f\"Training set size: {len(dataset_train)}\")\n",
        "print(f\"Test set size: {len(dataset_test)}\")"
      ],
      "metadata": {
        "id": "-X-JQyXdxyMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Explore the MNIST Data\n",
        "\n",
        "In this exercise, we'll explore the MNIST dataset by examining individual images.\n",
        "\n",
        "Tasks:\n",
        "1. Randomly generate images. To do this, write a line of code that generates a random integer between 0 and the length of the dataset - 1 and assign it to the variable `random_idx`. Then generate 5 sample images.\n",
        "2. Extract the labels from the training and testing datasets using `dataset.targets.to_numpy()` and plot a histogram of the labels to look at the distribution of classes.\n",
        "3. In order to feed an image into a feedforward neural network model, we must unwrap it in a vector form. Visualize an unwrapped image example. Why might this unwrapping process remove valuable information? What valuable information might be removed during this unwrapping process?"
      ],
      "metadata": {
        "id": "djM_f6aV63q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Explore the MNIST Dataset"
      ],
      "metadata": {
        "id": "TIecD0AZz4e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Pick a random index\n",
        "random_idx = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "# Extract the image and label\n",
        "image, label = dataset_train[random_idx]\n",
        "\n",
        "# Convert the image tensor to a NumPy array for visualization\n",
        "image_np = image.squeeze().numpy()  # Remove channel dimension for grayscale image\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(image_np, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "njHigUu3tyun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 SOLUTION\n",
        "\n",
        "# Pick a random index\n",
        "random_idx = random.randint(0, len(dataset_train) - 1)\n",
        "\n",
        "# Extract the image and label\n",
        "image, label = dataset_train[random_idx]\n",
        "\n",
        "# Convert the image tensor to a NumPy array for visualization\n",
        "image_np = image.squeeze().numpy()  # Remove channel dimension for grayscale image\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(image_np, cmap='gray')\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dxq7BAItzIEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Look at the Dataset Labels"
      ],
      "metadata": {
        "id": "cmZEmuhODHa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Extract labels\n",
        "labels_train = <REPLACE ME AND MY ARROWS>  # Convert tensor to numpy array for plotting\n",
        "labels_test = <REPLACE ME AND MY ARROWS> # Convert tensor to numpy array for plotting\n",
        "\n",
        "# Plot the histogram of labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(labels_train, bins=10, edgecolor='black', alpha=0.75, label = 'train')\n",
        "plt.hist(labels_test, bins=10, edgecolor='black', alpha=0.75, label = 'test')\n",
        "plt.title('Distribution of Labels in the MNIST Training Set')\n",
        "plt.xlabel('Digit Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(10))\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tBFBQRjADHjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3: Look at an Unwrapped Image\n",
        "\n",
        "- In order to feed an image into a feedforward neural network model, we must unwrap it in a vector form. Visualize an unwrapped image example. Why might this unwrapping process remove valuable information? What valuable information might be removed during this unwrapping process?"
      ],
      "metadata": {
        "id": "SIGbHbd58lr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Unwrap an image\n",
        "data_unwrapped = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "# Plot the pixel values of the first unwrapped image (row 0) in the training dataset\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(data_unwrapped, 'bo', markersize=5, alpha = .5)\n",
        "plt.title('Pixel Values for One Unwrapped Image')\n",
        "plt.ylabel('Pixel Value')\n",
        "plt.xlabel('Unwrapped Pixel Location')\n",
        "plt.grid(True)\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XU7mraDw8lyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoaders\n",
        "\n",
        "This code creates a DataLoader to load the MNIST dataset in batches of 5, converting images to tensors and shuffling the data for training. It automatically downloads the dataset if not already present."
      ],
      "metadata": {
        "id": "AkabXMT34m91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('sample_data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor()\n",
        "                             ])),\n",
        "  batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "ZiRpsWn6kwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE: Explore a DataLoader\n",
        "\n",
        "In this exercise, we'll explore how to use PyTorch's DataLoader class to efficiently load and process batches of image data. The DataLoader is a crucial tool for handling large datasets and implementing efficient training pipelines.\n",
        "\n",
        "Tasks:\n",
        "1. Run the following code and take note of the structure of a PyTorch dataloader."
      ],
      "metadata": {
        "id": "AOpMGrrx6qRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 1: Explore a DataLoader"
      ],
      "metadata": {
        "id": "CPgY0469zz-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE AND SOLUTION\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for batch_idx, (images, labels) in enumerate(data_loader):\n",
        "\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"Images shape: {images.shape}\")  # Shape: (batch_size, channels, height, width)\n",
        "    print(f\"Labels: {labels}\")  # Labels for the batch\n",
        "\n",
        "    # Break after first batch for demonstration\n",
        "    batch_size = images.shape[0]\n",
        "\n",
        "    fig, axes = plt.subplots(1, batch_size, figsize=(15, 5))\n",
        "    for i in range(batch_size):\n",
        "        img = images[i].squeeze().numpy()\n",
        "        axes[i].imshow(img, cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'Label: {labels[i].item()}')\n",
        "    plt.show()\n",
        "\n",
        "    if batch_idx == 4:\n",
        "      break"
      ],
      "metadata": {
        "id": "oHGE2pG04y6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardize Data"
      ],
      "metadata": {
        "id": "oeKQQwg_3GYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Calculate the Mean and Standard Deviation of the MNIST Dataset Pixels\n",
        "\n",
        "Tasks:\n",
        "1. Calculate the mean and standard deviation of the images in the MNIST dataset using the `torch.Tensor()` functions `.mean()` and `.std()` which are commonly used in PyTorch to compute the mean and standard deviation of a tensor.\n",
        "2. Use the calculated mean and standard deviation values to standardize your images using the PyTorch function `torchvision.transforms.Normalize(~)`.\n",
        "\n",
        "  In `torchvision.transforms.Normalize()`, the term \"normalize\" actually refers to **standardization**, where pixel values are adjusted to have a **mean of 0** and **standard deviation of 1** using the formula $$x' = \\frac{x - \\mu}{\\sigma}. $$Although technically a standardization process, it's called \"normalize\" in PyTorch due to historical usage in deep learning frameworks."
      ],
      "metadata": {
        "id": "3_sKf-FJHvx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Calculate the Mean and Standard Deviation of the MNIST Dataset"
      ],
      "metadata": {
        "id": "3difU8PBzLpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Use DataLoader for batch processing\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize variables to calculate mean and std\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# Loop through the dataset\n",
        "for images, _ in loader:\n",
        "    # images shape: (batch_size, channels, height, width)\n",
        "    batch_samples = images.size(0)  # Number of samples in the batch\n",
        "    images = images.view(batch_samples, images.size(1), -1)  # Flatten height and width\n",
        "    mean += <REPLACE ME AND MY ARROWS> # Mean across height and width, then sum batch\n",
        "    std += <REPLACE ME AND MY ARROWS> # Std across height and width, then sum batch\n",
        "    total_samples += <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "# Final mean and std\n",
        "mean /= total_samples\n",
        "std /= total_samples\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")"
      ],
      "metadata": {
        "id": "A7Jqbs5XzLyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Standardize the MNIST Images"
      ],
      "metadata": {
        "id": "R2HGCt-WIFEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Updated DataLoader with normalization\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\n",
        "        'sample_data/',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),  # Converts to tensor and scales [0, 255] to [0, 1]\n",
        "            <REPLACE ME AND MY BRACKETS>  # Standardize to mean=mean, std=std\n",
        "        ])\n",
        "    ),\n",
        "    batch_size=1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define visualization function with color bars for original and normalized images\n",
        "def visualize_normalization_with_colorbar(data_loader, num_images=5):\n",
        "\n",
        "    dataset = data_loader.dataset\n",
        "\n",
        "    fig, axs = plt.subplots(2, num_images, figsize=(15, 6))\n",
        "    fig.suptitle(\"Original vs. Normalized Images with Color Bar\", fontsize=16)\n",
        "\n",
        "    for i in range(num_images):\n",
        "\n",
        "        # Original image (non-normalized)\n",
        "        original_image = dataset.data[i].numpy()\n",
        "        im1 = axs[0, i].imshow(original_image, cmap='gray', vmin=0, vmax=255)\n",
        "        axs[0, i].axis('off')\n",
        "        axs[0, i].set_title(\"Original\")\n",
        "\n",
        "        # Normalized image\n",
        "        pil_image = transforms.ToPILImage()(dataset.data[i].unsqueeze(0))  # Convert to PIL.Image\n",
        "        normalized_image = dataset.transform(pil_image).squeeze().numpy()\n",
        "        im2 = axs[1, i].imshow(normalized_image, cmap='gray', vmin=-1, vmax=1)\n",
        "        axs[1, i].axis('off')\n",
        "        axs[1, i].set_title(\"Normalized\")\n",
        "\n",
        "    # Add color bars\n",
        "    cbar_ax1 = fig.add_axes([0.92, 0.55, 0.02, 0.35])  # Position for the original images' color bar\n",
        "    fig.colorbar(im1, cax=cbar_ax1, orientation='vertical', label='Pixel Intensity (0-255)')\n",
        "\n",
        "    cbar_ax2 = fig.add_axes([0.92, 0.15, 0.02, 0.35])  # Position for the normalized images' color bar\n",
        "    fig.colorbar(im2, cax=cbar_ax2, orientation='vertical', label='Normalized Value (-1 to 1)')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout to fit color bars\n",
        "    plt.show()\n",
        "\n",
        "# Visualize normalized images with color bars\n",
        "visualize_normalization_with_colorbar(data_loader)"
      ],
      "metadata": {
        "id": "YUtS97syIufz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "faXPq2UBN5_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section extracts the MNIST dataset from the `data_loader`, converts it to a NumPy array, and displays the shape of the data matrix. It then selects a specific image from the dataset and defines a function, `plot_image()`, to visualize the image in grayscale using Matplotlib."
      ],
      "metadata": {
        "id": "pef4bFFy9fYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_loader.dataset.data\n",
        "data = np.array(data)\n",
        "print('Shape of our MNIST Data Matrix: ' + str(data.shape))\n",
        "\n",
        "image_index = 10\n",
        "image_to_manipulate = data[image_index, :, :]\n",
        "\n",
        "def plot_image(data):\n",
        "\n",
        "  # Input = Index of the Image You Want to\n",
        "  plt.figure()\n",
        "  plt.imshow(data, cmap='gray')\n",
        "  plt.title('Example Image from MNIST')\n",
        "  plt.xlabel('Pixel')\n",
        "  plt.ylabel('Pixel')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ff2pCmtYeRMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE: Image Rotation and Flipping\n",
        "\n",
        "Tasks:\n",
        "Manipulate an image selected above with the following recommended functions:\n",
        "1. `np.fliplr()`\n",
        "2. `np.flipud()`\n",
        "3. `rotate()` from `scipy.ndimage`\n",
        "    - 80 degree rotation\n",
        "    - 120 degree rotation"
      ],
      "metadata": {
        "id": "e3mqo1bLosyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 1: `np.fliplr()`"
      ],
      "metadata": {
        "id": "yKa-aNdX9oiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Mirror Image with np.fliplr()\n",
        "mirror = <REPLACE ME AND MY BRACKETS>\n",
        "plot_image(mirror)"
      ],
      "metadata": {
        "id": "c03T-SeO9oqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 2: `np.flipud()`"
      ],
      "metadata": {
        "id": "EV2Fhjg294h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Flip Image with np.flipud()\n",
        "updown = <REPLACE ME AND MY BRACKET>\n",
        "plot_image(updown)"
      ],
      "metadata": {
        "id": "NqCpNWfV94p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 SOLUTION\n",
        "\n",
        "# Flip Image with np.flipud()\n",
        "updown = np.flipud(image_to_manipulate)\n",
        "plot_image(updown)"
      ],
      "metadata": {
        "id": "0Z_iW14CpPFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 3: `rotate()`"
      ],
      "metadata": {
        "id": "D2_kM6YS92yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# rotate image by 80' with rotate() from scipy.ndimage\n",
        "rot_80 = <REPLACE ME AND MY BRACKETS>\n",
        "plot_image(rot_80)\n",
        "\n",
        "# rotate image by 120' with rotate() from scipy.ndimage\n",
        "rot_120 = <REPLACE ME AND MY BRACKETS>\n",
        "plot_image(rot_120)"
      ],
      "metadata": {
        "id": "nG2qJ1tS929Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 SOLUTION\n",
        "\n",
        "# rotate image by 80' with rotate() from scipy.ndimage\n",
        "rot_80 = rotate(image_to_manipulate, 80)\n",
        "plot_image(rot_80)\n",
        "\n",
        "# rotate image by 120' with rotate() from scipy.ndimage\n",
        "rot_120 = rotate(image_to_manipulate, 120)\n",
        "plot_image(rot_120)"
      ],
      "metadata": {
        "id": "rfD5L1Xrpnrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTIONAL: Training a Neural Network with Data Augmentation"
      ],
      "metadata": {
        "id": "FbippVv5FYTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section introduces the implementation of a **simple feedforward neural network** using PyTorch. It consists of three key components:\n",
        "\n",
        "1. **Defining the Neural Network** – The `SimpleNN` class inherits from `nn.Module` and outlines a straightforward architecture with an input layer, one hidden layer with ReLU activation, and an output layer. This structure is suitable for tasks like image classification, where the input images are flattened before being passed through the network.\n",
        "\n",
        "2. **Training the Model** – The `train_model` function handles the training process, iterating over the provided dataset for a specified number of epochs. It uses backpropagation to calculate gradients and updates the model's weights using an optimizer. The function also tracks and prints the average loss per epoch to monitor the model’s learning progress.\n",
        "\n",
        "3. **Testing the Model** – The `test_model` function evaluates the trained model's performance on a separate test dataset. It calculates the overall accuracy by comparing the model’s predictions to the actual labels, providing a measure of how well the model generalizes to unseen data.\n",
        "\n",
        "This comprehensive setup lays the foundation for building, training, and testing a neural network, forming a core part of many deep learning workflows."
      ],
      "metadata": {
        "id": "ry3EjmWwJKnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer (784) to hidden layer (128)\n",
        "        self.relu = nn.ReLU()               # Activation function\n",
        "        self.fc2 = nn.Linear(128, 10)       # Hidden layer (128) to output layer (10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Zero out gradients\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "# Define the testing function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "zU01lfvSFgg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Train a Neural Network\n",
        "\n",
        "Tasks:\n",
        "1. **Create a Basic Data Transformation Pipeline**\n",
        "  - Create a variable called `transform_basic` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation.  \n",
        "  - This transformation converts images to tensors and scales pixel values from the range `[0, 255]` to `[0, 1]`.\n",
        "2. **Load the MNIST Training and Testing Data**\n",
        "  - Use the `torchvision.datasets.MNIST()` function to download the training and testing datasets.  \n",
        "  - Apply the `transform_basic` pipeline to both datasets.  \n",
        "  - Name the variables `dataset_train` for the training data and `dataset_test` for the testing data.\n",
        "3. **Create DataLoaders for Training and Testing**\n",
        "  - Use the `DataLoader()` function to create `train_loader` and `test_loader` variables.  \n",
        "  - Set `batch_size=64` for both loaders.  \n",
        "  - Use `shuffle=True` for the training loader to randomize the order of images.  \n",
        "  - Use `shuffle=False` for the testing loader to maintain consistent evaluation.\n",
        "4. **Train and Test the Neural Network**\n",
        "  - Run the provided code which goes through the following steps:\n",
        "    - Initialize the model using the `SimpleNN()` class.  \n",
        "    - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "    - Train the model for 5 epochs using the `train_model()` function.  \n",
        "    - Test the model using the `test_model()` function and print the accuracy."
      ],
      "metadata": {
        "id": "WtZRM_pVMoTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 1: Create a basic data transformation pipeline\n",
        "\n",
        "- Create a variable called `transform_basic` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation.  \n",
        "- This transformation converts images to tensors and scales pixel values from the range `[0, 255]` to `[0, 1]`."
      ],
      "metadata": {
        "id": "aH0w44bgP0Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Load dataset without augmentation\n",
        "transform_basic = <REPLACE ME AND MY ARROWS>  # Converts to tensor and scales [0, 255] to [0, 1]"
      ],
      "metadata": {
        "id": "ZHbPIYumUMZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 2: Extract your Training and Testing Data\n",
        "\n",
        "- Use the `torchvision.datasets.MNIST()` function to download the training and testing datasets.  \n",
        "- Apply the `transform_basic` pipeline to both datasets.  \n",
        "- Name the variables `dataset_train` for the training data and `dataset_test` for the testing data."
      ],
      "metadata": {
        "id": "fKlZS0LuUMmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Define Training and Testing Datasets\n",
        "dataset_train = <REPLACE ME AND MY ARROWS>\n",
        "dataset_test = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "wtSrurZXUMvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 3: Create your DataLoaders\n",
        "\n",
        "- Use the `DataLoader()` function to create `train_loader` and `test_loader` variables.  \n",
        "- Set `batch_size=64` for both loaders.  \n",
        "- Use `shuffle=True` for the training loader to randomize the order of images.  \n",
        "- Use `shuffle=False` for the testing loader to maintain consistent evaluation."
      ],
      "metadata": {
        "id": "sYbmq29zUM7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = <REPLACE ME AND MY ARROWS>\n",
        "test_loader = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "DaEmkz8RUND0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 4: Train a Neural Network\n",
        "\n",
        "  - Run the provided code which goes through the following steps:\n",
        "    - Initialize the model using the `SimpleNN()` class.  \n",
        "    - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "    - Train the model for 5 epochs using the `train_model()` function.  \n",
        "    - Test the model using the `test_model()` function and print the accuracy."
      ],
      "metadata": {
        "id": "k9n9_IZNYC_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 EXERCISE\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = <REPLACE ME AND MY ARROWS>\n",
        "optimizer = optim.SGD(model.parameters(), lr=<REPLACE ME AND MY ARROWS>)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training model without standardization or augmentation...\")\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nTesting model without standardization or augmentation...\")\n",
        "test_accuracy = test_model(model, test_loader)"
      ],
      "metadata": {
        "id": "YinocWRzYDIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Train a Neural Network with Standardization\n",
        "\n",
        "Now, let's build upon our previous exercise by incorporating data standardization into our neural network training process. We'll see how standardizing our input data can impact model performance.\n",
        "\n",
        "Tasks:\n",
        "1. **Create a Basic Data Transformation Pipeline**\n",
        "  - Create a variable called `transform_std` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation, as well as the `torchvision.transforms.Normalize(~)` transformation.\n",
        "2. **Load the MNIST Training and Testing Data**\n",
        "  - Use the `torchvision.datasets.MNIST()` function to download the new training dataset.\n",
        "  - Apply the `transform_std` pipeline to the training dataset  \n",
        "  - Name the variables `dataset_train_std` for the training data.\n",
        "3. **Create DataLoaders for Training and Testing**\n",
        "  - Use the `DataLoader()` function to create `train_loader_std`.\n",
        "  - Set `batch_size=64` for this new loader.  \n",
        "  - Use `shuffle=True` for the training loader to randomize the order of images.\n",
        "4. **Train and Test the Neural Network**\n",
        "  - Run the provided code which goes through the following steps:\n",
        "    - Initialize the model using the `SimpleNN()` class.  \n",
        "    - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "    - Train the model for 5 epochs using the `train_model()` function.  \n",
        "    - Test the model using the `test_model()` function and print the accuracy."
      ],
      "metadata": {
        "id": "SmHiWZ6cNoy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Create a Basic Data Transformation Pipeline\n",
        "  - Create a variable called `transform_std` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation, as well as the `torchvision.transforms.Normalize(~)` transformation."
      ],
      "metadata": {
        "id": "F56VQsoGcCga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Load dataset without augmentation\n",
        "transform_std = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "ZklL4a30cCpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Load the MNIST Training and Testing Data\n",
        "  - Use the `torchvision.datasets.MNIST()` function to download the new training dataset.\n",
        "  - Apply the `transform_std` pipeline to the training dataset  \n",
        "  - Name the variables `dataset_train_std` for the training data.\n"
      ],
      "metadata": {
        "id": "roF05DQGdz75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Create the new dataset\n",
        "dataset_train_std = <REPLACE ME AND MY BRACKETS>"
      ],
      "metadata": {
        "id": "k5qWb-EEd0F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3: Create DataLoaders for Training and Testing\n",
        "  - Use the `DataLoader()` function to create `train_loader_std`.\n",
        "  - Set `batch_size=64` for this new loader.  \n",
        "  - Use `shuffle=True` for the training loader to randomize the order of images."
      ],
      "metadata": {
        "id": "KLWNFSepd0_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Create the new DataLoader\n",
        "train_loader_std = <REPLACE ME AND MY BRACKETS>"
      ],
      "metadata": {
        "id": "m2sswprSd1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 4: Train and Test the Neural Network\n",
        "- Run the provided code which goes through the following steps:\n",
        "  - Initialize the model using the `SimpleNN()` class.  \n",
        "  - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "  - Train the model for 5 epochs using the `train_model()` function.  \n",
        "  - Test the model using the `test_model()` function and print the accuracy."
      ],
      "metadata": {
        "id": "xQAPS1CccC8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 SOLUTION AND EXERCISE\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model_std = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_std.parameters(), lr=<REPLACE ME AND MY ARROWS>)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training model with standardization...\")\n",
        "train_model(model_std, train_loader_std, criterion, optimizer, epochs=5)\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nTesting model without standardization...\")\n",
        "test_accuracy_std = test_model(model_std, test_loader)"
      ],
      "metadata": {
        "id": "-7k-oA47cDGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Train a Neural Network with Standardization and Data Augmentation\n",
        "\n",
        "In this final exercise, we'll combine both data standardization and data augmentation techniques to create a more robust training pipeline and observe how these combined techniques affect model performance.\n",
        "\n",
        "Tasks:\n",
        "1. **Create a Data Transformation Pipeline with Data Augmentation**\n",
        "  - Create a variable called `transform_aug_std` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation, the `torchvision.transforms.Normalize(~)` transformation and some data augmentation transformations. Some to add in include `transforms.RandomRotation(~)` and `transforms.RandomAffine(degrees=0, translate=(~, ~))`.\n",
        "    - *Try using small values for the parameters to ensure that you don't manipulate the data to much so that it looks completely unlike the original dataset.*\n",
        "2. **Load the Adjusted MNIST Training  Data**\n",
        "  - Use the `torchvision.datasets.MNIST()` function to download the new training dataset.\n",
        "  - Apply the `transform_aug_std` pipeline to the training dataset  \n",
        "  - Name the variables `dataset_train_aug_std` for the training data.\n",
        "3. **Create DataLoaders for Training**\n",
        "  - Use the `DataLoader()` function to create `train_loader_aug_std`.\n",
        "  - Set `batch_size=64` for this new loader.  \n",
        "  - Use `shuffle=True` for the training loader to randomize the order of images.\n",
        "4. **Train and Test the Neural Network**\n",
        "  - Run the provided code which goes through the following steps:\n",
        "    - Initialize the model using the `SimpleNN()` class.  \n",
        "    - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "    - Train the model for 5 epochs using the `train_model()` function.  \n",
        "    - Test the model using the `test_model()` function and print the accuracy.\n",
        "5. ** Compare the Accuracies of the three Different Neural Networks **\n",
        "  - Compare the accuracies of the three different models and come up with theories as to why the testing accuracies either increased or decreased with the adjusted transformation pipelines."
      ],
      "metadata": {
        "id": "lsvR_69XMuwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Create a Data Transformation Pipeline with Data Augmentation\n",
        "  - Create a variable called `transform_aug_std` using `torchvision.transforms.Compose([~])` that applies the `torchvision.transforms.ToTensor()` transformation, the `torchvision.transforms.Normalize(~)` transformation and some data augmentation transformations. Some to add in include `transforms.RandomRotation(~)` and `transforms.RandomAffine(degrees=0, translate=(~, ~))`.\n",
        "    - *Try using small values for the parameters to ensure that you don't manipulate the data to much so that it looks completely unlike the original dataset.*"
      ],
      "metadata": {
        "id": "GG2P3h74gpJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Load dataset with standardization and augmentation\n",
        "transform_aug_std = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "nWSgLGBFgpJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Load the adjusted MNIST Training\n",
        "  - Use the `torchvision.datasets.MNIST()` function to download the new training dataset.\n",
        "  - Apply the `transform_aug_std` pipeline to the training dataset  \n",
        "  - Name the variables `dataset_train_aug_std` for the training data.\n"
      ],
      "metadata": {
        "id": "pBEnzL3MgpJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Create the new dataset\n",
        "dataset_train_aug_std = <REPLACE ME AND MY BRACKETS>"
      ],
      "metadata": {
        "id": "7EYbvQYAgpJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3: Create DataLoaders for Training\n",
        "  - Use the `DataLoader()` function to create `train_loader_aug_std`.\n",
        "  - Set `batch_size=64` for this new loader.  \n",
        "  - Use `shuffle=True` for the training loader to randomize the order of images."
      ],
      "metadata": {
        "id": "gxOcyEjCgpJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Create the new DataLoader\n",
        "train_loader_aug_std = <REPLACE ME AND MY BRACKETS>"
      ],
      "metadata": {
        "id": "90irWcZxgpJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 4: Train and Test the Neural Network\n",
        "  - Run the provided code which goes through the following steps:\n",
        "    - Initialize the model using the `SimpleNN()` class.  \n",
        "    - Define the loss function using `nn.CrossEntropyLoss()` and the optimizer using `optim.SGD()` with a learning rate of `0.01`.  \n",
        "    - Train the model for 5 epochs using the `train_model()` function.  \n",
        "    - Test the model using the `test_model()` function and print the accuracy."
      ],
      "metadata": {
        "id": "Tw8wH0X8gpJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 SOLUTION AND EXERCISE\n",
        "\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model_aug_std = SimpleNN()\n",
        "optimizer_aug_std = optim.SGD(model_aug_std.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training model with standardization and augmentation...\")\n",
        "train_model(model_aug_std, train_loader_aug_std, criterion, optimizer_aug_std, epochs=5)\n",
        "\n",
        "# Test the model\n",
        "print(\"\\nTesting model with standardization and augmentation...\")\n",
        "test_accuracy_aug_std = test_model(model_aug_std, test_loader)"
      ],
      "metadata": {
        "id": "pwNb4LdEgpJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 5: Compare the Accuracies of the three Different Neural Networks\n",
        "\n",
        "- Compare the accuracies of the three different models and come up with theories as to why the testing accuracies either increased or decreased with the adjusted transformation pipelines."
      ],
      "metadata": {
        "id": "ISPG1LWkhEcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store accuracies\n",
        "accuracies = []\n",
        "\n",
        "# Test the basic model\n",
        "print(\"\\nTesting model without standardization or augmentation...\")\n",
        "test_accuracy = test_model(model, test_loader)\n",
        "accuracies.append((\"Basic Model\", test_accuracy))\n",
        "\n",
        "# Test the standardized model\n",
        "print(\"\\nTesting model with standardization...\")\n",
        "test_accuracy_std = test_model(model_std, test_loader)\n",
        "accuracies.append((\"Standardized Model\", test_accuracy_std))\n",
        "\n",
        "# Test the augmented + standardized model\n",
        "print(\"\\nTesting model with standardization and augmentation...\")\n",
        "test_accuracy_aug_std = test_model(model_aug_std, test_loader)\n",
        "accuracies.append((\"Augmented + Standardized Model\", test_accuracy_aug_std))\n",
        "\n",
        "# Print out all accuracies\n",
        "print(\"\\nSummary of Test Accuracies:\")\n",
        "for model_name, accuracy in accuracies:\n",
        "    print(f\"{model_name}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "aBP80I2tHfaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "dzPoIkFpCoJQ"
      }
    }
  ]
}