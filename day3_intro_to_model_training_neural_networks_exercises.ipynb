{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day3_intro_to_model_training_neural_networks_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
      ],
      "metadata": {
        "id": "EU7YJnzI7Ne4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a Simple Neural Network for Regression with the California Housing Dataset"
      ],
      "metadata": {
        "id": "mGXBDYuOp4C5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the California Housing Datset"
      ],
      "metadata": {
        "id": "gL9xKxtDzQ-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "ecVNVkMPUt_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a Neural Network model to Predict Median House Value"
      ],
      "metadata": {
        "id": "slDSPG6oikAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide a function which can help you visualize the neural network you are creating with an example output from this function as well."
      ],
      "metadata": {
        "id": "lONLWGG0kvws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_nn_from_model(model):\n",
        "    \"\"\"\n",
        "    Visualizes a neural network architecture from a PyTorch nn.Module.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The PyTorch model to visualize.\n",
        "\n",
        "    The function interprets linear layers and activation functions, then\n",
        "    plots a graph representation of the architecture.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for layer in model.network:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            layers.append(f\"Linear ({layer.in_features} → {layer.out_features})\")\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            layers.append(\"ReLU\")\n",
        "        elif isinstance(layer, nn.Sigmoid):\n",
        "            layers.append(\"Sigmoid\")\n",
        "        elif isinstance(layer, nn.Tanh):\n",
        "            layers.append(\"Tanh\")\n",
        "        # Add more cases for other layers as needed\n",
        "\n",
        "    # Build visualization graph\n",
        "    G = nx.DiGraph()\n",
        "    positions = {}\n",
        "    y_offset = 0\n",
        "\n",
        "    for idx, layer in enumerate(layers):\n",
        "        node_name = f\"L{idx}: {layer}\"\n",
        "        G.add_node(node_name)\n",
        "        positions[node_name] = (0, -y_offset)\n",
        "        y_offset += 1\n",
        "\n",
        "    # Add edges\n",
        "    for i in range(len(layers) - 1):\n",
        "        G.add_edge(f\"L{i}: {layers[i]}\", f\"L{i+1}: {layers[i+1]}\")\n",
        "\n",
        "    # Draw the graph\n",
        "    plt.figure(figsize=(10, len(layers) * 1.5))\n",
        "    nx.draw(\n",
        "        G,\n",
        "        pos=positions,\n",
        "        with_labels=True,\n",
        "        node_size=3000,\n",
        "        node_color=\"lightblue\",\n",
        "        edge_color=\"gray\",\n",
        "        arrows=True\n",
        "    )\n",
        "    plt.title(\"Neural Network Architecture\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7Iaf-AAST-Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Define a Simple Neural Network Model\n",
        "\n",
        "Your objective is to define and implement a simple feedforward neural network with the following structure:\n",
        "- Input layer: Accepts the input features.\n",
        "- Hidden layer 1: 128 neurons with ReLU activation.\n",
        "- Hidden layer 2: 64 neurons with ReLU activation.\n",
        "- Output layer: A single neuron (for regression tasks).\n",
        "\n",
        "\n",
        "Tasks:\n",
        "1. Define the Neural Network Model\n",
        "  - Create a class named NeuralNetwork that inherits from nn.Module. Inside the class:\n",
        "    - Implement the __init__ method to define the network architecture using nn.Sequential. Include the following:\n",
        "      - A linear layer connecting the input to 128 neurons using the method `nn.Linear(~)`.\n",
        "      - A ReLU activation function using the method `nn.ReLU(~)`.\n",
        "      - A linear layer connecting 128 neurons to 64 neurons using the method `nn.Linear(~)`.\n",
        "      - Another ReLU activation function using the method `nn.ReLU(~)`.\n",
        "      - A linear layer connecting 64 neurons to 1 output neuron using the method `nn.Linear(~)`.\n",
        "    - You could alternately define a neural network using the `nn.Sequential(~)` method. This is easier, but less flexible.\n",
        "2. Verify your implementation by creating a dummy input tensor, and running it through your neural network model. *Remember, you haven't trained it yet.*"
      ],
      "metadata": {
        "id": "KTH8DbQEF9oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Build a Neural Network Class"
      ],
      "metadata": {
        "id": "J6CV_BQgoj7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Initialize the neural network model.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of features in the input data.\n",
        "        \"\"\"\n",
        "        # Call the constructor of the parent class (nn.Module)\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define the network architecture\n",
        "        self.network = nn.Sequential(\n",
        "            ''' ADD YOUR CODE HERE '''\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "H2z4rOf8oPjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2: Validate your Neural Network Class"
      ],
      "metadata": {
        "id": "QGYNkdbwoobj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Create an Instance of your Neural Network Class\n",
        "input_size = 10  # Example input size\n",
        "model = <REPLACE ME AND MY ARROWS>  # Initialize the model\n",
        "\n",
        "# Visualize your Model\n",
        "visualize_nn_from_model(model)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "dummy_input = torch.randn(1, input_size)  # Batch size of 1\n",
        "output = <REPLACE ME AND MY ARROWS>  # Forward pass\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output: \" + str(output))"
      ],
      "metadata": {
        "id": "4DN2y98ioyJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Write a function to Train your Neural Network Model\n",
        "\n",
        "Now, we are going to build a function which then trains the neural network model.\n",
        "\n",
        "Tasks:\n",
        "1. Define the training function:\n",
        "  - For each batch of data:\n",
        "    - Reset the gradients using `optimizer.zero_grad()`.\n",
        "    - Perform a forward pass through the model.\n",
        "    - Calculate the loss using the `criterion(~)` function.\n",
        "    - Perform backpropagation using `loss.backward()`.\n",
        "    - Update the model weights using `optimizer.step()`.\n",
        "2. Train your model with a synthetic dataset."
      ],
      "metadata": {
        "id": "5X2s7vj8GCcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Build a function to Train your Neural Network Model"
      ],
      "metadata": {
        "id": "_uWJUFwe_C7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    \"\"\"\n",
        "    Train a neural network model.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model to train.\n",
        "        train_loader: DataLoader for the training dataset.\n",
        "        criterion: Loss function to minimize.\n",
        "        optimizer: Optimization algorithm to update model weights.\n",
        "        epochs (int): Number of epochs to train for.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for data, targets in train_loader:\n",
        "            <REPLACE ME AND MY ARROWS>  # Zero the parameter gradients\n",
        "            outputs = model(<REPLACE ME AND MY ARROWS>).squeeze()  # Forward pass where .squeeze() removes any unnecessary dimensions\n",
        "            loss = <REPLACE ME AND MY ARROWS>  # Compute loss\n",
        "            <REPLACE ME AND MY ARROWS>  # Backpropagation\n",
        "            <REPLACE ME AND MY ARROWS>  # Update weights\n",
        "            running_loss += loss.item()  # Accumulate loss where .item() turns the tensor into a float\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "nB8kLcLx_DER",
        "outputId": "53f60903-1f51-4b30-a65a-cdfa2cad323b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-0811f40deda8>, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0811f40deda8>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    <REPLACE ME AND MY ARROWS>  # Zero the parameter gradients\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Validate your Training Function"
      ],
      "metadata": {
        "id": "l8ECSwIkAbNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Create a small dummy dataset\n",
        "X_train = torch.randn(100, 10)\n",
        "y_train = torch.randn(100)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "input_size = 10\n",
        "model = NeuralNetwork(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "<REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "bBGhRMfEEW2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Make the California Housing Dataset Amenable with PyTorch Dataloaders\n",
        "\n",
        "Tasks:\n",
        "1. Create a custom dataset class `NewDataset`\n",
        "  - Accept data (features) and targets (labels) as input.\n",
        "  - Implement the `__len__()` method to return the number of samples.\n",
        "  - Implement the `__getitem__()` method to return a sample and its corresponding label as tensors.\n"
      ],
      "metadata": {
        "id": "YNJ1g_ElFTUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Create a Custom Dataset Class"
      ],
      "metadata": {
        "id": "qGDXOuO-Pa-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Custom Dataset\n",
        "class NewDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = <REPLACE ME AND MY ARROWS>\n",
        "        self.targets = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "    def __len__(self):\n",
        "        return <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(<REPLACE ME AND MY ARROWS>, dtype=torch.float32), torch.tensor(<REPLACE ME AND MY ARROWS>, dtype=torch.float32)\n",
        "\n",
        "# Load and preprocess the California Housing Dataset\n",
        "def load_data(test_size=0.2, batch_size=32):\n",
        "\n",
        "    # Fetch dataset\n",
        "    housing = fetch_california_housing()\n",
        "    data, targets = housing.data, housing.target\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Create PyTorch Datasets\n",
        "    train_dataset = NewDataset(X_train, y_train)\n",
        "    test_dataset = NewDataset(X_test, y_test)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "test_size = 0.2\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "\n",
        "# Load data\n",
        "train_loader, test_loader = load_data(test_size, batch_size)\n",
        "\n",
        "# Iterate through the train_loader and print the shape of data and labels\n",
        "for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"  Data shape: {data.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    break  # Stop after the first batch for inspection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "aulhsTNuKf84",
        "outputId": "81eeb4f1-b84b-45d7-9d7b-b1ead24e0c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-21-d8ba35b52ba1>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-d8ba35b52ba1>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    self.data = <REPLACE ME AND MY ARROWS>\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate your Model\n",
        "\n",
        "The following code defines two functions to assess a model's performance: `evaluate_model`, which computes key regression metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R² score using predictions from a test dataset, and `plot_predictions`, which visualizes the relationship between predicted and actual values to help evaluate the model's fit."
      ],
      "metadata": {
        "id": "B1NTxl3zP7P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data).squeeze()\n",
        "            y_true.extend(targets.numpy())\n",
        "            y_pred.extend(outputs.numpy())\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Plot Predicted vs Actual Values\n",
        "def plot_predictions(y_true, y_pred):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.6, label='Predicted vs Actual')\n",
        "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', lw=2, label='Ideal Fit')\n",
        "    plt.title(\"Predicted vs Actual Values\")\n",
        "    plt.xlabel(\"Actual Values\")\n",
        "    plt.ylabel(\"Predicted Values\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VrO15TicC8ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE: Train and Validate your Neural Network Model on the California Housing Dataset\n",
        "\n",
        "Now, let's apply our neural network to a real-world dataset by training it on the California Housing Dataset. We'll need to prepare our data and initialize the model with appropriate parameters.\n",
        "\n",
        "Tasks:\n",
        "1. Prepare the Data and Initialize the Model\n",
        "  - Load the **California Housing dataset** using a data-loading function (e.g., `load_data()`).\n",
        "  - **Determine the input size** by extracting the number of features from the training data batch.\n",
        "  - **Initialize your neural network model**, specifying the correct input size.\n",
        "  - **Set up the loss function** using **`nn.MSELoss()`** and the optimizer using **`optim.Adam()`**.\n",
        "2. Train, Evaluate, and Plot Results\n",
        "  - **Train the model** using the `train_model()` function.  \n",
        "  - **Evaluate the model** using the `evaluate_model()` function to compute predictions on the test set.\n",
        "  - **Plot predictions** using the `plot_predictions()` function."
      ],
      "metadata": {
        "id": "n58MZOojPivG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 1: Prepare the Data and Initialize the Model"
      ],
      "metadata": {
        "id": "J7gM6ewWRUsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "test_size = 0.2\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "\n",
        "# Load data\n",
        "train_loader, test_loader = load_data(test_size, batch_size)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_size = <REPLACE ME AND MY ARROWS>  # Get number of features\n",
        "model = <REPLACE ME AND MY ARROWS>\n",
        "criterion = <REPLACE ME AND MY ARROWS>\n",
        "optimizer = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "rWQ6LBRlRU3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TASK 2: Train, Evaluate, and Plot Results"
      ],
      "metadata": {
        "id": "_tr2_TriRV0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 SOLUION\n",
        "\n",
        "# Train and evaluate\n",
        "print(\"Training the model...\")\n",
        "train_model(<REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nEvaluating the model...\")\n",
        "y_true, y_pred = evaluate_model(<REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>,  <REPLACE ME AND MY ARROWS>)\n",
        "\n",
        "# Plot predictions\n",
        "plot_predictions(<REPLACE ME AND MY ARROWS>, <REPLACE ME AND MY ARROWS>)"
      ],
      "metadata": {
        "id": "Zb9TTSlRRV-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Hyperparameters"
      ],
      "metadata": {
        "id": "l7B45zAaiXMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYZE: Build a Function to Test Different Sets of Hyperparameters"
      ],
      "metadata": {
        "id": "c9qp1jprrC3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tune and evaluate\n",
        "def tune_hyperparameters(batch_size, learning_rate, hidden_layer_sizes, epochs):\n",
        "\n",
        "    print(f\"Tuning: batch_size={batch_size}, learning_rate={learning_rate}, hidden_layers={hidden_layer_sizes}, epochs={epochs}\")\n",
        "\n",
        "    # Load data\n",
        "    train_loader, test_loader = load_data(test_size=0.2, batch_size=batch_size)\n",
        "\n",
        "    # Define the model\n",
        "    class TunableNeuralNetwork(nn.Module):\n",
        "        def __init__(self, input_size, hidden_layer_sizes):\n",
        "            super(TunableNeuralNetwork, self).__init__()\n",
        "            layers = []\n",
        "            in_features = input_size\n",
        "            for out_features in hidden_layer_sizes:\n",
        "                layers.append(nn.Linear(in_features, out_features))\n",
        "                layers.append(nn.ReLU())\n",
        "                in_features = out_features\n",
        "            layers.append(nn.Linear(in_features, 1))\n",
        "            self.network = nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.network(x)\n",
        "\n",
        "    input_size = next(iter(train_loader))[0].shape[1]\n",
        "    model = TunableNeuralNetwork(input_size, hidden_layer_sizes)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_true, y_pred = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "    # Metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"Metrics: MSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
        "    return mse, rmse, r2\n",
        "\n",
        "# Example: Grid search over parameters\n",
        "batch_sizes = [32, 64]\n",
        "learning_rates = [0.001, 0.01]\n",
        "hidden_layer_configs = [[24, 12], [6, 3]]\n",
        "epoch_counts = [5, 10]\n",
        "\n",
        "best_config = None\n",
        "best_r2 = -np.inf\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for hidden_layers in hidden_layer_configs:\n",
        "            for epochs in epoch_counts:\n",
        "                mse, rmse, r2 = tune_hyperparameters(batch_size, learning_rate, hidden_layers, epochs)\n",
        "                if r2 > best_r2:\n",
        "                    best_r2 = r2\n",
        "                    best_config = (batch_size, learning_rate, hidden_layers, epochs)\n",
        "\n",
        "print(f\"Best Configuration: batch_size={best_config[0]}, learning_rate={best_config[1]}, hidden_layers={best_config[2]}, epochs={best_config[3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3eS2_sGjlIq",
        "outputId": "bb510943-3b39-4c6a-9165-a8c4b3a01565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning: batch_size=32, learning_rate=0.001, hidden_layers=[24, 12], epochs=5\n",
            "Epoch 1/5, Loss: 1.6302\n",
            "Epoch 2/5, Loss: 0.4770\n",
            "Epoch 3/5, Loss: 0.4038\n",
            "Epoch 4/5, Loss: 0.3869\n",
            "Epoch 5/5, Loss: 0.3753\n",
            "Mean Squared Error (MSE): 0.3733\n",
            "Root Mean Squared Error (RMSE): 0.6109\n",
            "R² Score: 0.7152\n",
            "Metrics: MSE=0.3733, RMSE=0.6109, R²=0.7152\n",
            "Tuning: batch_size=32, learning_rate=0.001, hidden_layers=[24, 12], epochs=10\n",
            "Epoch 1/10, Loss: 1.2408\n",
            "Epoch 2/10, Loss: 0.4861\n",
            "Epoch 3/10, Loss: 0.4244\n",
            "Epoch 4/10, Loss: 0.4011\n",
            "Epoch 5/10, Loss: 0.3871\n",
            "Epoch 6/10, Loss: 0.3752\n",
            "Epoch 7/10, Loss: 0.3633\n",
            "Epoch 8/10, Loss: 0.3574\n",
            "Epoch 9/10, Loss: 0.3486\n",
            "Epoch 10/10, Loss: 0.3406\n",
            "Mean Squared Error (MSE): 0.3457\n",
            "Root Mean Squared Error (RMSE): 0.5880\n",
            "R² Score: 0.7362\n",
            "Metrics: MSE=0.3457, RMSE=0.5880, R²=0.7362\n",
            "Tuning: batch_size=32, learning_rate=0.001, hidden_layers=[6, 3], epochs=5\n",
            "Epoch 1/5, Loss: 6.9207\n",
            "Epoch 2/5, Loss: 4.9307\n",
            "Epoch 3/5, Loss: 3.5196\n",
            "Epoch 4/5, Loss: 2.5619\n",
            "Epoch 5/5, Loss: 1.9555\n",
            "Mean Squared Error (MSE): 1.6980\n",
            "Root Mean Squared Error (RMSE): 1.3031\n",
            "R² Score: -0.2957\n",
            "Metrics: MSE=1.6980, RMSE=1.3031, R²=-0.2957\n",
            "Tuning: batch_size=32, learning_rate=0.001, hidden_layers=[6, 3], epochs=10\n",
            "Epoch 1/10, Loss: 2.5708\n",
            "Epoch 2/10, Loss: 0.7153\n",
            "Epoch 3/10, Loss: 0.5518\n",
            "Epoch 4/10, Loss: 0.4744\n",
            "Epoch 5/10, Loss: 0.4380\n",
            "Epoch 6/10, Loss: 0.4260\n",
            "Epoch 7/10, Loss: 0.4209\n",
            "Epoch 8/10, Loss: 0.4172\n",
            "Epoch 9/10, Loss: 0.4143\n",
            "Epoch 10/10, Loss: 0.4118\n",
            "Mean Squared Error (MSE): 0.4154\n",
            "Root Mean Squared Error (RMSE): 0.6445\n",
            "R² Score: 0.6830\n",
            "Metrics: MSE=0.4154, RMSE=0.6445, R²=0.6830\n",
            "Tuning: batch_size=32, learning_rate=0.01, hidden_layers=[24, 12], epochs=5\n",
            "Epoch 1/5, Loss: 0.5660\n",
            "Epoch 2/5, Loss: 0.3873\n",
            "Epoch 3/5, Loss: 0.3624\n",
            "Epoch 4/5, Loss: 0.3429\n",
            "Epoch 5/5, Loss: 0.3409\n",
            "Mean Squared Error (MSE): 0.3685\n",
            "Root Mean Squared Error (RMSE): 0.6070\n",
            "R² Score: 0.7188\n",
            "Metrics: MSE=0.3685, RMSE=0.6070, R²=0.7188\n",
            "Tuning: batch_size=32, learning_rate=0.01, hidden_layers=[24, 12], epochs=10\n",
            "Epoch 1/10, Loss: 0.5590\n",
            "Epoch 2/10, Loss: 0.3910\n",
            "Epoch 3/10, Loss: 0.4097\n",
            "Epoch 4/10, Loss: 0.3623\n",
            "Epoch 5/10, Loss: 0.3594\n",
            "Epoch 6/10, Loss: 0.3429\n",
            "Epoch 7/10, Loss: 0.3404\n",
            "Epoch 8/10, Loss: 0.3360\n",
            "Epoch 9/10, Loss: 0.3303\n",
            "Epoch 10/10, Loss: 0.3211\n",
            "Mean Squared Error (MSE): 0.3324\n",
            "Root Mean Squared Error (RMSE): 0.5765\n",
            "R² Score: 0.7464\n",
            "Metrics: MSE=0.3324, RMSE=0.5765, R²=0.7464\n",
            "Tuning: batch_size=32, learning_rate=0.01, hidden_layers=[6, 3], epochs=5\n",
            "Epoch 1/5, Loss: 0.7616\n",
            "Epoch 2/5, Loss: 0.4133\n",
            "Epoch 3/5, Loss: 0.4247\n",
            "Epoch 4/5, Loss: 0.4051\n",
            "Epoch 5/5, Loss: 0.3925\n",
            "Mean Squared Error (MSE): 0.3897\n",
            "Root Mean Squared Error (RMSE): 0.6242\n",
            "R² Score: 0.7026\n",
            "Metrics: MSE=0.3897, RMSE=0.6242, R²=0.7026\n",
            "Tuning: batch_size=32, learning_rate=0.01, hidden_layers=[6, 3], epochs=10\n",
            "Epoch 1/10, Loss: 0.6315\n",
            "Epoch 2/10, Loss: 0.4273\n",
            "Epoch 3/10, Loss: 0.4212\n",
            "Epoch 4/10, Loss: 0.4091\n",
            "Epoch 5/10, Loss: 0.3948\n",
            "Epoch 6/10, Loss: 0.3957\n",
            "Epoch 7/10, Loss: 0.3761\n",
            "Epoch 8/10, Loss: 0.3678\n",
            "Epoch 9/10, Loss: 0.3644\n",
            "Epoch 10/10, Loss: 0.3603\n",
            "Mean Squared Error (MSE): 0.3691\n",
            "Root Mean Squared Error (RMSE): 0.6076\n",
            "R² Score: 0.7183\n",
            "Metrics: MSE=0.3691, RMSE=0.6076, R²=0.7183\n",
            "Tuning: batch_size=64, learning_rate=0.001, hidden_layers=[24, 12], epochs=5\n",
            "Epoch 1/5, Loss: 1.9581\n",
            "Epoch 2/5, Loss: 0.5529\n",
            "Epoch 3/5, Loss: 0.4428\n",
            "Epoch 4/5, Loss: 0.4126\n",
            "Epoch 5/5, Loss: 0.4000\n",
            "Mean Squared Error (MSE): 0.4080\n",
            "Root Mean Squared Error (RMSE): 0.6387\n",
            "R² Score: 0.6887\n",
            "Metrics: MSE=0.4080, RMSE=0.6387, R²=0.6887\n",
            "Tuning: batch_size=64, learning_rate=0.001, hidden_layers=[24, 12], epochs=10\n",
            "Epoch 1/10, Loss: 1.5525\n",
            "Epoch 2/10, Loss: 0.5605\n",
            "Epoch 3/10, Loss: 0.4657\n",
            "Epoch 4/10, Loss: 0.4237\n",
            "Epoch 5/10, Loss: 0.4008\n",
            "Epoch 6/10, Loss: 0.3868\n",
            "Epoch 7/10, Loss: 0.3779\n",
            "Epoch 8/10, Loss: 0.3703\n",
            "Epoch 9/10, Loss: 0.3652\n",
            "Epoch 10/10, Loss: 0.3592\n",
            "Mean Squared Error (MSE): 0.3639\n",
            "Root Mean Squared Error (RMSE): 0.6032\n",
            "R² Score: 0.7223\n",
            "Metrics: MSE=0.3639, RMSE=0.6032, R²=0.7223\n",
            "Tuning: batch_size=64, learning_rate=0.001, hidden_layers=[6, 3], epochs=5\n",
            "Epoch 1/5, Loss: 2.5013\n",
            "Epoch 2/5, Loss: 0.7241\n",
            "Epoch 3/5, Loss: 0.5779\n",
            "Epoch 4/5, Loss: 0.4973\n",
            "Epoch 5/5, Loss: 0.4564\n",
            "Mean Squared Error (MSE): 0.4578\n",
            "Root Mean Squared Error (RMSE): 0.6766\n",
            "R² Score: 0.6506\n",
            "Metrics: MSE=0.4578, RMSE=0.6766, R²=0.6506\n",
            "Tuning: batch_size=64, learning_rate=0.001, hidden_layers=[6, 3], epochs=10\n",
            "Epoch 1/10, Loss: 4.3993\n",
            "Epoch 2/10, Loss: 1.0705\n",
            "Epoch 3/10, Loss: 0.6095\n",
            "Epoch 4/10, Loss: 0.5318\n",
            "Epoch 5/10, Loss: 0.4962\n",
            "Epoch 6/10, Loss: 0.4706\n",
            "Epoch 7/10, Loss: 0.4559\n",
            "Epoch 8/10, Loss: 0.4452\n",
            "Epoch 9/10, Loss: 0.4386\n",
            "Epoch 10/10, Loss: 0.4318\n",
            "Mean Squared Error (MSE): 0.4406\n",
            "Root Mean Squared Error (RMSE): 0.6638\n",
            "R² Score: 0.6638\n",
            "Metrics: MSE=0.4406, RMSE=0.6638, R²=0.6638\n",
            "Tuning: batch_size=64, learning_rate=0.01, hidden_layers=[24, 12], epochs=5\n",
            "Epoch 1/5, Loss: 0.6644\n",
            "Epoch 2/5, Loss: 0.3969\n",
            "Epoch 3/5, Loss: 0.3702\n",
            "Epoch 4/5, Loss: 0.3552\n",
            "Epoch 5/5, Loss: 0.3402\n",
            "Mean Squared Error (MSE): 0.3609\n",
            "Root Mean Squared Error (RMSE): 0.6007\n",
            "R² Score: 0.7246\n",
            "Metrics: MSE=0.3609, RMSE=0.6007, R²=0.7246\n",
            "Tuning: batch_size=64, learning_rate=0.01, hidden_layers=[24, 12], epochs=10\n",
            "Epoch 1/10, Loss: 0.6950\n",
            "Epoch 2/10, Loss: 0.3756\n",
            "Epoch 3/10, Loss: 0.3557\n",
            "Epoch 4/10, Loss: 0.3384\n",
            "Epoch 5/10, Loss: 0.3329\n",
            "Epoch 6/10, Loss: 0.3292\n",
            "Epoch 7/10, Loss: 0.3201\n",
            "Epoch 8/10, Loss: 0.3179\n",
            "Epoch 9/10, Loss: 0.3110\n",
            "Epoch 10/10, Loss: 0.3081\n",
            "Mean Squared Error (MSE): 0.3459\n",
            "Root Mean Squared Error (RMSE): 0.5881\n",
            "R² Score: 0.7361\n",
            "Metrics: MSE=0.3459, RMSE=0.5881, R²=0.7361\n",
            "Tuning: batch_size=64, learning_rate=0.01, hidden_layers=[6, 3], epochs=5\n",
            "Epoch 1/5, Loss: 0.8686\n",
            "Epoch 2/5, Loss: 0.4102\n",
            "Epoch 3/5, Loss: 0.3939\n",
            "Epoch 4/5, Loss: 0.3860\n",
            "Epoch 5/5, Loss: 0.3815\n",
            "Mean Squared Error (MSE): 0.4267\n",
            "Root Mean Squared Error (RMSE): 0.6532\n",
            "R² Score: 0.6744\n",
            "Metrics: MSE=0.4267, RMSE=0.6532, R²=0.6744\n",
            "Tuning: batch_size=64, learning_rate=0.01, hidden_layers=[6, 3], epochs=10\n",
            "Epoch 1/10, Loss: 0.8531\n",
            "Epoch 2/10, Loss: 0.4043\n",
            "Epoch 3/10, Loss: 0.3892\n",
            "Epoch 4/10, Loss: 0.3832\n",
            "Epoch 5/10, Loss: 0.3822\n",
            "Epoch 6/10, Loss: 0.3767\n",
            "Epoch 7/10, Loss: 0.3742\n",
            "Epoch 8/10, Loss: 0.3753\n",
            "Epoch 9/10, Loss: 0.3702\n",
            "Epoch 10/10, Loss: 0.3676\n",
            "Mean Squared Error (MSE): 0.3715\n",
            "Root Mean Squared Error (RMSE): 0.6095\n",
            "R² Score: 0.7165\n",
            "Metrics: MSE=0.3715, RMSE=0.6095, R²=0.7165\n",
            "Best Configuration: batch_size=32, learning_rate=0.01, hidden_layers=[24, 12], epochs=10\n"
          ]
        }
      ]
    }
  ]
}