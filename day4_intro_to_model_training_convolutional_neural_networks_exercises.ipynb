{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7dCz57XUMWbE",
        "lu9yjZpDTX0O",
        "CzkS8LwbTX0O"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day4_intro_to_model_training_convolutional_neural_networks_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
      ],
      "metadata": {
        "id": "EU7YJnzI7Ne4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training with Convolutional Neural Networks using MNIST"
      ],
      "metadata": {
        "id": "yQgEbfHL4dCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==============================================\n",
        "# 0. Module imports\n",
        "# ==============================================\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import scipy\n",
        "\n",
        "# plots\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "# classification algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# dimension reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection\n",
        "\n",
        "# parallel processing\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "# model evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "device = \"cpu\""
      ],
      "metadata": {
        "id": "vv29cylo7Ne5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncF7Q1R7Ne5"
      },
      "source": [
        "Some visualization code. No need to understand this cell, but don't forget to run it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvCnQKml7Ne5",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "from graphviz import Digraph\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "def visualize(model):\n",
        "    hidden_layers = []\n",
        "    output_layer = 0\n",
        "    layers = [layer for layer in model.modules() if type(layer) == torch.nn.Linear]\n",
        "\n",
        "    # custom script to get all the shapes of a network's layers\n",
        "    sizes = list()\n",
        "    state_dict = net.state_dict()\n",
        "    for idx, key in enumerate(state_dict.keys()):\n",
        "      if key.endswith('weight'):\n",
        "        output_size, input_size = state_dict[key].shape\n",
        "        if idx == 0:\n",
        "          sizes.append(input_size)\n",
        "        sizes.append(output_size)\n",
        "\n",
        "    input_layer = sizes[0]\n",
        "    for layer in layers:\n",
        "        if layer == layers[0]:\n",
        "            hidden_layers.append(layer.out_features)\n",
        "\n",
        "        else:\n",
        "            if layer == layers[-1]:\n",
        "                output_layer = layer.out_features\n",
        "            else:\n",
        "                hidden_layers.append(layer.out_features)\n",
        "\n",
        "        last_layer_nodes = input_layer\n",
        "        nodes_up = input_layer\n",
        "\n",
        "    g = Digraph(\"g\", filename='visualization_tmp')\n",
        "    n = 0\n",
        "    g.graph_attr.update(splines=\"false\", nodesep=\"0.5\", ranksep=\"0\", rankdir='LR')\n",
        "    # Input Layer\n",
        "    with g.subgraph(name=\"cluster_input\") as c:\n",
        "          c.attr(color=\"white\")\n",
        "          for i in range(0, sizes[0]):\n",
        "              n += 1\n",
        "              c.node(str(n))\n",
        "              c.attr(labeljust=\"1\")\n",
        "              c.attr(label=\"Input Layer\", labelloc=\"bottom\")\n",
        "              c.attr(rank=\"same\")\n",
        "              c.node_attr.update(\n",
        "                  width=\"0.65\",\n",
        "                  style=\"filled\",\n",
        "                  shape=\"circle\",\n",
        "                  color=HAPPY_COLORS_PALETTE[3],\n",
        "                  fontcolor=HAPPY_COLORS_PALETTE[3],\n",
        "              )\n",
        "    for layer_idx in range(0, len(sizes) - 2):\n",
        "        with g.subgraph(name=\"cluster_\" + str(layer_idx + 1)) as c:\n",
        "            c.attr(color=\"white\")\n",
        "            c.attr(rank=\"same\")\n",
        "            label = f'Hidden Layer {layer_idx + 1}'\n",
        "            c.attr(labeljust=\"right\", labelloc=\"b\", label=label)\n",
        "            for j in range(0, hidden_layers[layer_idx]):\n",
        "                n += 1\n",
        "                c.node(\n",
        "                    str(n),\n",
        "                    width=\"0.65\",\n",
        "                    shape=\"circle\",\n",
        "                    style=\"filled\",\n",
        "                    color=HAPPY_COLORS_PALETTE[0],\n",
        "                    fontcolor=HAPPY_COLORS_PALETTE[0],\n",
        "                )\n",
        "                for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
        "                    g.edge(str(h), str(n))\n",
        "            last_layer_nodes = hidden_layers[layer_idx]\n",
        "            nodes_up += hidden_layers[layer_idx]\n",
        "\n",
        "    with g.subgraph(name=\"cluster_output\") as c:\n",
        "        c.attr(color=\"white\")\n",
        "        c.attr(rank=\"same\")\n",
        "        c.attr(labeljust=\"1\")\n",
        "        for i in range(1, output_layer + 1):\n",
        "            n += 1\n",
        "            c.node(\n",
        "                str(n),\n",
        "                width=\"0.65\",\n",
        "                shape=\"circle\",\n",
        "                style=\"filled\",\n",
        "                color=HAPPY_COLORS_PALETTE[4],\n",
        "                fontcolor=HAPPY_COLORS_PALETTE[4],\n",
        "\n",
        "            )\n",
        "            for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
        "                g.edge(str(h), str(n))\n",
        "        c.attr(label=\"Output Layer\", labelloc=\"bottom\")\n",
        "        c.node_attr.update(\n",
        "            color=\"#2ecc71\", style=\"filled\", fontcolor=\"#2ecc71\", shape=\"circle\"\n",
        "        )\n",
        "\n",
        "    g.attr(arrowShape=\"none\")\n",
        "    g.edge_attr.update(arrowhead=\"none\", color=\"#707070\", penwidth=\"2\")\n",
        "    g.view()\n",
        "    return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define DataLoaders\n",
        "\n",
        "This code creates **data loaders** for the **MNIST dataset** using PyTorch, which handles the loading and batching of training and testing data. The **batch sizes** are set to 64 for training and 1000 for testing to balance computation efficiency and memory usage. The `train_loader` shuffles the training data for better generalization, while the `test_loader` keeps the test data in a fixed order.\n",
        "\n",
        "Both data loaders apply a series of **transformations** to the images: converting them to tensors and normalizing pixel values to have a mean of 0.1307 and a standard deviation of 0.3081. These normalization values are calculated based on the MNIST dataset and help improve model convergence during training."
      ],
      "metadata": {
        "id": "vpHm7R-_DOWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Batch Sizes\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "# Training DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('sample_data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "# Testing DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('sample_data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=False)"
      ],
      "metadata": {
        "id": "YEcN-Wd8DOhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYZE: Explore Common Neural Network Layers\n",
        "\n",
        "In this exercise, we explore common PyTorch layers and use them to transform an example MNIST image.\n",
        "\n",
        "Tasks:\n",
        "1. Explore `nn.Flatten()` - Converts a multi-dimensional tensor into a single-dimensional array.\n",
        "2. Explore `nn.Linear()`– Applies a linear transformation to the input.\n",
        "3. Explore `nn.ReLU()`– Introduces non-linearity to the network.\n",
        "4. Explore `nn.Sequential()`– Combines layers into a sequence.\n",
        "5. Explore `nn.Softmax()`Converts raw scores (logits) into probabilities."
      ],
      "metadata": {
        "id": "mzn8yOQ51bVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "input_image = images[0]\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea8a4cf-1d25-4211-8886-8bfd8a19372a",
        "id": "jZ2jBl5C2mqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: nn.Flatten\n",
        "\n",
        "The `nn.Flatten()` layer reshapes a 2D image into a 1D array of pixel values. This is often the first step when using fully connected layers, as they require a flat input."
      ],
      "metadata": {
        "id": "wOVBWc4N2mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Original image\n",
        "ax[0].imshow(input_image.squeeze(), cmap='gray')\n",
        "ax[0].set_title(f'Original Image - Label: {labels[0].item()}')\n",
        "ax[0].axis('off')\n",
        "\n",
        "# Plot the flattened Image\n",
        "ax[1].imshow(flat_image.detach().numpy(), cmap='gray', aspect='auto')\n",
        "ax[1].set_title('Flattened Image (784 pixels)')\n",
        "ax[1].set_xlabel('Pixel Index')\n",
        "ax[1].set_ylabel('Pixel Value')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZmlyCc5x2mqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: nn.Linear\n",
        "\n",
        "The `nn.Linear()` layer applies a linear transformation to the input data using its weights and biases. This helps the network create a feature space."
      ],
      "metadata": {
        "id": "AqYORmW72mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Original image\n",
        "ax[0].imshow(input_image.squeeze(), cmap='gray')\n",
        "ax[0].set_title(f'Original Image - Label: {labels[0].item()}')\n",
        "ax[0].axis('off')\n",
        "\n",
        "# Hidden layer output as a bar chart\n",
        "ax[1].bar(range(hidden1.size(1)), hidden1.detach().numpy().squeeze())\n",
        "ax[1].set_title('Hidden Layer Output (20 features)')\n",
        "ax[1].set_xlabel('Feature Index')\n",
        "ax[1].set_ylabel('Feature Value')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mA8KFW8s2mqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3: nn.ReLU\n",
        "\n",
        "The `nn.ReLU()` activation function introduces non-linearity into the model. Without non-linear functions, the network would be limited to learning only linear relationships."
      ],
      "metadata": {
        "id": "uENYj4KL2mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden1_ReLU = nn.ReLU()(hidden1)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharey = True)\n",
        "\n",
        "# Original image\n",
        "ax[0].bar(range(hidden1.size(1)), hidden1.detach().numpy().squeeze())\n",
        "ax[0].set_title(f'Original Image - Label: {labels[0].item()}')\n",
        "\n",
        "# Hidden layer output as a bar chart after ReLU\n",
        "ax[1].bar(range(hidden1_ReLU.size(1)), hidden1_ReLU.detach().numpy().squeeze())\n",
        "ax[1].set_title('Hidden Layer Output (ReLU Applied, 20 Features)')\n",
        "ax[1].set_xlabel('Feature Index')\n",
        "ax[1].set_ylabel('Feature Value')\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ymIaHIIf2mqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 4: nn.Sequential\n",
        "\n",
        "The `nn.Sequential()` module allows us to define a complete network by chaining multiple layers together. This creates a straightforward, ordered flow of data through the network."
      ],
      "metadata": {
        "id": "2w_utkyP2mqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "# Capture outputs at each step\n",
        "flattened = flatten(input_image)\n",
        "layer1_output = layer1(flattened)\n",
        "relu_output = nn.ReLU()(layer1_output)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "# Plot the outputs\n",
        "fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
        "\n",
        "axes[0].imshow(input_image.squeeze().numpy(), cmap='gray')\n",
        "axes[0].set_title(\"Original Image\")\n",
        "\n",
        "axes[1].imshow(flattened.detach().numpy(), cmap='gray', aspect='auto')\n",
        "axes[1].set_title(\"Flattened\")\n",
        "\n",
        "axes[2].bar(range(20), layer1_output.detach().numpy().squeeze())\n",
        "axes[2].set_title(\"Layer 1 Output\")\n",
        "\n",
        "axes[3].bar(range(10), logits.detach().numpy().squeeze())\n",
        "axes[3].set_title(\"Logits (Output)\")\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1URIkrrg2mqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 5: nn.Softmax\n",
        "\n",
        "The `nn.Softmax()` layer converts raw logits into probabilities that sum to 1. It’s typically applied to the final layer of a classification network to produce class probabilities."
      ],
      "metadata": {
        "id": "OEtwolNm2mqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Softmax to convert logits to probabilities\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "# Plot the logits and the resulting probabilities\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Logits (Before Softmax)\n",
        "axes[0].bar(range(10), logits.squeeze().detach().numpy())\n",
        "axes[0].set_title(\"Logits (Before Softmax)\")\n",
        "axes[0].set_xlabel(\"Class Index\")\n",
        "axes[0].set_ylabel(\"Value\")\n",
        "\n",
        "# Predicted Probabilities (After Softmax)\n",
        "axes[1].bar(range(10), pred_probab.squeeze().detach().numpy())\n",
        "axes[1].set_title(\"Predicted Probabilities (After Softmax)\")\n",
        "axes[1].set_xlabel(\"Class Index\")\n",
        "axes[1].set_ylabel(\"Probability\")\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pXPns1D82mqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Simple Convolutional Network\n",
        "\n",
        "The `train()` function is responsible for updating the network's parameters during the **training phase** using data from the training set. It sets the network to **training mode**, enabling layers like dropout that improve generalization. For **fully connected networks**, the input data is flattened into a 1D vector before being passed to the network, but this step is **skipped for convolutional neural networks (CNNs)**, which handle multi-dimensional inputs directly. During each batch, the function performs a **forward pass** to make predictions, calculates the **loss** using the Negative Log Likelihood function, and performs **backpropagation** to update the network’s weights. The function tracks and logs the **training progress** to monitor performance over time."
      ],
      "metadata": {
        "id": "dhhsjPaiCrZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: network.train() puts the network into \"nondeterministic mode\".\n",
        "# This doesn't matter for the fully connected network, but it will matter for convolutional networks.\n",
        "# CNNs rely on preserving spatial structure, so the nondeterministic mode can impact results.\n",
        "def train(network, optimizer, epoch, trainloader, flatten_data=True):\n",
        "\n",
        "    network.train()  # Set the network to training mode\n",
        "    loss_logs = list()  # Track the loss over time\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        # For fully connected networks, we flatten the image data to feed it into the network.\n",
        "        # CNNs, however, expect multi-dimensional image inputs, so we skip flattening.\n",
        "        if flatten_data:\n",
        "            data = flatten(data)  # Flatten the image (only for fully connected networks)\n",
        "\n",
        "        # Move data to GPU if available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform the forward pass through the network\n",
        "        output = network(data)\n",
        "\n",
        "        # Compute the loss using the Negative Log Likelihood loss function\n",
        "        # This loss function is often used for classification tasks with multiple classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Backpropagation: Calculate the gradients of the loss with respect to the network's parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform the optimization step: Update the network's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress at regular intervals\n",
        "        if batch_idx % log_interval == 0:\n",
        "            examples_shown = batch_idx * len(data)\n",
        "            total_examples = len(trainloader.dataset)\n",
        "            fraction_shown = round(examples_shown / total_examples * 100.0, 2)\n",
        "            rounded_loss = round(loss.item(), 4)\n",
        "            print(f'Train Epoch {epoch} Progress: {fraction_shown}%\\tLoss: {rounded_loss}')\n",
        "\n",
        "            examples_so_far = batch_idx * len(data) + epoch * len(trainloader.dataset)\n",
        "            loss_logs.append((examples_so_far, loss.item()))\n",
        "\n",
        "    return loss_logs"
      ],
      "metadata": {
        "id": "2SZSsfzTySj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `test()` function evaluates the performance of a trained neural network on unseen data from the test set. It puts the network into **evaluation mode** to ensure consistent behavior by disabling layers like dropout, which are only active during training. The function then loops through the test data, performs a **forward pass** to generate predictions, and calculates both the **average loss** and **accuracy**. For fully connected networks, the input data must be **flattened into a 1D vector** before being passed to the network, but this step is unnecessary for **convolutional neural networks (CNNs)**, which process multi-dimensional image data directly. The function finally prints the results, helping us assess how well the model generalizes to new data."
      ],
      "metadata": {
        "id": "ubENYa9uExW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(network, flatten_data=True):\n",
        "    # Set the network to evaluation mode.\n",
        "    # This disables certain behaviors like dropout that are only used during training.\n",
        "    network.eval()\n",
        "\n",
        "    # Initialize test loss and correct predictions counters\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Disable gradient calculation for efficiency since we are only testing the model.\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # For fully connected networks, flatten the image data into a 1D vector.\n",
        "            # CNNs expect multi-dimensional image inputs, so flattening is unnecessary for them.\n",
        "            if flatten_data:\n",
        "                data = flatten(data)\n",
        "\n",
        "            # Move data and target to the GPU if available\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Perform a forward pass through the network\n",
        "            output = network(data)\n",
        "\n",
        "            # Calculate the cumulative test loss using Negative Log Likelihood loss\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "\n",
        "            # Get the predicted class for each input sample\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "            # Count the number of correct predictions\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    # Calculate the average test loss by dividing by the number of samples\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Calculate the test accuracy as a percentage\n",
        "    accuracy = round(100. * correct.item() / len(test_loader.dataset), 2)\n",
        "\n",
        "    # Print the test loss and accuracy\n",
        "    print(f'\\nTest set: Avg loss {round(test_loss, 4)}, Accuracy {accuracy}%\\n')\n",
        "\n",
        "    # Return the test loss and accuracy for logging purposes\n",
        "    return test_loss, accuracy"
      ],
      "metadata": {
        "id": "by-BsEV0ynJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Define a Convolutional Neural Network\n",
        "\n",
        "Tasks:\n",
        "1. Define a convolutional network with the following architecture.\n",
        "\n",
        "  - Define the CNN layers.\n",
        "    \n",
        "    A. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to define the second convolutional layer using `nn.Conv2d()`.\n",
        "      - Input channels = 1 (grayscale image)\n",
        "      - Output channels = 10\n",
        "      - Kernel size = 5\n",
        "    \n",
        "    B. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to define the second convolutional layer using `nn.Conv2d()`.\n",
        "      - Input channels = 10 (from the previous layer's output)\n",
        "      - Output channels = 20\n",
        "      - Kernel size = 5\n",
        "    \n",
        "    C. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to add a dropout layer using `nn.Dropout2d()`.\n",
        "    \n",
        "    D. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to define the first fully connected layer using `nn.Linear()`.\n",
        "      - Input features = 320 (flattened feature maps)\n",
        "      - Output features = 50\n",
        "    \n",
        "    E. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to define the second fully connected layer using `nn.Linear()`.\n",
        "      - Input features = 50 (from the previous fully connected layer)\n",
        "      - Output features = 10 (number of classes)\n",
        "    \n",
        "  - Define the forward pass of the CNN.\n",
        "\n",
        "    F. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply the first convolutional layer followed by ReLU activation and 2x2 max pooling.\n",
        "    \n",
        "    G. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply the second convolutional layer followed by a dropout layer.\n",
        "    \n",
        "    H. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply 2x2 max pooling to the output of the second convolutional layer.\n",
        "\n",
        "    I. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply a ReLU activation to introduce non-linearity\n",
        "    \n",
        "    J. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply a fully connected layer with a ReLU activation.\n",
        "\n",
        "    K. Replace `<REPLACE ME AND MY ARROWS>` with the correct code to apply a fully connected layer."
      ],
      "metadata": {
        "id": "pZNJ_Rk17XFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Define the convolutional neural network (CNN) architecture\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # First convolutional layer:\n",
        "        # - 1 input channel (grayscale image)\n",
        "        # - 10 output channels (feature maps)\n",
        "        # - 5x5 kernel size (learns 5x5 filters to detect patterns)\n",
        "        self.conv1 = <REPLACE ME AND MY ARROWS> # QUESTION A\n",
        "\n",
        "        # Second convolutional layer:\n",
        "        # - Takes 10 input feature maps from conv1\n",
        "        # - Produces 20 output feature maps\n",
        "        # - 5x5 kernel size\n",
        "        self.conv2 = <REPLACE ME AND MY ARROWS> # QUESTION B\n",
        "\n",
        "        # Dropout layer to reduce overfitting by randomly dropping units during training\n",
        "        self.conv2_drop = <REPLACE ME AND MY ARROWS> # QUESTION C\n",
        "\n",
        "        # First fully connected layer:\n",
        "        # - 320 input features (flattened feature maps after convolutions)\n",
        "        # - 50 output features (reduces dimensionality for classification)\n",
        "        self.fc1 = <REPLACE ME AND MY ARROWS> # QUESTION D\n",
        "\n",
        "        # Second fully connected layer:\n",
        "        # - 50 input features from the previous layer\n",
        "        # - 10 output features corresponding to the number of classes\n",
        "        self.fc2 = <REPLACE ME AND MY ARROWS> # QUESTION E\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional layer followed by:\n",
        "        # - ReLU activation (introduces non-linearity)\n",
        "        # - Max pooling (reduces dimensionality by taking the max value from 2x2 regions)\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION F\n",
        "\n",
        "        # Second convolutional layer followed by dropout:\n",
        "        # - Helps reduce overfitting by randomly dropping out units\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION G\n",
        "\n",
        "        # Apply max pooling to reduce the feature map size\n",
        "        # - Uses a 2x2 pooling region\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION H\n",
        "\n",
        "        # Apply ReLU activation to introduce non-linearity\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION I\n",
        "\n",
        "        # Flatten the output to a 1D vector before passing it to fully connected layers\n",
        "        # - The input size is reshaped to (-1, 320), where -1 infers the batch size\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Apply the first fully connected layer followed by ReLU activation\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION J\n",
        "\n",
        "        # Apply dropout to the fully connected layer's output to reduce overfitting\n",
        "        # - Dropout is only applied during training\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Apply the second fully connected layer (output layer)\n",
        "        # - Produces 10 output values corresponding to class scores\n",
        "        x = <REPLACE ME AND MY ARROWS> # QUESTION K\n",
        "\n",
        "        # Return the log of softmax probabilities for classification\n",
        "        # - Converts the output scores into a categorical probability distribution\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "uFxIYxixDEuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Train a Convolutional Neural Network with MNIST\n",
        "\n",
        "Tasks:\n",
        "1. Set Up Your Environment and Define the Network\n",
        "  - Define key hyperparameters to control the training process.\n",
        "  - Create an instance of your CNN model using `ConvNet()`.\n",
        "  - Move the model to the appropriate device (CPU/GPU).\n",
        "  - Configure the optimizer using `optim.SGD()`.\n",
        "2. Train Your CNN Model.\n",
        "  - Train your model for the specified number of epochs using the `train()` function.\n",
        "3. Evaluate and Visualize the Model’s Performance.\n",
        "  - Use the `plot_results()` function to visualize:\n",
        "    - Training Loss and Test Loss Over Time\n",
        "    - Test Accuracy Over Epochs\n",
        "    - Analyze the plots to assess your model's performance."
      ],
      "metadata": {
        "id": "z_jeYdLc7d5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Prepare your Environment to Train your CNN"
      ],
      "metadata": {
        "id": "wcLsCdPBKg1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Define hyperparameters\n",
        "# Hyperparameters control the behavior of the training process.\n",
        "\n",
        "learning_rate = 0.01  # Controls how large the weight updates are during training.\n",
        "# A smaller learning rate leads to slower but more stable convergence.\n",
        "# A larger learning rate can speed up convergence but may cause instability.\n",
        "\n",
        "momentum = 0.9\n",
        "# Momentum helps accelerate convergence by adding a fraction of the previous update to the current update.\n",
        "# This helps the model avoid getting stuck in local minima and smooths the optimization process.\n",
        "\n",
        "n_epochs = 2\n",
        "# Number of complete passes through the training dataset during training.\n",
        "# Increasing the number of epochs can improve performance, but may lead to overfitting if too high.\n",
        "\n",
        "log_interval = 10\n",
        "# How often (in batches) we print the training progress.\n",
        "# For example, a log_interval of 10 means the network will print updates every 10 batches.\n",
        "\n",
        "# Initialize the convolutional network\n",
        "conv_network = <REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "# Move the network to the GPU if available\n",
        "<REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "# Define the optimizer\n",
        "# We're using SGD (Stochastic Gradient Descent) with the specified learning rate and momentum.\n",
        "conv_optimizer = <REPLACE ME AND MY BRACKETS>"
      ],
      "metadata": {
        "id": "sXAsonmtKhId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Train your Model"
      ],
      "metadata": {
        "id": "awx6GFwHIubr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Initialize lists to track test accuracy, test losses, and training losses\n",
        "test_accuracies, test_losses, train_losses = list(), list(), list()\n",
        "total_examples_seen = 0  # Tracks the total number of examples processed during training\n",
        "\n",
        "# Training loop over the specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Run the test phase before training each epoch to log initial performance\n",
        "    test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "\n",
        "    # Train the network on the training dataset for the current epoch\n",
        "    train_loss_logs = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "    # Log test performance\n",
        "    test_accuracies.append((epoch, test_accuracy))  # Log accuracy for this epoch\n",
        "    test_losses.append((total_examples_seen, test_loss))  # Log test loss\n",
        "\n",
        "    # Log training losses\n",
        "    train_losses.extend(train_loss_logs)\n",
        "\n",
        "    # Update the total number of examples seen\n",
        "    total_examples_seen = train_loss_logs[-1][0]\n",
        "\n",
        "# Final test phase after completing all epochs\n",
        "test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "test_accuracies.append((n_epochs, test_accuracy))\n",
        "test_losses.append((total_examples_seen, test_loss))\n",
        "\n",
        "# At the end of the training process, we have:\n",
        "# - test_accuracies: A list tracking the model's accuracy on the test set after each epoch.\n",
        "# - test_losses: A list tracking the test loss after each epoch.\n",
        "# - train_losses: A detailed log of the training loss over time."
      ],
      "metadata": {
        "id": "Fqw3hghMMHa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3: Assess the Performance of your Model"
      ],
      "metadata": {
        "id": "7hG1nqWzJo1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_results(test_accuracies, test_losses, train_losses):\n",
        "    \"\"\"\n",
        "    Plot training losses, test losses, and test accuracy over time.\n",
        "\n",
        "    Parameters:\n",
        "    - test_accuracies: List of tuples [(epoch, accuracy)], representing test accuracy over epochs.\n",
        "    - test_losses: List of tuples [(epoch, loss)], representing test loss over examples seen.\n",
        "    - train_losses: List of tuples [(example, loss)], representing training loss over examples seen.\n",
        "    \"\"\"\n",
        "    # --- Plot Training and Test Losses ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract values for training losses\n",
        "    train_x, train_y = zip(*train_losses)\n",
        "\n",
        "    # Extract values for test losses\n",
        "    test_x, test_y = zip(*test_losses)\n",
        "\n",
        "    # Plot training losses\n",
        "    plt.plot(train_x, train_y, label='Training Loss', color='b', linestyle='-', marker='.')\n",
        "\n",
        "    # Plot test losses\n",
        "    plt.plot(test_x, test_y, label='Test Loss', color='r', linestyle='--', marker='o')\n",
        "\n",
        "    # Formatting\n",
        "    plt.xlabel('Examples Seen')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.title('Training and Test Losses Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Test Accuracy ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract values for test accuracy\n",
        "    acc_x, acc_y = zip(*test_accuracies)\n",
        "\n",
        "    # Plot test accuracy\n",
        "    plt.plot(acc_x, acc_y, label='Test Accuracy', color='g', marker='o')\n",
        "\n",
        "    # Formatting\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    sns.despine()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "9aZ0qfy_L6HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Plot your Results using the function `plot_resolts\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "uqmXShBWMAMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendices"
      ],
      "metadata": {
        "id": "QDO9j_Bqx2y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning with CNNs"
      ],
      "metadata": {
        "id": "7dCz57XUMWbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Perform Hyperparameter Tuning to Improve Performance of your Convolutional Neural Network\n",
        "\n"
      ],
      "metadata": {
        "id": "v10kvsT1MfEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Define ranges for hyperparameters\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "momentums = [0.5, 0.9]\n",
        "batch_sizes = [32, 64, 128]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for lr, momentum, batch_size in product(learning_rates, momentums, batch_sizes):\n",
        "    print(f\"Training with lr={lr}, momentum={momentum}, batch_size={batch_size}\")\n",
        "\n",
        "    # Update train loader with the current batch size\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('sample_data/', train=True, download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                   ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize network\n",
        "    conv_network = ConvNet()\n",
        "    conv_network.to(device)\n",
        "\n",
        "    # Define optimizer\n",
        "    conv_optimizer = optim.SGD(conv_network.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "    # Train and test\n",
        "    test_accuracies, test_losses, train_losses = list(), list(), list()\n",
        "    total_examples_seen = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "        train_loss_logs = train(conv_network, conv_optimizer, epoch, train_loader, flatten_data=False)\n",
        "        test_accuracies.append((epoch, test_accuracy))\n",
        "        test_losses.append((total_examples_seen, test_loss))\n",
        "        train_losses.extend(train_loss_logs)\n",
        "        total_examples_seen = train_loss_logs[-1][0]\n",
        "    test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "\n",
        "    # Log the performance\n",
        "    results.append({\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_accuracy\n",
        "    })"
      ],
      "metadata": {
        "id": "TClwGyGEMp3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results sorted by accuracy\n",
        "sorted_results = sorted(results, key=lambda x: x[\"test_accuracy\"], reverse=True)\n",
        "for result in sorted_results:\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "njYWgNuAOENY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu9yjZpDTX0O"
      },
      "source": [
        "## Insight into Saving Models\n",
        "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPWUrSc3TX0O",
        "outputId": "704d33da-c96e-4ac9-a0ee-cd9e6dcdffd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(conv_network.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzkS8LwbTX0O"
      },
      "source": [
        "## Insight into Loading Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKibjThpTX0P",
        "outputId": "036894e9-b6b5-4291-f97f-1b1221267466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QY_UxH_TX0P"
      },
      "source": [
        "This model can now be used to make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haXjMkCxTX0P",
        "outputId": "0a25bb09-2ee4-4a54-ebd7-9421ed6f6d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"8\", Actual: \"8\"\n"
          ]
        }
      ],
      "source": [
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST('sample_data/',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "classes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    }
  ]
}