{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vpHm7R-_DOWm"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day5_cnns_inherent_feature_engineering_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
      ],
      "metadata": {
        "id": "EU7YJnzI7Ne4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aABmoeonN2t"
      },
      "source": [
        "# Explore the Inherent Feature Engineering of Convolutional Neural Networks with Pytorch and MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==============================================\n",
        "# 0. Module imports\n",
        "# ==============================================\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import scipy\n",
        "\n",
        "# plots\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "\n",
        "# classification algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# dimension reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection\n",
        "\n",
        "# parallel processing\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "# model evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = \"cpu\""
      ],
      "metadata": {
        "id": "vv29cylo7Ne5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncF7Q1R7Ne5"
      },
      "source": [
        "Some visualization code. No need to understand this cell, but don't forget to run it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvCnQKml7Ne5",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "from graphviz import Digraph\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
        "def visualize(model):\n",
        "    hidden_layers = []\n",
        "    output_layer = 0\n",
        "    layers = [layer for layer in model.modules() if type(layer) == torch.nn.Linear]\n",
        "\n",
        "    # custom script to get all the shapes of a network's layers\n",
        "    sizes = list()\n",
        "    state_dict = net.state_dict()\n",
        "    for idx, key in enumerate(state_dict.keys()):\n",
        "      if key.endswith('weight'):\n",
        "        output_size, input_size = state_dict[key].shape\n",
        "        if idx == 0:\n",
        "          sizes.append(input_size)\n",
        "        sizes.append(output_size)\n",
        "\n",
        "    input_layer = sizes[0]\n",
        "    for layer in layers:\n",
        "        if layer == layers[0]:\n",
        "            hidden_layers.append(layer.out_features)\n",
        "\n",
        "        else:\n",
        "            if layer == layers[-1]:\n",
        "                output_layer = layer.out_features\n",
        "            else:\n",
        "                hidden_layers.append(layer.out_features)\n",
        "\n",
        "        last_layer_nodes = input_layer\n",
        "        nodes_up = input_layer\n",
        "\n",
        "    g = Digraph(\"g\", filename='visualization_tmp')\n",
        "    n = 0\n",
        "    g.graph_attr.update(splines=\"false\", nodesep=\"0.5\", ranksep=\"0\", rankdir='LR')\n",
        "    # Input Layer\n",
        "    with g.subgraph(name=\"cluster_input\") as c:\n",
        "          c.attr(color=\"white\")\n",
        "          for i in range(0, sizes[0]):\n",
        "              n += 1\n",
        "              c.node(str(n))\n",
        "              c.attr(labeljust=\"1\")\n",
        "              c.attr(label=\"Input Layer\", labelloc=\"bottom\")\n",
        "              c.attr(rank=\"same\")\n",
        "              c.node_attr.update(\n",
        "                  width=\"0.65\",\n",
        "                  style=\"filled\",\n",
        "                  shape=\"circle\",\n",
        "                  color=HAPPY_COLORS_PALETTE[3],\n",
        "                  fontcolor=HAPPY_COLORS_PALETTE[3],\n",
        "              )\n",
        "    for layer_idx in range(0, len(sizes) - 2):\n",
        "        with g.subgraph(name=\"cluster_\" + str(layer_idx + 1)) as c:\n",
        "            c.attr(color=\"white\")\n",
        "            c.attr(rank=\"same\")\n",
        "            label = f'Hidden Layer {layer_idx + 1}'\n",
        "            c.attr(labeljust=\"right\", labelloc=\"b\", label=label)\n",
        "            for j in range(0, hidden_layers[layer_idx]):\n",
        "                n += 1\n",
        "                c.node(\n",
        "                    str(n),\n",
        "                    width=\"0.65\",\n",
        "                    shape=\"circle\",\n",
        "                    style=\"filled\",\n",
        "                    color=HAPPY_COLORS_PALETTE[0],\n",
        "                    fontcolor=HAPPY_COLORS_PALETTE[0],\n",
        "                )\n",
        "                for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
        "                    g.edge(str(h), str(n))\n",
        "            last_layer_nodes = hidden_layers[layer_idx]\n",
        "            nodes_up += hidden_layers[layer_idx]\n",
        "\n",
        "    with g.subgraph(name=\"cluster_output\") as c:\n",
        "        c.attr(color=\"white\")\n",
        "        c.attr(rank=\"same\")\n",
        "        c.attr(labeljust=\"1\")\n",
        "        for i in range(1, output_layer + 1):\n",
        "            n += 1\n",
        "            c.node(\n",
        "                str(n),\n",
        "                width=\"0.65\",\n",
        "                shape=\"circle\",\n",
        "                style=\"filled\",\n",
        "                color=HAPPY_COLORS_PALETTE[4],\n",
        "                fontcolor=HAPPY_COLORS_PALETTE[4],\n",
        "\n",
        "            )\n",
        "            for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
        "                g.edge(str(h), str(n))\n",
        "        c.attr(label=\"Output Layer\", labelloc=\"bottom\")\n",
        "        c.node_attr.update(\n",
        "            color=\"#2ecc71\", style=\"filled\", fontcolor=\"#2ecc71\", shape=\"circle\"\n",
        "        )\n",
        "\n",
        "    g.attr(arrowShape=\"none\")\n",
        "    g.edge_attr.update(arrowhead=\"none\", color=\"#707070\", penwidth=\"2\")\n",
        "    g.view()\n",
        "    return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore MNIST"
      ],
      "metadata": {
        "id": "X3rFiFZaDIol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define DataLoaders"
      ],
      "metadata": {
        "id": "vpHm7R-_DOWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Batch Sizes\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "# Training DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('sample_data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "# Testing DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('sample_data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "YEcN-Wd8DOhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Simple Convolutional Network\n",
        "\n",
        "The `train()` function is responsible for updating the network's parameters during the **training phase** using data from the training set. It sets the network to **training mode**, enabling layers like dropout that improve generalization. For **fully connected networks**, the input data is flattened into a 1D vector before being passed to the network, but this step is **skipped for convolutional neural networks (CNNs)**, which handle multi-dimensional inputs directly. During each batch, the function performs a **forward pass** to make predictions, calculates the **loss** using the Negative Log Likelihood function, and performs **backpropagation** to update the network’s weights. The function tracks and logs the **training progress** to monitor performance over time."
      ],
      "metadata": {
        "id": "XGuL1p-OZkEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: network.train() puts the network into \"nondeterministic mode\".\n",
        "# This doesn't matter for the fully connected network, but it will matter for convolutional networks.\n",
        "# CNNs rely on preserving spatial structure, so the nondeterministic mode can impact results.\n",
        "def train(network, optimizer, epoch, trainloader, flatten_data=True):\n",
        "\n",
        "    network.train()  # Set the network to training mode\n",
        "    loss_logs = list()  # Track the loss over time\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        # For fully connected networks, we flatten the image data to feed it into the network.\n",
        "        # CNNs, however, expect multi-dimensional image inputs, so we skip flattening.\n",
        "        if flatten_data:\n",
        "            data = flatten(data)  # Flatten the image (only for fully connected networks)\n",
        "\n",
        "        # Move data to GPU if available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform the forward pass through the network\n",
        "        output = network(data)\n",
        "\n",
        "        # Compute the loss using the Negative Log Likelihood loss function\n",
        "        # This loss function is often used for classification tasks with multiple classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Backpropagation: Calculate the gradients of the loss with respect to the network's parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform the optimization step: Update the network's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress at regular intervals\n",
        "        if batch_idx % log_interval == 0:\n",
        "            examples_shown = batch_idx * len(data)\n",
        "            total_examples = len(trainloader.dataset)\n",
        "            fraction_shown = round(examples_shown / total_examples * 100.0, 2)\n",
        "            rounded_loss = round(loss.item(), 4)\n",
        "            print(f'Train Epoch {epoch} Progress: {fraction_shown}%\\tLoss: {rounded_loss}')\n",
        "\n",
        "            examples_so_far = batch_idx * len(data) + epoch * len(trainloader.dataset)\n",
        "            loss_logs.append((examples_so_far, loss.item()))\n",
        "\n",
        "    return loss_logs"
      ],
      "metadata": {
        "id": "2SZSsfzTySj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `test()` function evaluates the performance of a trained neural network on unseen data from the test set. It puts the network into **evaluation mode** to ensure consistent behavior by disabling layers like dropout, which are only active during training. The function then loops through the test data, performs a **forward pass** to generate predictions, and calculates both the **average loss** and **accuracy**. For fully connected networks, the input data must be **flattened into a 1D vector** before being passed to the network, but this step is unnecessary for **convolutional neural networks (CNNs)**, which process multi-dimensional image data directly. The function finally prints the results, helping us assess how well the model generalizes to new data."
      ],
      "metadata": {
        "id": "ubENYa9uExW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(network, flatten_data=True):\n",
        "    # Set the network to evaluation mode.\n",
        "    # This disables certain behaviors like dropout that are only used during training.\n",
        "    network.eval()\n",
        "\n",
        "    # Initialize test loss and correct predictions counters\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Disable gradient calculation for efficiency since we are only testing the model.\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # For fully connected networks, flatten the image data into a 1D vector.\n",
        "            # CNNs expect multi-dimensional image inputs, so flattening is unnecessary for them.\n",
        "            if flatten_data:\n",
        "                data = flatten(data)\n",
        "\n",
        "            # Move data and target to the GPU if available\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Perform a forward pass through the network\n",
        "            output = network(data)\n",
        "\n",
        "            # Calculate the cumulative test loss using Negative Log Likelihood loss\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "\n",
        "            # Get the predicted class for each input sample\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "\n",
        "            # Count the number of correct predictions\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    # Calculate the average test loss by dividing by the number of samples\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Calculate the test accuracy as a percentage\n",
        "    accuracy = round(100. * correct.item() / len(test_loader.dataset), 2)\n",
        "\n",
        "    # Print the test loss and accuracy\n",
        "    print(f'\\nTest set: Avg loss {round(test_loss, 4)}, Accuracy {accuracy}%\\n')\n",
        "\n",
        "    # Return the test loss and accuracy for logging purposes\n",
        "    return test_loss, accuracy"
      ],
      "metadata": {
        "id": "by-BsEV0ynJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Simple Convolutional Neural Network\n",
        "\n",
        "This is a **simple convolutional neural network (CNN)** model called `ConvNet`, designed to classify images into different categories. CNNs are widely used in image processing because they can automatically learn spatial features like edges, textures, and shapes from images. This example includes **two convolutional layers** followed by **fully connected layers** to map the learned features to class probabilities. Each layer in the network serves a specific purpose, such as feature extraction or classification, and the structure is designed to handle image data efficiently.\n",
        "\n",
        "The `forward()` method defines how the input image flows through the network — from convolutional layers for feature extraction to fully connected layers for decision-making. Below are detailed comments explaining each step in the network."
      ],
      "metadata": {
        "id": "pZNJ_Rk17XFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the convolutional neural network (CNN) architecture\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # First convolutional layer:\n",
        "        # - 1 input channel (grayscale image)\n",
        "        # - 10 output channels (feature maps)\n",
        "        # - 5x5 kernel size (learns 5x5 filters to detect patterns)\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "\n",
        "        # Second convolutional layer:\n",
        "        # - Takes 10 input feature maps from conv1\n",
        "        # - Produces 20 output feature maps\n",
        "        # - 5x5 kernel size\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "        # Dropout layer to reduce overfitting by randomly dropping units during training\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        # First fully connected layer:\n",
        "        # - 320 input features (flattened feature maps after convolutions)\n",
        "        # - 50 output features (reduces dimensionality for classification)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "\n",
        "        # Second fully connected layer:\n",
        "        # - 50 input features from the previous layer\n",
        "        # - 10 output features corresponding to the number of classes\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional layer followed by:\n",
        "        # - ReLU activation (introduces non-linearity)\n",
        "        # - Max pooling (reduces dimensionality by taking the max value from 2x2 regions)\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        # Second convolutional layer followed by dropout:\n",
        "        # - Helps reduce overfitting by randomly dropping out units\n",
        "        x = self.conv2_drop(self.conv2(x))\n",
        "\n",
        "        # Apply max pooling to reduce the feature map size\n",
        "        # - Uses a 2x2 pooling region\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Apply ReLU activation to introduce non-linearity\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Flatten the output to a 1D vector before passing it to fully connected layers\n",
        "        # - The input size is reshaped to (-1, 320), where -1 infers the batch size\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Apply the first fully connected layer followed by ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply dropout to the fully connected layer's output to reduce overfitting\n",
        "        # - Dropout is only applied during training\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Apply the second fully connected layer (output layer)\n",
        "        # - Produces 10 output values corresponding to class scores\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Return the log of softmax probabilities for classification\n",
        "        # - Converts the output scores into a categorical probability distribution\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "fEJeLt-oBX-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYZE: Train a Convolutional Neural Network with MNIST\n",
        "\n",
        "Tasks:\n",
        "1. Set the hyperparameters (`learning_rate`, `momentum`, `n_epochs`), and then run the following code. As the code is running, move through each line step by step, and try to understand the process of training a convolutional neural network.\n",
        "  - In general, this code trains a **convolutional neural network (CNN)** using the Stochastic Gradient Descent (SGD) optimization algorithm. The network is trained over multiple epochs, with each epoch consisting of a **training phase** and a **testing phase**. The **hyperparameters** such as learning rate, momentum, and number of epochs control how the training process progresses. After each epoch, the model’s performance on the test set is logged, tracking both **test accuracy** and **loss** over time.\n",
        "2. Assess the performance of the trained convolutional neural network using the provided function `plot_results(~)`. Would you expect thi performance s to be better than that of a standard feedforward neural network?"
      ],
      "metadata": {
        "id": "z_jeYdLc7d5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "##### TASK 1: Set your Hyperparameters\n",
        "\n",
        "- Set the hyperparameters (`learning_rate`, `momentum`, `n_epochs`), and then run the following code. As the code is running, move through each line step by step, and try to understand the process of training a convolutional neural network.\n",
        "  - In general, this code trains a **convolutional neural network (CNN)** using the Stochastic Gradient Descent (SGD) optimization algorithm. The network is trained over multiple epochs, with each epoch consisting of a **training phase** and a **testing phase**. The **hyperparameters** such as learning rate, momentum, and number of epochs control how the training process progresses. After each epoch, the model’s performance on the test set is logged, tracking both **test accuracy** and **loss** over time."
      ],
      "metadata": {
        "id": "q7VtXb9mMfq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Define hyperparameters\n",
        "# Hyperparameters control the behavior of the training process.\n",
        "\n",
        "learning_rate = <REPLACE ME AND MY ARROWS>  # Controls how large the weight updates are during training.\n",
        "# A smaller learning rate leads to slower but more stable convergence.\n",
        "# A larger learning rate can speed up convergence but may cause instability.\n",
        "\n",
        "momentum = <REPLACE ME AND MY ARROWS>\n",
        "# Momentum helps accelerate convergence by adding a fraction of the previous update to the current update.\n",
        "# This helps the model avoid getting stuck in local minima and smooths the optimization process.\n",
        "\n",
        "n_epochs = <REPLACE ME AND MY ARROWS>\n",
        "# Number of complete passes through the training dataset during training.\n",
        "# Increasing the number of epochs can improve performance, but may lead to overfitting if too high.\n",
        "\n",
        "log_interval = 10\n",
        "# How often (in batches) we print the training progress.\n",
        "# For example, a log_interval of 10 means the network will print updates every 10 batches.\n",
        "\n",
        "# Initialize the convolutional network\n",
        "conv_network = ConvNet()\n",
        "\n",
        "# Move the network to the GPU if available\n",
        "conv_network.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "# We're using SGD (Stochastic Gradient Descent) with the specified learning rate and momentum.\n",
        "conv_optimizer = optim.SGD(conv_network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "# Initialize lists to track test accuracy, test losses, and training losses\n",
        "test_accuracies, test_losses, train_losses = list(), list(), list()\n",
        "total_examples_seen = 0  # Tracks the total number of examples processed during training\n",
        "\n",
        "# Training loop over the specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Run the test phase before training each epoch to log initial performance\n",
        "    test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "\n",
        "    # Train the network on the training dataset for the current epoch\n",
        "    train_loss_logs = train(conv_network, conv_optimizer, epoch, train_loader, flatten_data=False)\n",
        "\n",
        "    # Log test performance\n",
        "    test_accuracies.append((epoch, test_accuracy))  # Log accuracy for this epoch\n",
        "    test_losses.append((total_examples_seen, test_loss))  # Log test loss\n",
        "\n",
        "    # Log training losses\n",
        "    train_losses.extend(train_loss_logs)\n",
        "\n",
        "    # Update the total number of examples seen\n",
        "    total_examples_seen = train_loss_logs[-1][0]\n",
        "\n",
        "# Final test phase after completing all epochs\n",
        "test_loss, test_accuracy = test(conv_network, flatten_data=False)\n",
        "test_accuracies.append((n_epochs, test_accuracy))\n",
        "test_losses.append((total_examples_seen, test_loss))\n",
        "\n",
        "# At the end of the training process, we have:\n",
        "# - test_accuracies: A list tracking the model's accuracy on the test set after each epoch.\n",
        "# - test_losses: A list tracking the test loss after each epoch.\n",
        "# - train_losses: A detailed log of the training loss over time."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "qOu46pHTMf9a",
        "outputId": "d620508f-9c55-41b0-f20f-aea30149b413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-34-df891b48807b>, line 6)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-df891b48807b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    learning_rate = <REPLACE ME AND MY ARROWS>  # Controls how large the weight updates are during training.\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Assess Performance of Trained Convolutional Neural Network with MNIST"
      ],
      "metadata": {
        "id": "KAro5sHl8VHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_results(test_accuracies, test_losses, train_losses):\n",
        "    \"\"\"\n",
        "    Plot training losses, test losses, and test accuracy over time.\n",
        "\n",
        "    Parameters:\n",
        "    - test_accuracies: List of tuples [(epoch, accuracy)], representing test accuracy over epochs.\n",
        "    - test_losses: List of tuples [(epoch, loss)], representing test loss over examples seen.\n",
        "    - train_losses: List of tuples [(example, loss)], representing training loss over examples seen.\n",
        "    \"\"\"\n",
        "    # --- Plot Training and Test Losses ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract values for training losses\n",
        "    train_x, train_y = zip(*train_losses)\n",
        "\n",
        "    # Extract values for test losses\n",
        "    test_x, test_y = zip(*test_losses)\n",
        "\n",
        "    # Plot training losses\n",
        "    plt.plot(train_x, train_y, label='Training Loss', color='b', linestyle='-', marker='.')\n",
        "\n",
        "    # Plot test losses\n",
        "    plt.plot(test_x, test_y, label='Test Loss', color='r', linestyle='--', marker='o')\n",
        "\n",
        "    # Formatting\n",
        "    plt.xlabel('Examples Seen')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.title('Training and Test Losses Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Test Accuracy ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract values for test accuracy\n",
        "    acc_x, acc_y = zip(*test_accuracies)\n",
        "\n",
        "    # Plot test accuracy\n",
        "    plt.plot(acc_x, acc_y, label='Test Accuracy', color='g', marker='o')\n",
        "\n",
        "    # Formatting\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    sns.despine()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LRGhFp7XKAa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Plot your Results using the function `plot_results`\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yCRdEj_zMuzN",
        "outputId": "21bffe92-85bf-4691-d3ad-9cabd8877ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ADD YOUR CODE HERE '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Learned in Convolutional Neural Networks?\n",
        "\n",
        "The following code contains a function called `preprocess_random_mnist_image` that extracts a random image from an MNIST-style dataset using a `DataLoader`, applies preprocessing transformations, and returns a ready-to-use tensor for model input.\n",
        "\n",
        "This function is designed to streamline the process of preparing an MNIST image for neural network inference by handling the following tasks:\n",
        "\n",
        "1. **Data Extraction**: It pulls image data from a `DataLoader` object, which typically wraps around an MNIST dataset.\n",
        "2. **Random Selection**: It selects a random image from the dataset, simulating a dynamic testing scenario.\n",
        "3. **Image Conversion**: The selected image is converted from a NumPy array to a PIL image, ensuring compatibility with common image transformation tools.\n",
        "4. **Preprocessing**: The image undergoes a series of transformations:\n",
        "   - Converted to a tensor using `transforms.ToTensor()`.\n",
        "   - Normalized using the standard mean and standard deviation of the MNIST dataset (`mean=0.1307`, `std=0.3081`).\n",
        "5. **Batch Formatting**: The processed tensor is reshaped to include a batch dimension, resulting in a tensor of shape `[1, 1, H, W]` — the expected input format for most deep learning models."
      ],
      "metadata": {
        "id": "nkVoysl1GI1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_random_mnist_image(data_loader, mnist_mean=0.1307, mnist_std=0.3081):\n",
        "    \"\"\"\n",
        "    Extracts a random image from a DataLoader, preprocesses it using MNIST dataset mean and std,\n",
        "    and returns a batch tensor ready for model input.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader: DataLoader containing the dataset (assumes MNIST format with .data attribute).\n",
        "    - mnist_mean: Mean value for normalization (default is MNIST mean 0.1307).\n",
        "    - mnist_std: Standard deviation for normalization (default is MNIST std 0.3081).\n",
        "\n",
        "    Returns:\n",
        "    - input_image: A torch tensor of shape [1, 1, H, W], ready for model input.\n",
        "    \"\"\"\n",
        "    # Extract dataset from the DataLoader\n",
        "    dataset = np.array(data_loader.dataset.data)  # Assuming the DataLoader is for MNIST\n",
        "    num_images = dataset.shape[0]\n",
        "\n",
        "    # Randomly select an image\n",
        "    image_idx = np.random.randint(low=0, high=num_images)\n",
        "    selected_image = dataset[image_idx]  # Shape: [H, W]\n",
        "\n",
        "    # Convert to PIL image for consistency\n",
        "    pil_image = Image.fromarray(selected_image)\n",
        "\n",
        "    # Define preprocessing transforms\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),       # Converts [H, W] to [1, H, W] and scales to [0, 1]\n",
        "        transforms.Normalize(mean=[mnist_mean], std=[mnist_std])  # Normalize using MNIST stats\n",
        "    ])\n",
        "\n",
        "    # Apply preprocessing\n",
        "    input_tensor = preprocess(pil_image)  # Shape: [1, H, W]\n",
        "\n",
        "    # Add batch dimension to match model input requirements\n",
        "    input_image = input_tensor.unsqueeze(0)  # Shape: [1, 1, H, W]\n",
        "\n",
        "    return input_image"
      ],
      "metadata": {
        "id": "C6EBFBTbIYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANALYZE: Explore What is Learned in CNNs\n",
        "\n",
        "Tasks:\n",
        "1. Visualize Convolutional Filters/Weights\n",
        "  - In this task, you will visualize the filters (weights) learned by the first convolutional layer of a trained CNN model. Filters represent the patterns that the model detects from the input images.\n",
        "    - Pass your trained model and the name of the first convolutional layer to the function, `visualize_filters_pytorch()`.\n",
        "2. Visualize Feature Maps\n",
        "  - Feature maps represent the intermediate outputs generated by the CNN at each layer after applying the filters to an input image.\n",
        "    - Preprocess a random image from the MNIST dataset using `preprocess_random_mnist_image()`.\n",
        "    - Call the function `visualize_feature_maps_pytorch()` to visualize the feature maps for a specific layer.\n",
        "3. Generate Saliency Maps\n",
        "  - Saliency maps highlight the most important pixels of an input image that influence the model's prediction for a specific class.\n",
        "    - Saliency maps highlight the most important pixels of an input image that influence the model's prediction for a specific class.\n",
        "    - Call the function `generate_saliency_map_pytorch()` to generate a saliency map for a given class index.\n",
        "4. Generate Class Activation Maps (CAMs)\n",
        "  - Class Activation Maps (CAMs) show which regions of an input image are most relevant to a specific class prediction by the model.\n",
        "    - Preprocess a random image from the MNIST dataset using `preprocess_random_mnist_image()`.\n",
        "    - Call the function `generate_cam_pytorch()` to generate a CAM for a given class index and layer."
      ],
      "metadata": {
        "id": "zuKa7A4RT-fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Convolutional Filters/Weights\n",
        "\n",
        "  - In this task, you will visualize the filters (weights) learned by the first convolutional layer of a trained CNN model. Filters represent the patterns that the model detects from the input images.\n",
        "    - Pass your trained model and the name of the first convolutional layer to the function, `visualize_filters_pytorch()`.\n"
      ],
      "metadata": {
        "id": "2Jb5diTVJEvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# FILTER VISUALIZATION\n",
        "# --------------------------------------------\n",
        "\n",
        "def visualize_filters_pytorch(model, layer_name):\n",
        "    \"\"\"\n",
        "    Visualize filters from a specific convolutional layer in a PyTorch model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model.\n",
        "    - layer_name: Name of the convolutional layer to visualize.\n",
        "    \"\"\"\n",
        "    # Extract the layer's weights\n",
        "    layer = dict(model.named_modules())[layer_name]\n",
        "    weights = layer.weight.data.cpu().numpy()\n",
        "\n",
        "    # Normalize the filters to 0-1 for visualization\n",
        "    filters = (weights - weights.min()) / (weights.max() - weights.min())\n",
        "\n",
        "    num_filters = filters.shape[0]\n",
        "    num_columns = 8\n",
        "    num_rows = int(np.ceil(num_filters / num_columns))\n",
        "\n",
        "    plt.figure(figsize=(num_columns * 2, num_rows * 2))\n",
        "    for i in range(num_filters):\n",
        "        plt.subplot(num_rows, num_columns, i + 1)\n",
        "        plt.imshow(filters[i, 0, :, :], cmap=\"viridis\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Filters in layer: {layer_name}\", fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QjFpoM_aFl0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Visualize Filters/Weights of your Trained Model\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "SpHbl8d9VxRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Feature Maps\n",
        "\n",
        "  - Feature maps represent the intermediate outputs generated by the CNN at each layer after applying the filters to an input image.\n",
        "    - Preprocess a random image from the MNIST dataset using `preprocess_random_mnist_image()`.\n",
        "    - Call the function `visualize_feature_maps_pytorch()` to visualize the feature maps for a specific layer."
      ],
      "metadata": {
        "id": "nTt7XmfmJAyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# FEATURE MAP VISUALIZATION\n",
        "# --------------------------------------------\n",
        "\n",
        "def visualize_feature_maps_pytorch(model, layer_name, input_image):\n",
        "    \"\"\"\n",
        "    Visualize feature maps from a specific layer in a PyTorch model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model.\n",
        "    - layer_name: Name of the layer to visualize.\n",
        "    - input_image: Input image tensor (should be preprocessed and have shape [1, C, H, W]).\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    hooks = []\n",
        "\n",
        "    # Hook to capture the output of the specified layer\n",
        "    def hook_fn(module, input, output):\n",
        "        hooks.append(output)\n",
        "\n",
        "    layer = dict(model.named_modules())[layer_name]\n",
        "    handle = layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Forward pass to get the feature maps\n",
        "    with torch.no_grad():\n",
        "        model(input_image)\n",
        "\n",
        "    # Detach the output and convert to numpy\n",
        "    feature_maps = hooks[0].detach().cpu().numpy()[0]\n",
        "\n",
        "    num_maps = feature_maps.shape[0]\n",
        "    num_columns = 8\n",
        "    num_rows = int(np.ceil(num_maps / num_columns))\n",
        "\n",
        "    plt.figure(figsize=(num_columns * 2, num_rows * 2))\n",
        "    for i in range(num_maps):\n",
        "        plt.subplot(num_rows, num_columns, i + 1)\n",
        "        plt.imshow(feature_maps[i, :, :], cmap=\"viridis\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Feature maps in layer: {layer_name}\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up the hook\n",
        "    handle.remove()"
      ],
      "metadata": {
        "id": "GOmmoJSfGqEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Preprocess an image from the `train_loader` using ``preprocess_random_mnist_image(~)`\n",
        "input_image = <REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "# Visualize feature maps\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "ia4rKi9vU2F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 3:  Saliency Maps\n",
        "\n",
        "  - Saliency maps highlight the most important pixels of an input image that influence the model's prediction for a specific class.\n",
        "    - Saliency maps highlight the most important pixels of an input image that influence the model's prediction for a specific class.\n",
        "    - Call the function `generate_saliency_map_pytorch()` to generate a saliency map for a given class index.\n"
      ],
      "metadata": {
        "id": "k9r1gotvI9J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# SALIENCY MAP GENERATION\n",
        "# --------------------------------------------\n",
        "\n",
        "def generate_saliency_map_pytorch(model, input_image, class_idx):\n",
        "    \"\"\"\n",
        "    Generate a saliency map for a given input image and class index.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model.\n",
        "    - input_image: Input image tensor (requires_grad=True, shape [1, C, H, W]).\n",
        "    - class_idx: Class index to compute saliency for.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_image.requires_grad_()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_image)\n",
        "    class_score = output[0, class_idx]\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    class_score.backward()\n",
        "\n",
        "    # Get the gradients and normalize\n",
        "    saliency = input_image.grad.data.abs().squeeze().cpu().numpy()\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
        "\n",
        "    # Plot the saliency map\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(saliency, cmap=\"hot\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Saliency Map\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rn-tFnDZGtil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Preprocess an image from the `train_loader` using ``preprocess_random_mnist_image(~)`\n",
        "input_image = <REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "# Visualize saliency maps\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "3k4Vt6F5WM33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 4: Class Activation Maps (CAM)\n",
        "\n",
        "  - Class Activation Maps (CAMs) show which regions of an input image are most relevant to a specific class prediction by the model.\n",
        "    - Preprocess a random image from the MNIST dataset using `preprocess_random_mnist_image()`.\n",
        "    - Call the function `generate_cam_pytorch()` to generate a CAM for a given class index and layer.\n"
      ],
      "metadata": {
        "id": "8fwOTDW8I4WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# CLASS ACTIVATION MAP (CAM) GENERATION\n",
        "# --------------------------------------------\n",
        "\n",
        "def generate_cam_pytorch(model, target_layer_name, input_image, class_idx):\n",
        "    \"\"\"\n",
        "    Generate a Class Activation Map (CAM) for a given input image and class index.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model.\n",
        "    - target_layer_name: Name of the target convolutional layer.\n",
        "    - input_image: Input image tensor (should be preprocessed and have shape [1, C, H, W]).\n",
        "    - class_idx: Class index to compute CAM for.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Store the feature maps and gradients\n",
        "    feature_maps = []\n",
        "    gradients = []\n",
        "\n",
        "    # Hook to capture the feature maps\n",
        "    def forward_hook(module, input, output):\n",
        "        feature_maps.append(output)\n",
        "\n",
        "    # Hook to capture the gradients\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        gradients.append(grad_out[0])\n",
        "\n",
        "    # Register hooks\n",
        "    target_layer = dict(model.named_modules())[target_layer_name]\n",
        "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
        "    backward_handle = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_image)\n",
        "    class_score = output[0, class_idx]\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    class_score.backward()\n",
        "\n",
        "    # Get feature maps and gradients\n",
        "    fmap = feature_maps[0].squeeze(0).detach().cpu().numpy()\n",
        "    grad = gradients[0].squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "    # Compute the weighted sum of feature maps\n",
        "    weights = np.mean(grad, axis=(1, 2))  # Global average pooling on gradients\n",
        "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
        "\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * fmap[i]\n",
        "\n",
        "    # Normalize CAM to 0-1\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
        "\n",
        "    # Plot the CAM\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cam, cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Class Activation Map (CAM) for Class {class_idx}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up hooks\n",
        "    forward_handle.remove()\n",
        "    backward_handle.remove()"
      ],
      "metadata": {
        "id": "MpBtXAdSJPGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 EXERCISE\n",
        "\n",
        "# Preprocess an image from the `train_loader` using ``preprocess_random_mnist_image(~)`\n",
        "input_image = <REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "# Visualize class activation maps\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "qWWiS1cRWkRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}