{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "NVb8ixyjMFd7",
        "wFlT3UJDIetO",
        "4M7DdWoSvsVH",
        "ZACCQ30IIetW",
        "IViRRp6IIetW"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day6_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIbri09H8lNn"
      },
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Transformers\n",
        "\n",
        "This notebook, modified from the Kaggle tutorial [Transformers from Scratch - DL](https://www.kaggle.com/code/auxeno/transformers-from-scratch-dl/notebook?scriptVersionId=137372275), is a step-by-step implementation of the transformer architecture from the seminal paper Attention Is All You Need https://arxiv.org/abs/1706.03762. We're only going to build the ***encoder block***, which correponds to the left side of the diagram.\n",
        "\n",
        "<img src=\"https://i.imgur.com/Iv0MBNZ.png\"\n",
        "     width=\"350\"\n",
        "     caption=\"Full Transformer Architecture\"/>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/mZRlDQP.png\"\n",
        "     align=\"right\"\n",
        "     width=\"300\" />\n",
        "     \n",
        "## The Encoder Only Architecture\n",
        "     \n",
        "The encoder on architecture has the following components:\n",
        "1. **Input embedding**: An embedding layer, where a tokenized input sequence is represented as a sequence of embedding vectors.\n",
        "2. **Positional encoding**: Provide the transformer with information about the position if each word in the sequence.\n",
        "3. **Multi-head self-attention**: The key component of the transformer. Builds on the concept of attention in RNNs from Bahdanau, Cho and Bengio 2014 https://arxiv.org/abs/1409.0473.\n",
        "4. **Residual connections and layer normalization**: Residual connections and layer normalization are applied multiple times in the transformer architecture. Residual connections came from He et al 2015 https://arxiv.org/abs/1512.03385 and layer normalization is considered to be an improvement on batch normalization, published in 2016 by Ba, Kiros and Hinton https://arxiv.org/abs/1607.06450.\n",
        "5. **Feed forward**: A special version of a 2 layer feed-forward neural network\n",
        "6. **Linear and softmax**: After applying layer normalization and skip connections a second time, our output is passed through a classification head consisting of one dense layer with a softmax output.\n",
        "7. **Output probabilities**: Our classification head outputs a probability distribution of our input belonging to each class."
      ],
      "metadata": {
        "id": "1iZQjCOUIetL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization\n",
        "Before we start working on our transformer architecture, we must first tokenize our text sequences.\n",
        "\n",
        "We're going to use subword tokenization, specifically WordPiece, which is used by the BERT and DistilBERT tokenizers.\n",
        "\n",
        "We will load the tokenizer from the Hugging Face Transformers library. Feel free to review this code to understand how tokenizers and word embeddings work"
      ],
      "metadata": {
        "id": "NVb8ixyjMFd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:26:43.922928Z",
          "iopub.execute_input": "2023-07-19T14:26:43.924008Z",
          "iopub.status.idle": "2023-07-19T14:26:47.67211Z",
          "shell.execute_reply.started": "2023-07-19T14:26:43.923951Z",
          "shell.execute_reply": "2023-07-19T14:26:47.670918Z"
        },
        "trusted": true,
        "id": "SZw754-JIetN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Tokenizing text is a core concept in NLP.'\n",
        "\n",
        "tokenized_text = {}\n",
        "tokenized_text['Numerical Token'] = tokenizer(text)['input_ids']\n",
        "tokenized_text['Token'] = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
        "\n",
        "print(\"Tokenizer has a vocabulary size of\", tokenizer.vocab_size, \"words.\")\n",
        "print(\"Tokenizer has a maximum sequence length of\", tokenizer.model_max_length, \"tokens.\")\n",
        "print(\"\\nOur text to tokenize:\", text, \"\\n\")\n",
        "pd.DataFrame(tokenized_text).T.style"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:26:47.674064Z",
          "iopub.execute_input": "2023-07-19T14:26:47.674556Z",
          "iopub.status.idle": "2023-07-19T14:26:47.764658Z",
          "shell.execute_reply.started": "2023-07-19T14:26:47.674525Z",
          "shell.execute_reply": "2023-07-19T14:26:47.763476Z"
        },
        "trusted": true,
        "id": "9Q8H29JzIetN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remarks:\n",
        "- `[CLS]` Stands for *classifier* and indicates the start of a sequence.\n",
        "- `[SEP]` Stands for *separate segment* and typically indicates the end of a sequence.\n",
        "- We can see that 'Tokenizing' and 'NLP' have had their casing changed to lower, and both words have been split into 'token' + 'izing' and 'nl' + 'p'.\n",
        "- The numerical indices each correspond to a word in the tokenizer's dictionary.\n",
        "\n",
        "We're also going to create a sample text string that we'll pass through our network piece by piece, to ensure everything is working as intended:"
      ],
      "metadata": {
        "id": "xm3Apyc3IetN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sample_text = \"We're going to reduce the maximum sequence length to 100 tokens, \\\n",
        "so we'll use a longer string here for demonstration purposes. We're not going to \\\n",
        "reach the full 100 tokens, so we'll pad our sequence with 0s.\"\n",
        "\n",
        "input_sequence = tokenizer.encode_plus(sample_text, return_tensors='pt', padding='max_length', truncation=True, max_length=100)['input_ids']\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_sequence = input_sequence.to(device)\n",
        "\n",
        "print(input_sequence)\n",
        "print(\"\\nShape of output:\", input_sequence.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:32:31.031825Z",
          "iopub.execute_input": "2023-07-19T14:32:31.03255Z",
          "iopub.status.idle": "2023-07-19T14:32:31.044756Z",
          "shell.execute_reply.started": "2023-07-19T14:32:31.03251Z",
          "shell.execute_reply": "2023-07-19T14:32:31.043362Z"
        },
        "trusted": true,
        "id": "fi-c2UYIIetN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Embedding\n",
        "\n",
        "<img src=\"https://i.imgur.com/h9XnkhD.png\"\n",
        "     align=\"right\"\n",
        "     width=\"175\" />\n",
        "\n",
        "Our transformer architecture starts off with an embedding layer, where we represent each word as a vector in an embedding space.\n",
        "\n",
        "### Implementing an Embedding Layer in PyTorch\n",
        "We initialize our config dictionary which we will use throughout the program to define the parameters of our model."
      ],
      "metadata": {
        "id": "wFlT3UJDIetO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a lightweight config class that allows for struct like attribute accessing\n",
        "class Config:\n",
        "    def __init__(self, config_dict):\n",
        "        self.__dict__.update(config_dict)\n",
        "\n",
        "# Setting parameters for our model\n",
        "config = {# We get the vocabulary size used by our tokenizer\n",
        "          'vocab_size': tokenizer.vocab_size,\n",
        "\n",
        "          # We will use 128 dimensional token embeddings initially\n",
        "          'embedding_dimensions': 128,\n",
        "\n",
        "          # We're only going to use a maximum of 100 tokens per input sequence\n",
        "          'max_tokens': 100,\n",
        "\n",
        "          # Number of attention heads to be used\n",
        "          'num_attention_heads': 8,\n",
        "\n",
        "          # Dropout on feed-forward network\n",
        "          'hidden_dropout_prob': 0.3,\n",
        "\n",
        "          # Number of neurons in the intermediate hidden layer (quadruple the number of emb dims)\n",
        "          'intermediate_size': 128 * 4,\n",
        "\n",
        "          # How many encoder blocks to use in our architecture\n",
        "          'num_encoder_layers': 2,\n",
        "\n",
        "          # Device\n",
        "          'device': device\n",
        "\n",
        "}\n",
        "# Wrapping our config dict with the lightweight class\n",
        "config = Config(config)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:32:34.570461Z",
          "iopub.execute_input": "2023-07-19T14:32:34.570843Z",
          "iopub.status.idle": "2023-07-19T14:32:34.579645Z",
          "shell.execute_reply.started": "2023-07-19T14:32:34.570809Z",
          "shell.execute_reply": "2023-07-19T14:32:34.578318Z"
        },
        "trusted": true,
        "id": "76VvpSvOIetO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Create an embedding layer, with ~32,000 possible embeddings, each having 128 dimensions\n",
        "        self.token_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_dimensions)\n",
        "\n",
        "    def forward(self, tokenized_sentence):\n",
        "        return self.token_embedding(tokenized_sentence)\n",
        "\n",
        "token_embedding = TokenEmbedding(config).to(config.device)\n",
        "embedding_output = token_embedding(input_sequence)\n",
        "print(\"Shape of output:\", embedding_output.size())"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:32:37.010199Z",
          "iopub.execute_input": "2023-07-19T14:32:37.010586Z",
          "iopub.status.idle": "2023-07-19T14:32:37.072492Z",
          "shell.execute_reply.started": "2023-07-19T14:32:37.010552Z",
          "shell.execute_reply": "2023-07-19T14:32:37.071278Z"
        },
        "trusted": true,
        "id": "mO-83AchIetO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remarks:\n",
        "- Our embedding layer ouputs a tensor of shape `[batch_size, seq_length, embedding_dims]`, which is what we expected.\n",
        "\n",
        "Next we will add the positional encodings to our embeddings:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1WAW0Ja3IetO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE: Positional Encodings\n",
        "\n",
        "<img src=\"https://i.imgur.com/jCnm8lE.png\"\n",
        "     align=\"right\"\n",
        "     width=\"175\" />\n",
        "\n",
        "Since transformers inherently lack information about the order of an input sequence, positional encodings provide the transformer model with information about the position of words within a sentence. They serve as a \"fingerprint\" that enables the transformer to identify the true position of each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ol3kyEIMOSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 1: Calculating Positional Encodings\n",
        "\n",
        "In 'Attention is all you need' , positional encodings are calculated with the following formulae:\n",
        "\n",
        "$$PE(position, i) = sin \\left( \\frac{position}{10000^{\\frac{i}{d_{model}}}}\\right) \\textrm{for even i}$$\n",
        "$$PE(position, i) = cos \\left( \\frac{position}{10000^{\\frac{i-1}{d_{model}}}}\\right) \\textrm{for odd i}$$\n",
        "\n",
        "Here, $i$ is the dimension and $d_{model}$ is the total number of dimensions in the token embedding. $position$ denotes the position of the word in the sentence.\n",
        "\n",
        "The choice of sine and cosine functions is strategic, as these trigonometric functions produce values within the range of -1 to 1, which align well with the requirements of neural network inputs. Additionally their periodicity allows for easy extrapolation for sentences of varying lengths."
      ],
      "metadata": {
        "id": "eyEh_CKvMSGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of possible positions (sequence length)\n",
        "pos = config.max_tokens\n",
        "\n",
        "# Number of latent space dimensions\n",
        "d_model = config.embedding_dimensions\n",
        "\n",
        "# Initialize matrix for positional encoding\n",
        "pe = np.zeros((pos, d_model))\n",
        "\n",
        "# Calculate positional encoding\n",
        "for pos in range(pos):\n",
        "    for i in range(0, d_model, 2):\n",
        "      # Calculate PE for even i with np.sin\n",
        "      pe[pos, i] = ''' ADD YOUR CODE HERE '''\n",
        "\n",
        "      # Calculate PE for odd i with np.cos\n",
        "      if i + 1 < d_model:\n",
        "      pe[pos, i + 1] = ''' ADD YOUR CODE HERE '''\n",
        "\n",
        "\n",
        "# Display as a heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(pe, cmap='coolwarm_r')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Dimension')\n",
        "plt.ylabel('Position in Sequence')\n",
        "plt.title('Positional Encoding Heatmap')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:32:48.178495Z",
          "iopub.execute_input": "2023-07-19T14:32:48.1791Z",
          "iopub.status.idle": "2023-07-19T14:32:48.656983Z",
          "shell.execute_reply.started": "2023-07-19T14:32:48.179051Z",
          "shell.execute_reply": "2023-07-19T14:32:48.655929Z"
        },
        "trusted": true,
        "id": "-E9zd36gIetP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Positional Encodings in PyTorch\n",
        "Run this code to implement positional encodings for our Transformer architecture"
      ],
      "metadata": {
        "id": "8ys9iBqqIetP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(config.max_tokens, config.embedding_dimensions)\n",
        "        position = torch.arange(0, config.max_tokens, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = 1 / (10000 ** (torch.arange(0, config.embedding_dimensions, 2).float() / config.embedding_dimensions))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.pe = self.pe.to(config.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, 0]\n",
        "\n",
        "\n",
        "# Confirm this module is working as intended\n",
        "positional_encoding = PositionalEncoding(config).to(config.device)\n",
        "pos_enc_output = positional_encoding(embedding_output)\n",
        "print(\"Shape of output:\", pos_enc_output.size())\n",
        "\n",
        "# View the difference between the two layers\n",
        "# These differences can be checked with our heatmap visualization\n",
        "diff = pos_enc_output - embedding_output\n",
        "print(\"\\nTensor at position 0 first 20 values:\")\n",
        "print(diff[0,0][:20])\n",
        "print(\"\\nTensor at position 0 last 20 values:\")\n",
        "print(diff[0,0][-20:])\n",
        "\n",
        "print(\"\\nTensor at position 50 first 20 values:\")\n",
        "print(diff[0,50][:20])\n",
        "print(\"\\nTensor at position 50 last 20 values:\")\n",
        "print(diff[0,50][-20:])"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:33:32.851352Z",
          "iopub.execute_input": "2023-07-19T14:33:32.852166Z",
          "iopub.status.idle": "2023-07-19T14:33:32.917459Z",
          "shell.execute_reply.started": "2023-07-19T14:33:32.852125Z",
          "shell.execute_reply": "2023-07-19T14:33:32.916252Z"
        },
        "trusted": true,
        "id": "n2rzlpC4IetP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE: Attention\n",
        "\n",
        "<img src=\"https://i.imgur.com/hsH3PKz.png\"\n",
        "     align=\"right\"\n",
        "     width=\"175\" />\n",
        "\n",
        "Whilst not literally being 'all you need', multi-headed self-attention is probably the most revolutionary feature that came from transformers, so let's explore what it is.\n",
        "\n"
      ],
      "metadata": {
        "id": "1nDJycOpMuf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention\n",
        "For each position in the sequence, the self-attention mechanism computes relevance scores with all other positions in the same sequence. This allows each position to be updated based on all other positions.\n",
        "\n",
        "Self-attention operates on all positions of the sequence simultaneously, making it highly *parallelizable* and more efficient on modern hardware accelerators. Each position in the sequence can be updated independently of the others, making self-attention a natural fit for 'transformer'-style models that avoid recurrent computations.\n",
        "\n",
        "## Query Key and Value Vectors\n",
        "During the computation of attention weights, we first project our token embedding vectors into three new vectors, called *query, key and value*.\n",
        "\n",
        "Each embedding vector will have its own associated query, key and value vectors associated with it.\n",
        "\n",
        "- **Query Vectors**: the \"question\" being asked. When we want to pay attention to specific information in the context, we do so in relation to a particular query.\n",
        "\n",
        "- **Key Vectors**: the \"address\" in the memory that matches a given query. These keys are compared with the query to determine how much attention should be paid to their corresponding values.\n",
        "\n",
        "- **Value Vectors**: the actual \"content\" in the memory that we want to focus on or retrieve.\n",
        "\n"
      ],
      "metadata": {
        "id": "VMER96n_M0z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TASK 1: Implementing Self-Attention\n",
        "1. Scaled dot product attention: A helper function that scales the value by the dot product of the query and key using Torch matrix operations.\n",
        "\n",
        "$$\n",
        "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "where\n",
        "- $Q,K,V \\in n \\times d_k$\n",
        "\n",
        "  Note that the query, key, and value vectors are also indexed by batch. $d_k$ is the embedding dimension and $n$ is the input sequence length.\n",
        "\n",
        "2. Attention head: A single attention head class."
      ],
      "metadata": {
        "id": "ktNL-yTfL5_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "    b, n, d_k = query.size()\n",
        "    # Compute attention scores\n",
        "    attn_scores = ''' ADD YOUR CODE HERE ''' # (b,n,d_k) @ (b,d_k,n) ->  (b,n,n)\n",
        "\n",
        "    # Apply softmax (F.softmax) to attention scores\n",
        "    attn_weights = ''' ADD YOUR CODE HERE ''' # (b,n,n)\n",
        "\n",
        "    # weighted aggregation\n",
        "    attn_output = ''' ADD YOUR CODE HERE ''' # (b,n,n) @ (b,n,d_k) -> (b,n,d_k)\n",
        "\n",
        "    return attn_output\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_dim)\n",
        "        self.k = nn.Linear(embed_dim, head_dim)\n",
        "        self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attn_outputs = scaled_dot_product_attention(self.q(hidden_state),\n",
        "                                                    self.k(hidden_state),\n",
        "                                                    self.v(hidden_state))\n",
        "        return attn_outputs\n",
        "\n",
        "# Example usage\n",
        "batch, seq_length, d_model = 2, 4, 8\n",
        "X_np = np.random.randn(batch, seq_length, d_model)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "Q = torch.from_numpy(X_np).float()\n",
        "K = torch.from_numpy(X_np).float()\n",
        "V = torch.from_numpy(X_np).float()\n",
        "\n",
        "output = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Self-Attention Output Shape:\", output.shape)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "G5oNAUqWIetQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 2: Multi-Head Self-Attention\n",
        "Multi-head self-attention simply allows our model to use several of these self-attention heads at once. This allows the attention-heads to divide labor and learn to pay attention to different aspects of the sentence. This division of labor permits for a richer understanding of context in the sentence than using just a single head would.\n",
        "\n",
        "Each for attention head $h$:\n",
        "$$Z^h = Attention(Q^h, K^h, V^h) \\in \\mathbb{R}^{n \\times d_k}$$\n",
        "\n",
        "\n",
        "Multi-head self-attention involves concatanating all $H$ attention heads and applying an output linear layer:\n",
        "$$MultiHead(X) = Concat(Z^1, \\ldots, Z^h)W^O $$\n",
        "where\n",
        "- $W_O \\in H*d_k \\times d$\n",
        "- $d_k$ is the dimension of the attention head space\n",
        "- $d$ is the embedding dimension.\n",
        "\n",
        "In the example below, $d = d_k$."
      ],
      "metadata": {
        "id": "th3CdnPgIetQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embedding_dimensions\n",
        "        num_heads = config.num_attention_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # Concatenate the attention heads together with torch.cat\n",
        "        x = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "        # Apply the output linear layer\n",
        "        x = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "        return x\n",
        "\n",
        "multihead_attn = MultiHeadAttention(config).to(config.device)\n",
        "atn_output = multihead_attn(pos_enc_output)\n",
        "print(\"Shape of output:\", atn_output.size())\n",
        "print(\"Number of heads:\", len(MultiHeadAttention(config).heads))\n",
        "multihead_attn.heads[:2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:33:52.088481Z",
          "iopub.execute_input": "2023-07-19T14:33:52.089277Z",
          "iopub.status.idle": "2023-07-19T14:33:54.329891Z",
          "shell.execute_reply.started": "2023-07-19T14:33:52.089235Z",
          "shell.execute_reply": "2023-07-19T14:33:54.328772Z"
        },
        "trusted": true,
        "id": "bM2C1zahIetQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remarks:\n",
        "- The shape of our output must have 8 heads, each with 16 output features such that the concatenation of all sums to 128.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "P0Lcbg5oPH3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE: Residual Connections and Layer Normalization\n",
        "\n",
        "The next step is to apply residual connections and layer normalization to our outputs. Let's look at both components.\n",
        "\n",
        "<img src=\"https://i.imgur.com/Uipv4De.png\"\n",
        "     align=\"right\"\n",
        "     width=\"175\" />\n",
        "\n",
        "## Residual Connections\n",
        "Residual connections, also known as skip or shortcut connections, allow gradients to flow directly through a few layers by having the original input added to the output of a block of layers.\n",
        "\n",
        "Residual connections allow us to train deeper models by reducing the problem ofvanishing gradients. As the network gets deeper, gradients calculated during backpropagation become increasingly small, a problem known as *vanishing gradients*.\n",
        "\n",
        "## Layer Normalization\n",
        "Layer Normalization is a type of normalization technique, similar to Batch Normalization but with different properties.\n",
        "\n",
        "The core idea of Layer Normalization is to normalize the values of each feature vector in a batch independently, making the average of their means and variances zero and one, respectively. The primary advantage of Layer Normalization is that it works independently of batch size. This is an important feature for Transformer models, which tend to use smaller batch sizes due to memory constraints."
      ],
      "metadata": {
        "id": "dw2NW0sUIetQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 1: Residual Connections and Layer Normalization in Transformers\n",
        "\n",
        "Each sub-layer in the Transformer (self-attention, point-wise feed-forward) has a residual connection around it followed by layer normalization. The output of each sub-layer is simply `LayerNorm(x + Sublayer(x))`, where Sublayer(x) is the function implemented by the sub-layer itself."
      ],
      "metadata": {
        "id": "-L-gBgwqQXZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "# First, we get our positional encoder output\n",
        "x = pos_enc_output\n",
        "\n",
        "# Instantiate the layer normalization\n",
        "layer_norm = nn.LayerNorm(config.embedding_dimensions).to(config.device)\n",
        "\n",
        "# Our output is then defined as the normalized output of the attention block\n",
        "# plus the ouput of the positional encoder (skip connection)\n",
        "add_norm_output = '''ADD YOUR CODE HERE'''\n",
        "\n",
        "print(\"Shape of output:\", add_norm_output.size())"
      ],
      "metadata": {
        "id": "b30S44CiQbn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE: Feed-Forward Network\n",
        "\n",
        "<img src=\"https://i.imgur.com/FxIFK65.png\"\n",
        "     align=\"right\"\n",
        "     width=\"175\" />\n",
        "     \n",
        "The next component we encounter is the *Position-Wise* Feed-Forward Network (FFN), otherwise known as Multi-layer Perception (MLP).\n",
        "The Feed-Forward Network in the Transformer takes the output of the attention mechanism (the context-enriched word representation) and applies further transformations to enable recognition of more complex patterns or features.\n",
        "\n",
        "### TASK 1: Implementing the Feed-Forward Network in PyTorch\n",
        "It consists of two fully connected layers, with a non-linear activation function in between, followed by a dropout layer to prevent overfitting. The activation function used in Attention is All You Need and in most transformer models is GELU (Gaussian Error Linear Units)."
      ],
      "metadata": {
        "id": "kH0FnZxBIetR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(config.embedding_dimensions, config.intermediate_size)\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.embedding_dimensions)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"ADD YOUR CODE HERE\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:34:17.274334Z",
          "iopub.execute_input": "2023-07-19T14:34:17.275311Z",
          "iopub.status.idle": "2023-07-19T14:34:17.284392Z",
          "shell.execute_reply.started": "2023-07-19T14:34:17.275256Z",
          "shell.execute_reply": "2023-07-19T14:34:17.283108Z"
        },
        "trusted": true,
        "id": "fmE8MTFVIetR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE: Encoder Block\n",
        "\n",
        "<img src=\"https://i.imgur.com/gYci9Lu.png\"\n",
        "     align=\"right\"\n",
        "     width=\"200\" />\n",
        "\n",
        "Now that we have implemented each piece of the encoder block, we can start assembling them together to form the encoder part of the Transformer architecture. To refresh, each encoder block is composed of the following components:\n",
        "\n",
        "1. **Multi-head self-attention**: Allows each input to interact with every other input to figure out how they should be weighted or 'attended to'.\n",
        "2. **Residual connections and layer normalization**: These techniques help to stabilize the learning process and mitigate the problem of vanishing/exploding gradients, which can occur in deep networks.\n",
        "3. **Feed forward network**: A small, fully connected neural network that applies further transformations to the output of the self-attention mechanism. It refines the word representations by processing them individually.\n",
        "\n"
      ],
      "metadata": {
        "id": "MYMg_wxHIetR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TASK 1: Implementing an Encoder Block\n",
        "In Attention Is All You Need, the layer normalization configuration used is *post layer normalization*, whereby we place layer normalization in between the skip connections. This arrangement is tricky to train from scratch as the gradients can easily diverge. Instead we will use a slightly different variant called *pre layer normalization*, which is the most common variant found in transformer literature (Xiong et al. 2020 https://arxiv.org/abs/2002.04745).\n",
        "\n",
        "<img src=\"https://i.imgur.com/lj5pme1.png\"\n",
        "     align=\"center\"\n",
        "     width=\"600\" />\n",
        "\n",
        "We're going to slightly diverge from Attention Is All You Need to utilize pre-LN."
      ],
      "metadata": {
        "id": "propi1vd0CYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "class PostLNEncoder(nn.Module):\n",
        "    \"The original architecture used in Attention Is All You Need\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.embedding_dimensions)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.embedding_dimensions)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer normalization + skip connections over the self-attention block\n",
        "        x = self.layer_norm1(x + self.attention(x))\n",
        "        # Layer norm + skip connections over the FFN\n",
        "        x = self.layer_norm2(x + self.feed_forward(x))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"The improved pre-LN architecture\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.embedding_dimensions)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.embedding_dimensions)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First perform layer normalization\n",
        "        hidden_state = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "\n",
        "        # Then apply attention + skip connection\n",
        "        x = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "\n",
        "        # Apply layer normalization before inputting to the FFN\n",
        "        hidden_state = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "\n",
        "        # Apply FNN + skip connection\n",
        "        x = \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "        return x\n",
        "\n",
        "\n",
        "# Verify this all works with the ouput of the positional embeddings\n",
        "# Initialize an encoder block\n",
        "encoder = Encoder(config).to(config.device)\n",
        "\n",
        "# Pass the output from the positional encoder to the encoder block\n",
        "encoder_output = encoder(pos_enc_output)\n",
        "\n",
        "print(\"Shape of output:\", encoder_output.size())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:34:32.763563Z",
          "iopub.execute_input": "2023-07-19T14:34:32.764039Z",
          "iopub.status.idle": "2023-07-19T14:34:32.798343Z",
          "shell.execute_reply.started": "2023-07-19T14:34:32.763993Z",
          "shell.execute_reply": "2023-07-19T14:34:32.797031Z"
        },
        "trusted": true,
        "id": "Tl20eIImIetR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE: Classifier Head\n",
        "Since our transformer is a classifier, we finish off with a classifier head.\n",
        "\n",
        "##TASK 1: Implementing the Classifier Head\n",
        "Our classifier head first flattens the output of the encoder block, then passes it through two dense layers with ReLU activation in the middle. Since we're going to use this transformer for binary classification, we then pass our final output through a sigmoid activation function (with torch.sigmoid)."
      ],
      "metadata": {
        "id": "_wdjGLmsIetR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 1 EXERCISE\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(config.max_tokens * config.embedding_dimensions, 2 * config.embedding_dimensions)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(2 * config.embedding_dimensions, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "\n",
        "# Initialize a classifier head\n",
        "classifier = ClassifierHead(config).to(config.device)\n",
        "\n",
        "# Pass the output from the positional encoder to the encoder block\n",
        "pred = classifier(encoder_output)\n",
        "\n",
        "print(\"Shape of output:\", pred.size())\n",
        "print(\"Prediction:\", pred.item())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:34:42.229892Z",
          "iopub.execute_input": "2023-07-19T14:34:42.23062Z",
          "iopub.status.idle": "2023-07-19T14:34:42.289372Z",
          "shell.execute_reply.started": "2023-07-19T14:34:42.230576Z",
          "shell.execute_reply": "2023-07-19T14:34:42.288138Z"
        },
        "trusted": true,
        "id": "AFwrknR7IetR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXERCISE: The Full Transformer\n",
        "\n",
        "<img src=\"https://i.imgur.com/iuyrnqm.png\"\n",
        "     align=\"right\"\n",
        "     width=\"240\" />\n",
        "\n",
        "Finally, with all the individual components of our transformer in place, it's time to assemble them. We have crafted each building block: the input embeddings, positional encodings, self-attention mechanisms, the feed-forward network, and the classification head.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mtK8YHzNIetR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TASK 1: Assembling the Full Transformer"
      ],
      "metadata": {
        "id": "-02eth347E3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenEmbedding(config)\n",
        "        self.positional_encoding = PositionalEncoding(config)\n",
        "        self.encoders = nn.ModuleList([Encoder(config) for _ in range(config.num_encoder_layers)])\n",
        "        self.classifier_head = ClassifierHead(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"ADD YOUR CODE HERE\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate a full model\n",
        "model = Transformer(config).to(config.device)\n",
        "\n",
        "# Make a prediction from the input sequence described earlier\n",
        "pred = model(input_sequence)\n",
        "print(\"Prediction:\", pred.item())"
      ],
      "metadata": {
        "id": "mWyYXGdN3bjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remarks:\n",
        "Our output represents the probability of the input text sequence belonging to class 1."
      ],
      "metadata": {
        "id": "MXna9l3pvcqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Training the Transformer\n",
        "Now that we have our transformer ready, we need to train it on some real data. For this, we'll use a well-known dataset named 'Natural Language Processing with Disaster Tweets'.\n",
        "\n",
        "The Disaster Tweets dataset is a compilation of 10,000 tweets that have been hand-classified. The tweets have been categorized into two classes: 'disaster' and 'non-disaster'. Our task is to train our Transformer to differentiate between the two based on the content of the tweet. This dataset is perfect for our purpose as it focuses solely on text data, and offers us a binary classification task to test our model.\n",
        "\n"
      ],
      "metadata": {
        "id": "HpSLbC5XIetR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data\n",
        "First, we load our data. The dataset is already well organized and can be readily loaded into a Pandas DataFrame.\n",
        "\n",
        "For our task, we're only interested in two fields: the 'text' and 'target'. The 'text' field contains the content of the tweet, which we'll use as our input, and the 'target' field contains the label (0 or 1), which will serve as our ground truth during training."
      ],
      "metadata": {
        "id": "4M7DdWoSvsVH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cdacfd9"
      },
      "source": [
        "# Install gdown for downloading files from Google Drive\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "587e587c"
      },
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "# Google Drive file IDs and desired filenames\n",
        "train_file_id = '1sGE48PIJAOP3bn9VJTv2yGTgtka1RKMW'\n",
        "train_filename = 'train.csv'\n",
        "test_file_id = '1m3mBXPTdntucev2TBFD1EvdBomakLUAa'\n",
        "test_filename = 'test.csv'\n",
        "\n",
        "# Download the files\n",
        "gdown.download(id=test_file_id, output=test_filename, quiet=False)\n",
        "gdown.download(id=train_file_id, output=train_filename, quiet=False)\n",
        "\n",
        "print(f\"Downloaded {test_filename} to {os.getcwd()}\")\n",
        "print(f\"Downloaded {train_filename} to {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6d54aaa"
      },
      "source": [
        "train_path, test_path = 'train.csv', 'test.csv'\n",
        "df = pd.read_csv(train_path, index_col=0, usecols=['id', 'text', 'target'])\n",
        "\n",
        "print(\"Columns in train.csv:\", df.columns.tolist())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data: Training and Validation\n",
        "To accurately gauge the performance of our model, we reserve a subset of our training data for validation. Here we will use an 80:20 training validation split."
      ],
      "metadata": {
        "id": "5eSilNl1IetS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:35:00.567383Z",
          "iopub.execute_input": "2023-07-19T14:35:00.568201Z",
          "iopub.status.idle": "2023-07-19T14:35:00.955391Z",
          "shell.execute_reply.started": "2023-07-19T14:35:00.568161Z",
          "shell.execute_reply": "2023-07-19T14:35:00.954281Z"
        },
        "trusted": true,
        "id": "5uv7GvbQIetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Tokenizer\n",
        "To process our text data, we initialize a preconfigured WordPiece tokenizer. This tokenizer converts our text data into a numerical format that can be interpreted by our model."
      ],
      "metadata": {
        "id": "ZACCQ30IIetW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "text = train_df.iloc[42].text\n",
        "\n",
        "tokenized_text = {}\n",
        "tokenized_text['Numerical Token'] = tokenizer(text)['input_ids']\n",
        "tokenized_text['Token'] = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
        "\n",
        "print(\"Tokenizer has a vocabulary size of\", tokenizer.vocab_size, \"words.\")\n",
        "print(\"Tokenizer has a maximum sequence length of\", tokenizer.model_max_length, \"tokens.\\n\")\n",
        "print(\"Sample tweet:\", text, \"\\n\")\n",
        "pd.DataFrame(tokenized_text).T.style"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:35:03.262533Z",
          "iopub.execute_input": "2023-07-19T14:35:03.262918Z",
          "iopub.status.idle": "2023-07-19T14:35:03.451671Z",
          "shell.execute_reply.started": "2023-07-19T14:35:03.262885Z",
          "shell.execute_reply": "2023-07-19T14:35:03.450594Z"
        },
        "trusted": true,
        "id": "7-lmT1IIIetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization: Converting Text to Numeric Sequences\n",
        "Here we proceed to apply our tokenizer to the text data present in both the training and validation datasets. This process transforms each text string into a corresponding sequence of numeric tokens. To ensure uniformity, each sequence is set to have a maximum length of 100 tokens. Any sequences shorter than this length are backward padded with zeros."
      ],
      "metadata": {
        "id": "hMvQ9Q8CIetW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We limit the maximum length to 100 per tweet\n",
        "max_length = config.max_tokens\n",
        "\n",
        "def tokenize_text(row, max_length):\n",
        "    text = row['text']\n",
        "    tokenized = tokenizer.encode_plus(text, padding='max_length', truncation=True, max_length=max_length)\n",
        "    return tokenized['input_ids']\n",
        "\n",
        "train_df['tokenized'] = train_df.apply(lambda row: tokenize_text(row, max_length), axis=1)\n",
        "val_df['tokenized'] = val_df.apply(lambda row: tokenize_text(row, max_length), axis=1)\n",
        "\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:35:05.673775Z",
          "iopub.execute_input": "2023-07-19T14:35:05.674517Z",
          "iopub.status.idle": "2023-07-19T14:35:07.307913Z",
          "shell.execute_reply.started": "2023-07-19T14:35:05.674473Z",
          "shell.execute_reply": "2023-07-19T14:35:07.305952Z"
        },
        "trusted": true,
        "id": "oZVh9IPjIetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataloaders\n",
        "Next we leverage PyTorch's DataLoader utility. DataLoaders serve as efficient data pipelines that handle batching and shuffling of data, saving a significant amount of manual work."
      ],
      "metadata": {
        "id": "IViRRp6IIetW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "X_train = train_df['tokenized']\n",
        "y_train = train_df['target']\n",
        "\n",
        "X_val = val_df['tokenized']\n",
        "y_val = val_df['target']\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, x_dataframe, y_dataframe):\n",
        "        self.x_dataframe = x_dataframe\n",
        "        self.y_dataframe = y_dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x_dataframe.iloc[idx]  # Get the 'tokenized' data\n",
        "        y = self.y_dataframe.iloc[idx]  # Get the 'target' data\n",
        "        return torch.LongTensor(x), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Create both datasets\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset = TextDataset(X_val, y_val)\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Shuffle for random sampling without replacement\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:35:07.402579Z",
          "iopub.execute_input": "2023-07-19T14:35:07.403249Z",
          "iopub.status.idle": "2023-07-19T14:35:07.414962Z",
          "shell.execute_reply.started": "2023-07-19T14:35:07.403212Z",
          "shell.execute_reply": "2023-07-19T14:35:07.413806Z"
        },
        "trusted": true,
        "id": "XAVHU1QaIetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "At this stage, we're ready to train our model. First, we define our loss function and the optimizer. After moving our model to a GPU (if one is available), we commence the training over 8 epochs.\n",
        "\n",
        "For monitoring purposes, we log our training and validation metrics every 20 batches. This data will allow us to visualize our model's performance dynamics over time."
      ],
      "metadata": {
        "id": "cJVeBEVoIetW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a transformer model\n",
        "model = Transformer(config).to(config.device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(config.device)\n",
        "\n",
        "# Number of training epochs\n",
        "n_epochs = 8\n",
        "\n",
        "# Metrics dictionary for plotting later\n",
        "metrics = {\n",
        "    'train_loss': [],\n",
        "    'train_accuracy': [],\n",
        "    'val_loss': [],\n",
        "    'val_accuracy': [],\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "\n",
        "        # Move inputs and targets to device\n",
        "        inputs = inputs.to(config.device)\n",
        "        targets = targets.to(config.device)\n",
        "\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        train_loss = loss_function(outputs.squeeze(), targets)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_predictions = (outputs.squeeze().detach().cpu() > 0.5)\n",
        "        train_accuracy = (train_predictions == targets.cpu()).type(torch.float).mean().item()\n",
        "\n",
        "        # Validation loop\n",
        "        if i % 20 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_losses = []\n",
        "                val_accuracies = []\n",
        "\n",
        "                for val_inputs, val_targets in val_dataloader:\n",
        "                    val_inputs = val_inputs.to(config.device)\n",
        "                    val_targets = val_targets.to(config.device)\n",
        "\n",
        "                    val_outputs = model(val_inputs)\n",
        "                    val_loss = loss_function(val_outputs.squeeze(), val_targets)\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                    val_predictions = (val_outputs.squeeze().detach().cpu() > 0.5)\n",
        "                    val_accuracy = (val_predictions == val_targets.cpu()).type(torch.float).mean().item()\n",
        "                    val_accuracies.append(val_accuracy)\n",
        "\n",
        "            # Get the mean loss and accuracy over the validation set\n",
        "            val_loss = np.mean(val_losses)\n",
        "            val_accuracy = np.mean(val_accuracies)\n",
        "\n",
        "            # Print metrics here during training\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} Step {i} \\tTrain Loss: {train_loss.item():.2f} \\tTrain Accuracy: {train_accuracy:.3f}\\n\\t\\t\\tVal Loss: {val_loss:.2f}   \\tVal Accuracy: {val_accuracy:.3f}\")\n",
        "\n",
        "            # Store metrics\n",
        "            metrics['train_loss'].append(train_loss.item())\n",
        "            metrics['train_accuracy'].append(train_accuracy)\n",
        "            metrics['val_loss'].append(val_loss)\n",
        "            metrics['val_accuracy'].append(val_accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-19T14:35:30.549135Z",
          "iopub.execute_input": "2023-07-19T14:35:30.549747Z",
          "iopub.status.idle": "2023-07-19T14:36:07.443272Z",
          "shell.execute_reply.started": "2023-07-19T14:35:30.549708Z",
          "shell.execute_reply": "2023-07-19T14:36:07.442174Z"
        },
        "trusted": true,
        "id": "dSXg-vJJIetW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure and axes\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "\n",
        "# Plot the loss\n",
        "ax[0].plot(metrics['train_loss'], color='blue', label='Train Loss')\n",
        "ax[0].plot(metrics['val_loss'], color='orange', label='Validation Loss')\n",
        "ax[0].set_title('Training and Validation Loss')\n",
        "ax[0].set_xlabel('Steps')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "# Plot the accuracy\n",
        "ax[1].plot(metrics['train_accuracy'], color='blue', label='Train Accuracy')\n",
        "ax[1].plot(metrics['val_accuracy'], color='orange', label='Validation Accuracy')\n",
        "ax[1].set_title('Training and Validation Accuracy')\n",
        "ax[1].set_xlabel('Steps')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend()\n",
        "\n",
        "# Display the figure\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-19T14:36:09.308102Z",
          "iopub.execute_input": "2023-07-19T14:36:09.308492Z",
          "iopub.status.idle": "2023-07-19T14:36:09.807853Z",
          "shell.execute_reply.started": "2023-07-19T14:36:09.308457Z",
          "shell.execute_reply": "2023-07-19T14:36:09.805262Z"
        },
        "trusted": true,
        "id": "R-D5DGYSIetW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}