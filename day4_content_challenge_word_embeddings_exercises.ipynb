{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day4_content_challenge_word_embeddings_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Challenge: Word Embeddings\n",
        "\n",
        "Today, we'll learn about word embeddings, visualize them, train a simple embedding model, and see how embeddings help with language tasks. Let's start!\n"
      ],
      "metadata": {
        "id": "s0m313PfiUT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Packages\n",
        "\n",
        "# Pre-trained Word Embedding Models\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Word Embedding Functions\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Dimensionality Reduction Methods\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# General Packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2Q1LKXnOtNzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE: Visualizing Pre-trained Word Embeddings\n",
        "\n",
        "Here, weâ€™ll use the `gensim` library to load a small set of pre-trained embeddings (e.g., Word2Vec).\n",
        "\n",
        "Tasks:\n",
        "1. Review the available pre-trained models from gensim using `print(api.info()['models'].keys())`, and select one of the models (e.g., 'fasttext-wiki-news-subwords-300'). You can look into the differences of each model. *Each model will have a different vocabulary, so keep that in mind.*\n",
        "2. Load the selected pre-trained model.\n",
        "3. Select a few words from the vocabulary of the pre-trained model (e.g. `[\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"fruit\"]`), extract their vectorized representation and then print them.\n",
        "4. Using the dimensionality reduction technique **principle component analysis**, reduce the number of dimensions of the vectorized representations so the information is amenable to plotting (i.e., reduce the dimensions to 2 or 3).\n",
        "5. Plot the dimensionality reduced vector representations.\n"
      ],
      "metadata": {
        "id": "RNeDIySliphJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 1: Select a pre-trained model"
      ],
      "metadata": {
        "id": "sd9wsqBT2OuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# List all pre-trained models available\n",
        "print(api.info()['models'].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD3aTZEemr0t",
        "outputId": "277418ea-bc91-478f-8f15-936f87affaf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 2: Load the pre-trained model"
      ],
      "metadata": {
        "id": "nItuw9zX2Vjr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6d6711ff-133d-4f7a-904e-9f4d60f138af",
        "id": "uYmx7nRKikZl"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-24f837c20b8f>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-24f837c20b8f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model = api.load(<REMOVE ME AND MY ARROWS>)  # This loads the selected model\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Load your selected pre-trained embeddings\n",
        "w2v_model = api.load(<REMOVE ME AND MY ARROWS>)  # This loads the selected model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 3: Extract the vectorized representations of sample words"
      ],
      "metadata": {
        "id": "03ARNVH63Uk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "words = list(w2v_model.index_to_key)\n",
        "print(words)\n",
        "\n",
        "# Select a Few Words and Visualize Relationships\n",
        "\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "fHSH1l0P2G0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 4: Dimensionality Reduction"
      ],
      "metadata": {
        "id": "glL15k2L3dMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 EXERCISE\n",
        "\n",
        "# Reduce dimensions to 2D for visualization\n",
        "\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "lKiMoNHE2Pcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 5: Visualize the Vector Representations"
      ],
      "metadata": {
        "id": "AfHOkpRJ4Alp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 5 EXERCISE\n",
        "\n",
        "# Plot the Dimensionality Reduced Vector Representations\n",
        "plt.figure(figsize=(5, 4))\n",
        "\n",
        "''' ADD YOUR CODE HERE '''\n",
        "\n",
        "plt.title(\"2D Visualization of Word Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZWrvtV0O4vfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE: Train a Simple Word2Vec Model\n",
        "\n",
        "\n",
        "Tasks:\n",
        "1. Design your own small text corpus either on your own, from an online source, or using some generative AI tool. This text corpus will be a `list` of sentence `strings`.\n",
        "2. Tokenize the sentence into individual words.\n",
        "3. Train a word embedding using `Word2Vec` from `gensim`, or another similar package.\n",
        "4. Extract the word embeddings for a few example words from your trained model.\n",
        "5. Visualize the embeddings from your newly trained model.\n",
        "6. Repeat tasks 1-5 adjusting different parameters to see how it affects the embedding model that is trained. Example parameters or hyperparameters (if you are using the gensim package) that you can adjust include the following: `corpus`, `vector_size`, `window`, `min_count`, etc. Remember, if you increase the vector_size, you will need to do dimensionality reduction to visualize the embedded words.\n"
      ],
      "metadata": {
        "id": "ayBX8T-Uj70x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 1: Define your corpus"
      ],
      "metadata": {
        "id": "CHqXRIo5UJOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "corpus = <REPLACE ME AND MY ARROWS>"
      ],
      "metadata": {
        "id": "N3u9A5lP5N_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 2: Tokenize the Corpus"
      ],
      "metadata": {
        "id": "HjL2TpccUlMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Tokenize the corpus\n",
        "\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "ehnBo6kl5Zm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 3: Train your own Word Embedding Model"
      ],
      "metadata": {
        "id": "rr6e2NWpVAV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "# Initialize and train the Word2Vec model\n",
        "my_w2v_model = Word2Vec(\n",
        "    sentences = <REPLACE ME AND MY ARROWS>,   # Input corpus\n",
        "    vector_size = <REPLACE ME AND MY ARROWS>, # Dimensionality of word embeddings\n",
        "    window = <REPLACE ME AND MY ARROWS>,      # Context window size\n",
        "    min_count = <REPLACE ME AND MY ARROWS>,   # Ignore words with frequency lower than this\n",
        "    workers = 4,                              # Use 4 CPU threads\n",
        "    sg = <REPLACE ME AND MY ARROWS>           # Skip-gram (1) or CBOW (0)\n",
        ")"
      ],
      "metadata": {
        "id": "cdjT4b_y5z8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 4: Extract example word embeddings"
      ],
      "metadata": {
        "id": "25lqrwuuVA0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4 EXERCISE\n",
        "\n",
        "word_embeddings = np.array([])\n",
        "\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "rJDldKYq6HTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 5: Visualize your Embeddings"
      ],
      "metadata": {
        "id": "7WGzk9yDVB00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 5 EXERCISE\n",
        "\n",
        "# Plot the Dimensionality Reduced Vector Representations\n",
        "plt.figure(figsize=(5, 4))\n",
        "\n",
        "''' ADD YOUR CODE HERE '''\n",
        "\n",
        "plt.title(\"2D Visualization of Word Embeddings\")\n",
        "plt.xlabel(\"Embedding Component 1\")\n",
        "plt.ylabel(\"Embedding Component 2\")\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a5OCbyhz6fFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE: Determine the Similarity of Sentences\n",
        "\n",
        "Here, you will explore the concept of capturing the similarity of words and groups of words in vector space.\n",
        "\n",
        "Tasks:\n",
        "1. Calculate the most similar words of your pre-trained model `w2v_model` to an example word (e.g., king) from the model's vocabulary using pre-built functions from `gensim` or a package of your choice.\n",
        "2. Calculate the similarity between two words in the vocabulary of your pre-trained model `w2v_model` (e.g., \"king\" and \"queen\" or \"apple\" and \"fruit\") using pre-built functions from `gensim` or a package of your choice.\n",
        "3. Develop a method to calculate the similarity between two sentences (e.g., \"king and queen\" with \"man and woman\"). Consider how you can create a summary vector of a sentence. Here you can use the imported function `cosine` from `scipy.spatial.distance` once you have created a summary vector of each sentence.\n"
      ],
      "metadata": {
        "id": "Q6PvLdpvmiwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 1: Find similar words to a given word"
      ],
      "metadata": {
        "id": "DlYjEChGnfGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "word_1 = <REPLACE ME AND MY ARROWS>\n",
        "word_2 = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "similar_words_1 = <REPLACE ME AND MY ARROWS>\n",
        "similar_words_2 = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "print(\"Words similar to \" + word_1 + \":\", similar_words_1)\n",
        "print(\"Words similar to \" + word_2 + \":\", similar_words_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ufbEFCaruqq_",
        "outputId": "50066400-5e79-4df8-f1f7-0b0ef58b0658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-37-f39b13b05652>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-f39b13b05652>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    word_1 = <REPLACE ME AND MY ARROWS>\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 2: Find the similarity between two words"
      ],
      "metadata": {
        "id": "UxhNEgvko69l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "word_to_compare_1 = <REPLACE ME AND MY ARROWS>\n",
        "word_to_compare_2 = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "word_pair_similarity = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "print(\"Similarity between \" + word_to_compare_1 + \" and \" + word_to_compare_2 + \":\", word_pair_similarity)"
      ],
      "metadata": {
        "id": "5OFK8piPvSpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 3: Find the similarity between two sentences"
      ],
      "metadata": {
        "id": "zJNydD0Zo6U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3 EXERCISE\n",
        "\n",
        "def sentence_vector(sentence, model):\n",
        "    '''Create a function which, given a word embedding model, and a sentence,\n",
        "       produces a vector representation of the sentence.\n",
        "    '''\n",
        "\n",
        "    pass\n",
        "\n",
        "# Define two example sentences\n",
        "sentence_1 = <REPLACE ME AND MY ARROWS>\n",
        "sentence_2 = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "# Calculate cosine similarity between the sentence embeddings\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "vector1 = sentence_vector(sentence_1, w2v_model)\n",
        "vector2 = sentence_vector(sentence_2, w2v_model)\n",
        "similarity = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "print(f\"Similarity between '{sentence_1}' and '{sentence_2}':\", similarity)"
      ],
      "metadata": {
        "id": "vTPRM8H6wein"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}