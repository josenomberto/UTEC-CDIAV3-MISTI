{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MISTI/blob/main/day6_model_validation_neural_networks_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you haven't already, please hit :**\n",
        "\n",
        "`File` -> `Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**"
      ],
      "metadata": {
        "id": "O_GXt5X5eWnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Validation with Neural Networks"
      ],
      "metadata": {
        "id": "rqXH1h0crfql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure you change runtime type to GPU! Today is computationally intensive.**\n",
        "\n",
        "You can use a free GPU/TPU through CoLab for faster training!\n",
        "Go to Runtime --> Change runtime type --> GPU or TPU"
      ],
      "metadata": {
        "id": "Ub0peVGnM-xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "AzzR8va1ez7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG6xX7YlrXzO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ----------------------------\n",
        "# ENVIRONMENT SETUP & LIBRARIES\n",
        "# ----------------------------\n",
        "\n",
        "# We comment out the IPython magic commands to ensure the code remains compatible with non-IPython environments.\n",
        "# (Uncomment when running inside a Jupyter notebook if needed.)\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "# Installing additional visualization and PyTorch utility tools (comment out if already installed).\n",
        "!pip install livelossplot\n",
        "!pip install pytorchtools\n",
        "\n",
        "# -------------\n",
        "# DATA HANDLING\n",
        "# -------------\n",
        "import pandas as pd  # For data analysis and manipulation\n",
        "import numpy as np   # For numerical operations and array manipulation\n",
        "from livelossplot import PlotLosses  # For real-time plotting of loss/metrics in training\n",
        "from livelossplot.outputs import MatplotlibPlot\n",
        "\n",
        "# ------------\n",
        "# VISUALIZATION\n",
        "# ------------\n",
        "# %matplotlib inline  # Uncomment in Jupyter notebooks for inline plots\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl  # Sometimes used for interactive plotting\n",
        "\n",
        "# ----------------\n",
        "# CROSS-VALIDATION\n",
        "# ----------------\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,      # Hyperparameter tuning using exhaustive search\n",
        "    train_test_split,  # Splitting data into train and test sets\n",
        "    cross_val_predict, # Generating cross-validated estimates for each input data point\n",
        "    cross_val_score,   # Evaluating a score by cross-validation\n",
        "    KFold              # K-Fold cross-validator\n",
        ")\n",
        "from sklearn import model_selection\n",
        "\n",
        "# ---------------------\n",
        "# PARALLEL PROCESSING\n",
        "# ---------------------\n",
        "from joblib import Parallel, delayed  # For parallel execution of tasks\n",
        "import multiprocessing                # Provides multiprocessing primitives\n",
        "\n",
        "# -------------\n",
        "# MISC SETTINGS\n",
        "# -------------\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n",
        "\n",
        "# Set a manual seed for reproducibility across different runs/environments.\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Extra Packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepping the Notebook for Training Neural Networks"
      ],
      "metadata": {
        "id": "Ifoq76S7e5Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we activate the GPU."
      ],
      "metadata": {
        "id": "u8_xToYffJvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "5_Ri7MiIrskL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create smaller subsets of the MNIST dataset for training, validation, and testing, then set up corresponding PyTorch DataLoaders to streamline batch processing during model training and evaluation."
      ],
      "metadata": {
        "id": "T6_O3Db1hJh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# -----------------------------------------\n",
        "# Define subset indices for train and test\n",
        "# -----------------------------------------\n",
        "train_indices = torch.arange(2500)\n",
        "# Optional subset for a larger training set (currently commented out):\n",
        "# tr_10k = data_utils.Subset(tr, indices)\n",
        "\n",
        "# Create a training subset from the MNIST training set\n",
        "train_set = data_utils.Subset(\n",
        "    torchvision.datasets.MNIST(\n",
        "        'sample_data/',            # Local directory for storing/loading data\n",
        "        train=True,                # Use the training portion of MNIST\n",
        "        download=True,             # Automatically download if not present\n",
        "        transform=torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),                       # Convert images to Tensor\n",
        "            torchvision.transforms.Normalize((0.1307,), (0.3081,))   # Normalize with MNIST statistics\n",
        "        ])\n",
        "    ),\n",
        "    train_indices                 # The indices that define the smaller training subset\n",
        ")\n",
        "\n",
        "# Define subset indices for testing\n",
        "test_indices = torch.arange(250)\n",
        "\n",
        "# Create a testing subset from the MNIST testing set\n",
        "test_set = data_utils.Subset(\n",
        "    torchvision.datasets.MNIST(\n",
        "        'sample_data/',            # Local directory for storing/loading data\n",
        "        train=False,               # Use the test portion of MNIST\n",
        "        download=True,             # Automatically download if not present\n",
        "        transform=torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize((0.1307,), (0.3081,))  # Same normalization as training\n",
        "        ])\n",
        "    ),\n",
        "    test_indices                  # The indices that define the smaller testing subset\n",
        ")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Create validation split from the train subset\n",
        "# ---------------------------------------------\n",
        "indices = [i for i in range(2500)]  # We'll train on 2,500 images total\n",
        "rand1, rand2 = train_test_split(indices, test_size=0.2, random_state=1, shuffle=True)\n",
        "train_sample = SubsetRandomSampler(rand1)  # 80% of indices\n",
        "val_sample = SubsetRandomSampler(rand2)    # 20% of indices\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# -------------------\n",
        "# Create Data Loaders\n",
        "# -------------------\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    sampler=train_sample,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    sampler=val_sample,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "y80guaK4rsmz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple fully-connected neural network class that uses two linear layers for classification tasks, demonstrating how to build and forward-pass data through a PyTorch model."
      ],
      "metadata": {
        "id": "xort0sK1hn92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self, n_features=784, hidden_size=32):\n",
        "        \"\"\"\n",
        "        Initialize a fully-connected neural network with:\n",
        "          - An input layer mapped to a hidden layer\n",
        "          - A second layer that outputs 10 logits (e.g., for 10-class classification)\n",
        "        \"\"\"\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "\n",
        "        # First linear layer: reduces input dimension (n_features) to hidden_size\n",
        "        self.layer0 = nn.Linear(n_features, hidden_size)\n",
        "\n",
        "        # Second linear layer: reduces hidden_size to 10 (common for MNIST-like 10-class problems)\n",
        "        self.layer1 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        Forward pass for the network:\n",
        "          1. Apply ReLU activation on output of layer0\n",
        "          2. Pass activated output to layer1\n",
        "          3. Apply log_softmax to get log probabilities for each class\n",
        "        \"\"\"\n",
        "        # Apply ReLU to the output of the first layer\n",
        "        x = F.relu(self.layer0(input_data))\n",
        "\n",
        "        # Pass the activated output through the second layer\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        # Apply log_softmax activation on the final outputs\n",
        "        # (default dim=1 for batch-first format)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "E0t7u6uNrspa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define essential training hyperparameters, instantiate our fully-connected network, move it to the chosen device (CPU or GPU), configure the optimizer, and finally print out the model’s architecture."
      ],
      "metadata": {
        "id": "P1gGTStUh3eE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------\n",
        "# HYPERPARAMETERS\n",
        "# ---------------\n",
        "n_epochs = 3              # Number of times to iterate over the entire dataset\n",
        "batch_size_train = 64     # Mini-batch size for training\n",
        "batch_size_test = 1000    # Mini-batch size for testing\n",
        "learning_rate = 0.01      # Initial learning rate for the optimizer\n",
        "momentum = 0.5            # Momentum factor for SGD\n",
        "log_interval = 10         # Frequency of logging training status\n",
        "\n",
        "# ---------------------\n",
        "# MODEL INITIALIZATION\n",
        "# ---------------------\n",
        "# Create an instance of the fully-connected network defined previously\n",
        "fc_network = FullyConnectedNetwork()\n",
        "\n",
        "# Move the model to the specified device (CPU or GPU)\n",
        "fc_network.to(device)\n",
        "\n",
        "# ----------------\n",
        "# OPTIMIZER SETUP\n",
        "# ----------------\n",
        "# Use Stochastic Gradient Descent with the given learning rate and momentum\n",
        "fc_optimizer = optim.SGD(fc_network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "# --------------------------------\n",
        "# INSPECTION: PRINT MODEL LAYERS\n",
        "# --------------------------------\n",
        "print(fc_network)"
      ],
      "metadata": {
        "id": "5ddc2hwxrsrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define a training function that handles the forward pass, loss computation, backpropagation, and optimization steps for each batch, while also providing periodic loss reporting during the epoch."
      ],
      "metadata": {
        "id": "e5f04G_-h9c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# TRAINING FUNCTION DEFINED\n",
        "# -------------------------\n",
        "# Note: network.train() sets the model to training mode, which can enable\n",
        "# specific layers (e.g., Dropout or BatchNorm) to behave differently\n",
        "# compared to evaluation mode.\n",
        "\n",
        "def train(network, optimizer, epoch, trainloader, flatten_data=True):\n",
        "    \"\"\"\n",
        "    Trains the given network for one epoch on the provided DataLoader.\n",
        "\n",
        "    Args:\n",
        "        network: The neural network model to be trained.\n",
        "        optimizer: The optimizer used (SGD, Adam, etc.) to update model weights.\n",
        "        epoch: The current epoch number (used for reporting).\n",
        "        trainloader: DataLoader that provides (data, target) batches.\n",
        "        flatten_data: If True, reshapes data from (batch, channels, height, width)\n",
        "                      to (batch, -1), which is often required for fully connected layers.\n",
        "\n",
        "    Returns:\n",
        "        loss_logs: A list of tuples containing the total number of processed examples\n",
        "                   and the corresponding loss at logging intervals.\n",
        "    \"\"\"\n",
        "    network.train()       # Ensure the model is in training mode\n",
        "    loss_logs = list()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # Optionally flatten the data for a fully connected layer\n",
        "        if flatten_data:\n",
        "            data = flatten(data)\n",
        "\n",
        "        # For convenience in referencing lengths\n",
        "        num_batches = len(trainloader)\n",
        "        size = len(trainloader.dataset)\n",
        "\n",
        "        # Move data and target tensors to the chosen device (CPU or GPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero out the gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute output of the network\n",
        "        output = network(data)\n",
        "\n",
        "        # Compute loss using Negative Log Likelihood (common for classification)\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimization step: update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log progress at specified intervals\n",
        "        if batch_idx % log_interval == 0:\n",
        "            examples_shown = batch_idx * len(data)\n",
        "            total_examples = len(train_loader.dataset)\n",
        "            fraction_shown = round(examples_shown / total_examples * 100., 2)\n",
        "            rounded_loss = round(loss.item(), 4)\n",
        "            print(f'Train Epoch {epoch} Progress: {fraction_shown}%\\tLoss: {rounded_loss}')\n",
        "\n",
        "            # Track the total examples seen so far in all epochs plus current batch\n",
        "            examples_so_far = batch_idx * batch_size_train + epoch * len(train_loader.dataset)\n",
        "            loss_logs.append((examples_so_far, loss.item()))\n",
        "\n",
        "    return loss_logs"
      ],
      "metadata": {
        "id": "QE1UFkxJrstr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a convolutional neural network (ConvNet) architecture with two convolutional layers (each followed by max-pooling, and the second by dropout), as well as two fully connected layers, designed for image classification tasks like MNIST."
      ],
      "metadata": {
        "id": "gBJuvcdficpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes a two-layer convolutional neural network with dropout and\n",
        "        two fully connected layers. Ideal for classification tasks such as MNIST.\n",
        "        \"\"\"\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # First convolution: input = 1 channel (e.g., grayscale image),\n",
        "        # output = 10 channels, kernel = 5x5\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
        "\n",
        "        # Second convolution: input = 10 channels, output = 20 channels, kernel = 5x5\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
        "\n",
        "        # 2D dropout (often used after convolutions to reduce overfitting)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "\n",
        "        # First fully connected layer: maps 320 features to 50\n",
        "        self.fc1 = nn.Linear(in_features=320, out_features=50)\n",
        "\n",
        "        # Second fully connected layer: maps 50 features to 10 (e.g., 10 classes for MNIST)\n",
        "        self.fc2 = nn.Linear(in_features=50, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass through the network layers:\n",
        "          1) Conv1 + ReLU + MaxPool\n",
        "          2) Conv2 + Dropout + MaxPool + ReLU\n",
        "          3) Flatten and pass through two fully connected layers\n",
        "          4) Apply log_softmax for classification probability outputs\n",
        "        \"\"\"\n",
        "        # First convolution, followed by ReLU activation and 2x2 max-pooling\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        # Second convolution, followed by dropout\n",
        "        x = self.conv2_drop(self.conv2(x))\n",
        "\n",
        "        # 2x2 max-pooling\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Another ReLU activation\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Flatten the feature map into a vector of size (batch, 320)\n",
        "        x = x.view(-1, 320)\n",
        "\n",
        "        # Pass the flattened output through the first fully connected layer + ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply dropout in training mode\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        # Pass through the final fully connected layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Return log probabilities (useful for classification tasks)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "o1-6sfwDrsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a utility function that reshapes a batch of 2D MNIST images (each 28×28) into a 1D vector of 784 elements, allowing them to be processed by a fully connected layer."
      ],
      "metadata": {
        "id": "QxFHHedRilnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! Fully connected networks can't take 2D images directly, so we flatten each image into a long vector.\n",
        "def flatten(batch):\n",
        "    \"\"\"\n",
        "    Flattens a batch of images from shape (batch_size, 1, 28, 28)\n",
        "    into (batch_size, 784).\n",
        "\n",
        "    Args:\n",
        "        batch (torch.Tensor): A batch of images of shape (B, C, H, W).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A batch of images of shape (B, 784).\n",
        "    \"\"\"\n",
        "    flat_batch = batch.reshape((batch.shape[0], 784))\n",
        "    return flat_batch"
      ],
      "metadata": {
        "id": "whmXvQbBrsyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a flexible, multi-layer fully-connected network (MLP) that allows you to specify the number of hidden layers and hidden nodes. It uses ReLU for activation and is suitable for classification tasks such as MNIST."
      ],
      "metadata": {
        "id": "-CgujSRC_JJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import ModuleList, Sequential, Linear, ReLU, Softmax, CrossEntropyLoss\n",
        "\n",
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self, hidden_layers=2, hidden_nodes=10):\n",
        "        \"\"\"\n",
        "        Initializes a Fully Connected Network with a variable number of hidden layers.\n",
        "\n",
        "        Args:\n",
        "            hidden_layers (int): Number of hidden layers to include (default=2).\n",
        "            hidden_nodes (int): Number of neurons in each hidden layer (default=10).\n",
        "        \"\"\"\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "\n",
        "        layers = list()\n",
        "\n",
        "        # -------------------------\n",
        "        # 1. Input -> Hidden Layer\n",
        "        # -------------------------\n",
        "        layers.append(Linear(in_features=784, out_features=hidden_nodes))  # For flattened 28x28 images (MNIST)\n",
        "        layers.append(ReLU())  # Activation function after the first layer\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # 2. Add the specified number of hidden layers + ReLU activations\n",
        "        # ----------------------------------------------------------\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(Linear(in_features=hidden_nodes, out_features=hidden_nodes))\n",
        "            layers.append(ReLU())\n",
        "\n",
        "        # ----------------------\n",
        "        # 3. Output Layer (10)\n",
        "        # ----------------------\n",
        "        layers.append(Linear(in_features=hidden_nodes, out_features=10))\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # Chain layers together into one sequential MLP component\n",
        "        # --------------------------------------------------------\n",
        "        self.mlp = Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        Defines the forward pass for the network.\n",
        "\n",
        "        Args:\n",
        "            input_data (torch.Tensor): The input data, typically of shape (batch_size, 784).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The network's output logits of shape (batch_size, 10).\n",
        "        \"\"\"\n",
        "        return self.mlp(input_data)\n"
      ],
      "metadata": {
        "id": "OTJyYIFVrsz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define two utility functions — `trainer` for model training and `tester` for model evaluation—that update and monitor performance metrics like loss and accuracy for each epoch."
      ],
      "metadata": {
        "id": "N4ZTfPt0_crX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(network, optimizer, device, flatten_data=True):\n",
        "    \"\"\"\n",
        "    Trains the given network for one epoch.\n",
        "\n",
        "    Args:\n",
        "        network (nn.Module): The neural network model to train.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer to apply during backpropagation.\n",
        "        device (str): The device type ('cpu' or 'cuda') to use for training.\n",
        "        flatten_data (bool): Whether to flatten the input images into vectors (True for fully connected networks).\n",
        "\n",
        "    Returns:\n",
        "        (float, float): A tuple (loss_ratio, correct_ratio) representing the average loss\n",
        "                        per example and overall accuracy after one epoch of training.\n",
        "    \"\"\"\n",
        "    network.train()  # Set the model to training mode\n",
        "\n",
        "    # Initialize accumulators for loss and accuracy calculations\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_items = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # Transfer the data and target to the specified device (GPU/CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Flatten the data if using fully connected layers\n",
        "        if flatten_data:\n",
        "            data = flatten(data)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        output = network(data)\n",
        "\n",
        "        # Compute batch loss (CrossEntropy is well-suited for multi-class classification)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Reset gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagation: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Parameter update step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss += loss\n",
        "        total_correct += torch.sum(torch.argmax(output, dim=1) == target)\n",
        "        total_items += len(target)\n",
        "\n",
        "        # Calculate average loss and accuracy so far\n",
        "        loss_ratio = (total_loss / total_items).item()\n",
        "        correct_ratio = (total_correct / total_items).item()\n",
        "\n",
        "        # Note: .item() converts a scalar tensor to a Python float (and moves it to CPU if on GPU)\n",
        "\n",
        "    return loss_ratio, correct_ratio\n",
        "\n",
        "\n",
        "def tester(network, loader, device, flatten_data=True):\n",
        "    \"\"\"\n",
        "    Evaluates the given network on a test/validation dataset.\n",
        "\n",
        "    Args:\n",
        "        network (nn.Module): The trained neural network model to test.\n",
        "        loader (DataLoader): DataLoader providing the test/validation dataset.\n",
        "        device (str): The device type ('cpu' or 'cuda') to use for evaluation.\n",
        "        flatten_data (bool): Whether to flatten the input images into vectors.\n",
        "\n",
        "    Returns:\n",
        "        (float, float): A tuple (loss_ratio, correct_ratio) representing the average loss\n",
        "                        and accuracy over the entire test/validation set.\n",
        "    \"\"\"\n",
        "    network.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Initialize accumulators for loss and accuracy calculations\n",
        "    test_loss = 0\n",
        "    test_correct = 0\n",
        "    test_items = 0\n",
        "\n",
        "    # Deactivate gradient calculations for evaluation\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "\n",
        "            # Transfer data to device for faster computation\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Flatten if required for fully connected layers\n",
        "            if flatten_data:\n",
        "                data = flatten(data)\n",
        "\n",
        "            # Forward pass: model predictions\n",
        "            output = network(data)\n",
        "\n",
        "            # Compute batch loss\n",
        "            loss_fn = CrossEntropyLoss()\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            test_loss += loss\n",
        "            test_correct += torch.sum(torch.argmax(output, dim=1) == target)\n",
        "            test_items += len(target)\n",
        "\n",
        "            # Calculate average loss and accuracy so far\n",
        "            loss_ratio = (test_loss / test_items).item()\n",
        "            correct_ratio = (test_correct / test_items).item()\n",
        "\n",
        "    return loss_ratio, correct_ratio"
      ],
      "metadata": {
        "id": "1bPSxCCprs2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting"
      ],
      "metadata": {
        "id": "WhoqT8gWw17F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Visualize Neural Network Overfitting\n",
        "\n",
        "The code initializes a Fully Connected Network (`FullyConnectedNetwork`) and trains it using stochastic gradient descent.\n",
        "\n",
        "A plotting utility (`plotlosses`) is used to visualize training and validation metrics over epochs.\n",
        "\n",
        "**Key Components**:\n",
        "\n",
        "- `n_epochs`: Number of epochs to train the model.\n",
        "- `hidden_layers`: Number of hidden layers in the FCN.\n",
        "- `hidden_nodes`: Number of nodes in each hidden layer.\n",
        "- `learning_rate`: Learning rate for the optimizer.\n",
        "\n",
        "Tasks:\n",
        "- Modify Hyperparameters\n",
        "  - Experiment with different values for `n_epochs`, `hidden_layers`, and `hidden_nodes`.\n",
        "    - Start with the default values:\n",
        "        - `n_epochs = 50`\n",
        "        - `hidden_layers = 3`\n",
        "        - `hidden_nodes = 250`\n",
        "    - Try reducing and increasing these values systematically.\n",
        "    - Observe the loss and accuracy curves for training and validation datasets.\n",
        "    - Record how changing `n_epochs`, `hidden_layers`, and `hidden_nodes` generally affects:\n",
        "      - Training loss and accuracy.\n",
        "      - Validation loss and accuracy.\n",
        "- Identify Overfitting\n",
        "    - Identify a single parameter combination where overfitting occurs. (e.g., training accuracy increases while validation accuracy plateaus or decreases).\n"
      ],
      "metadata": {
        "id": "Eo3xJ_aztRoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Identify an Instance of Overfitting"
      ],
      "metadata": {
        "id": "F37wccPYPD_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "# Adjust the Hyperparameters to Identify an instance of overfitting\n",
        "learning_rate = 0.01\n",
        "n_epochs = <REPLACE ME AND MY ARROWS>\n",
        "fc_network = FullyConnectedNetwork(hidden_layers = <REPLACE ME AND MY ARROWS>, hidden_nodes = <REPLACE ME AND MY ARROWS>)\n",
        "fc_network.to(device)\n",
        "fc_optimizer = optim.SGD(fc_network.parameters(), lr=learning_rate)\n",
        "\n",
        "plotlosses = PlotLosses(outputs=[MatplotlibPlot(cell_size = (3, 2))])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(\"Current Epoch: \" + str(epoch))\n",
        "\n",
        "  train_loss, train_acc = trainer(fc_network, fc_optimizer, device)\n",
        "  val_loss, val_acc = tester(fc_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "# Following your training procedure, remember to assess the performance of your model on your test data\n",
        "test_loss, test_acc = tester(fc_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "Sod4h4hqG7dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization"
      ],
      "metadata": {
        "id": "XxBtweEGtJE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Effects of Regularization on Overfitting\n",
        "\n",
        "Tasks:\n",
        "1. Run the Code with Default Parameters\n",
        "    - Execute the code with the following settings:\n",
        "        - `weight_decay = 0`\n",
        "        - `hidden_layers = 3`\n",
        "        - `hidden_nodes = 1000`\n",
        "        - `num_epoch = 100`\n",
        "    - Observe and note the training and validation curves.\n",
        "2. Experiment with Different Weight Decay Values\n",
        "    - Modify `weight_decay` to test the following values:\n",
        "        - 0.0005\n",
        "        - 0.001\n",
        "        - 0.005\n",
        "        - 0.01\n",
        "    - For each value:\n",
        "        - Run the code.\n",
        "        - Save the Accuracy and Loss training plots.\n",
        "        - Record the test loss and accuracy for each `weight_decay` value."
      ],
      "metadata": {
        "id": "QQnQ7M_ItLVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Run without Regularization"
      ],
      "metadata": {
        "id": "R4nUx-lbfHB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0\n",
        "\n",
        "# create network and optimizer\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "fc_network = FullyConnectedNetwork(hidden_layers=3, hidden_nodes = 1000)\n",
        "fc_network.to(device)\n",
        "fc_optimizer = optim.SGD(fc_network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "plotlosses = PlotLosses(outputs=[MatplotlibPlot(cell_size = (3, 2))])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  train_loss, train_acc = trainer(fc_network, fc_optimizer, device)\n",
        "  val_loss, val_acc = tester(fc_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "test_loss, test_acc = tester(fc_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "mymPN91NVqct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Run with Regularization"
      ],
      "metadata": {
        "id": "bAUOjYCrfScp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "''' ADD YOUR CODE HERE '''"
      ],
      "metadata": {
        "id": "4j9MOTZqfnBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Validation with k-fold Cross Validation for Neural Networks\n",
        "\n",
        "Tasks:\n",
        "1. Run the following code to implement k-fold cross validation on this dataset.\n",
        "  - Analyze the results."
      ],
      "metadata": {
        "id": "jOHIyN3mwee5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 SOLUTION AND EXERCISE\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
        "\n",
        "# Set up k-fold cross-validation\n",
        "k = 5  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Set some hyperparameters\n",
        "num_epochs = 10\n",
        "\n",
        "# Prepare results storage\n",
        "fold_accuracies = []\n",
        "fold_losses = []\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_set)):\n",
        "    print(f\"Training fold {fold + 1}/{k}\")\n",
        "\n",
        "    # SubsetRandomSampler for train and validation splits\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "    # Use DataLoader with these samplers\n",
        "    train_loader = DataLoader(train_set, sampler=train_sampler, batch_size=batch_size)\n",
        "    val_loader = DataLoader(train_set, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Define the neural network model\n",
        "    class NeuralNet(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(NeuralNet, self).__init__()\n",
        "            self.fc1 = nn.Linear(28 * 28, 16)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(16, 10)  # 10 output classes (digits 0-9)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.view(x.size(0), -1)  # Flatten the image\n",
        "            x = self.fc1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    # Initialize the model, loss, and optimizer\n",
        "    model = NeuralNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in val_loader:\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "    print(f\"Fold {fold + 1} - Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate overall performance\n",
        "average_accuracy = sum(fold_accuracies) / k\n",
        "average_loss = sum(fold_losses) / k\n",
        "print(\"\\nK-Fold Cross-Validation Results:\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
        "print(f\"Average Loss: {average_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "sJReHTsWXim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF PART A. STOP HERE (for now)\n",
        "We will present more content which will help you with the rest of the notebook."
      ],
      "metadata": {
        "id": "YPt9EWFftB-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout Layers\n",
        "\n",
        "When training deep neural networks, one common challenge is **overfitting**, where the model learns patterns that are specific to the training data but do not generalize well to unseen data. To combat this, **dropout** is a widely used regularization technique that randomly **\"drops\"** a fraction of the neurons in the network during each training step, preventing the model from becoming overly reliant on any specific neurons. This forces the network to learn more robust features and improves its ability to generalize.\n",
        "\n",
        "The following code defines a **fully connected neural network** with **dropout layers** integrated into its architecture. The **DropoutNetwork** class allows you to configure the number of hidden layers, the number of nodes per layer, and the **dropout probability**, which controls how many neurons are randomly dropped during training. This modular implementation makes it easy to adjust the network's complexity and regularization strength to suit different tasks and datasets."
      ],
      "metadata": {
        "id": "Ly8yGJHFFJl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Dropout\n",
        "\n",
        "# Define a fully connected network with dropout layers to improve generalization\n",
        "class DropoutNetwork(nn.Module):\n",
        "    def __init__(self, hidden_layers=2, hidden_nodes=10, p=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the DropoutNetwork.\n",
        "\n",
        "        Args:\n",
        "            hidden_layers (int): The number of hidden layers in the network.\n",
        "            hidden_nodes (int): The number of nodes in each hidden layer.\n",
        "            p (float): The dropout probability, controlling how many neurons are randomly zeroed out.\n",
        "        \"\"\"\n",
        "        super(DropoutNetwork, self).__init__()\n",
        "\n",
        "        layers = list()  # Initialize an empty list to hold the network layers\n",
        "\n",
        "        # Add the input layer (784 input features for a typical flattened image input)\n",
        "        layers.append(Linear(784, hidden_nodes))\n",
        "        layers.append(ReLU())  # Apply ReLU activation to introduce non-linearity\n",
        "\n",
        "        # Loop to create the specified number of hidden layers\n",
        "        for _ in range(hidden_layers):\n",
        "            # Add a fully connected hidden layer\n",
        "            layers.append(Linear(hidden_nodes, hidden_nodes))\n",
        "\n",
        "            # Add a dropout layer to randomly zero out a fraction of neurons, reducing overfitting\n",
        "            layers.append(Dropout(p=p))\n",
        "\n",
        "            # Apply ReLU activation after the dropout layer\n",
        "            layers.append(ReLU())\n",
        "\n",
        "        # Add the output layer (10 output features for a typical classification task with 10 classes)\n",
        "        layers.append(Linear(hidden_nodes, 10))\n",
        "\n",
        "        # Chain the layers together using nn.Sequential\n",
        "        self.mlp = Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        Forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            input_data (Tensor): The input data to the network.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output predictions from the network.\n",
        "        \"\"\"\n",
        "        return self.mlp(input_data)"
      ],
      "metadata": {
        "id": "sIRW1tdozqbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Effects of Dropout on Overfitting\n",
        "\n",
        "Tasks:\n",
        "- Adjust the $p$ parameter, and see how this affects model training performance.\n",
        "  - Try three different values, and note how the plotted loss curves change as $p$ gets closer to 1."
      ],
      "metadata": {
        "id": "1c2PM5MPFJl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "plotlosses = PlotLosses()\n",
        "\n",
        "p = <REPLACE ME AND MY ARROWS> # DEFAULT VALUE IS .3\n",
        "\n",
        "# create network and optimizer\n",
        "learning_rate = 0.01\n",
        "n_epochs = 50\n",
        "\n",
        "# TODO: Create DropoutNetwork with same NN parameters from Exercise 1 (for which you saw overfitting)\n",
        "drop_network = DropoutNetwork(hidden_layers=3, hidden_nodes = 250, p=p)\n",
        "\n",
        "drop_network.to(device)\n",
        "\n",
        "drop_optimizer = optim.SGD(drop_network.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  train_loss, train_acc = trainer(drop_network, drop_optimizer, device)\n",
        "  val_loss, val_acc = tester(drop_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "test_loss, test_acc = tester(drop_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "8-U_Gas00l19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping\n",
        "\n",
        "Early stopping is a regularization technique used to prevent overfitting during model training. It works by monitoring the model's performance on the validation set and stopping the training process once the performance stops improving.  \n",
        "\n",
        "The key advantage of early stopping is that it helps save time and resources by avoiding unnecessary training cycles, while also improving the model's generalization to unseen data. A critical parameter to tune in early stopping is **`patience`**, which defines how many epochs the model will wait before stopping if no further improvement is observed.  \n",
        "\n",
        "This approach ensures the model trains efficiently without overfitting to the training data."
      ],
      "metadata": {
        "id": "EIrT7Kom12wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Early Stopping\n",
        "\n",
        "In this exercise, you'll implement an early stopping callback within a training loop to prevent overfitting and optimize training time.\n",
        "\n",
        "The key parameter to focus on is `patience`, which defines how many epochs the model will wait without improvement in validation performance or loss before halting training. Tuning this parameter helps strike a balance between giving the model enough time to improve and stopping it before unnecessary training cycles.\n",
        "\n",
        "Tasks:\n",
        "1. Experiment with different `patience` values and observe how they impact validation accuracy and the overall training process.\n"
      ],
      "metadata": {
        "id": "qIA0gaOR1873"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 50\n",
        "fc_network = FullyConnectedNetwork(hidden_layers = 3, hidden_nodes = 250)\n",
        "fc_network.to(device)\n",
        "fc_optimizer = optim.SGD(fc_network.parameters(), lr=learning_rate)\n",
        "\n",
        "patience = <REMOVE ME AND MY ARROWS> # Try values 1, 2, 3, 4, 5\n",
        "\n",
        "plotlosses = PlotLosses()\n",
        "# Train 1st epoch separately\n",
        "train_loss, train_acc = trainer(fc_network, fc_optimizer, device)\n",
        "val_loss, val_acc = tester(fc_network, val_loader, device)\n",
        "\n",
        "plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "# Early stopping\n",
        "last_loss = val_loss\n",
        "triggertimes = 0\n",
        "\n",
        "# Train remaining epochs\n",
        "for epoch in range(1,n_epochs):\n",
        "\n",
        "  # Save latest model\n",
        "  latest_model = fc_network\n",
        "\n",
        "  train_loss, train_acc = trainer(fc_network, fc_optimizer, device)\n",
        "  val_loss, val_acc = tester(fc_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "  current_loss = val_loss\n",
        "\n",
        "  print('The Current Loss:', current_loss)\n",
        "\n",
        "  if current_loss > last_loss:\n",
        "\n",
        "      trigger_times += 1\n",
        "      print('Trigger Times:', trigger_times)\n",
        "\n",
        "      if trigger_times >= patience:\n",
        "          print('Early stopping!\\nStart to test process.')\n",
        "          break\n",
        "\n",
        "  else:\n",
        "    print('trigger times: 0')\n",
        "    trigger_times = 0\n",
        "\n",
        "  last_loss = current_loss\n",
        "\n",
        "test_loss, test_acc = tester(fc_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "xt0kE8CrfGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Batch Normalization is a technique used to **improve the training process** of deep neural networks by **normalizing the inputs** of each layer. It reduces the **internal covariate shift** — a phenomenon where the **distribution of inputs** to a layer changes during training — by ensuring that the inputs to each layer have a **consistent mean and variance**.\n",
        "\n",
        "This normalization helps **stabilize** and **speed up** the training process, allowing the model to **converge faster** and **perform better**. It also acts as a **regularizer**, reducing the need for other methods like **dropout** in some cases.\n",
        "\n",
        "In practice, Batch Normalization is applied **after the linear transformations** (like **fully connected layers** or **convolutional layers**) but **before applying the activation function**. By **maintaining stable input distributions**, it enables the use of **higher learning rates** and makes the network more **robust to parameter initialization**."
      ],
      "metadata": {
        "id": "r0H0AH5nzxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE: Effects of Batch Normalization on Overfitting\n",
        "\n",
        "In this exercise, we'll explore how batch normalization can improve neural network performance by stabilizing the training process and reducing internal covariate shift. You'll implement batch normalization layers and observe their effects on model training and validation performance.\n",
        "\n",
        "Tasks:\n",
        "1. Introduce batch normalization into a neural network using the `BatchNorm1d(~)` function.\n",
        "2. Using the `BatchNormNetwork(~)` and the previous exercises to see how introducing batch normalization can lead to reductions in overfitting. You can then manipulate the hyperparameters to see how Batch Normalization interacts with different values for `hidden_layers`, `hidden_nodes`, and `learning_rate`.\n"
      ],
      "metadata": {
        "id": "Zj8MnhWAzxui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Introduce Batch Normalization"
      ],
      "metadata": {
        "id": "HJNQfz6XgIcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1 EXERCISE\n",
        "\n",
        "from torch.nn import BatchNorm1d\n",
        "\n",
        "class BatchNormNetwork(nn.Module):\n",
        "  def __init__(self,  hidden_layers=2, hidden_nodes=10):\n",
        "    super(BatchNormNetwork, self).__init__()\n",
        "    layers = list()\n",
        "\n",
        "    layers.append(Linear(784, hidden_nodes))            # input layer\n",
        "    layers.append(ReLU())\n",
        "    for hdim in range(hidden_layers):                   # create hidden layers\n",
        "      layers.append(Linear(hidden_nodes, hidden_nodes))\n",
        "      layers.append(ReLU())\n",
        "\n",
        "      # TODO: Add 1D batch normalization to all hidden nodes\n",
        "      # Append BatchNorm1d layer\n",
        "      <REPLACE ME AND MY BRACKETS>\n",
        "\n",
        "    layers.append(Linear(hidden_nodes, 10))             # output layer\n",
        "\n",
        "    self.mlp = Sequential(*layers)                      # chain together\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    return self.mlp(input_data)\n"
      ],
      "metadata": {
        "id": "tJbzLHmbgInF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 2: Train a BatchNormNetworK"
      ],
      "metadata": {
        "id": "RD3Qab41hKnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2 EXERCISE\n",
        "\n",
        "# Here is an example set of parameters that leads to overfitting\n",
        "learning_rate = .01\n",
        "n_epochs = 100\n",
        "hidden_layers = 3\n",
        "hidden_nodes = 1000\n",
        "\n",
        "# Use the BatchNormNetwork\n",
        "batch_network = <REPLACE ME AND MY ARROWS>\n",
        "batch_network.to(device)\n",
        "batch_optimizer = <REPLACE ME AND MY ARROWS>\n",
        "\n",
        "plotlosses = PlotLosses(outputs=[MatplotlibPlot(cell_size = (3, 2))])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  print(\"Current Epoch: \" + str(epoch))\n",
        "\n",
        "  train_loss, train_acc = trainer(batch_network, batch_optimizer, device)\n",
        "  val_loss, val_acc = tester(batch_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n",
        "\n",
        "# Following your training procedure, remember to assess the performance of your model on your test data\n",
        "test_loss, test_acc = tester(batch_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "fJGCh61ghKw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify learning rate and optimizer"
      ],
      "metadata": {
        "id": "TV4ZmHCT0R1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't be covering it here but you can also set a desired time-varying learning rate schedule as well, using the `torch.optim.lr_scheduler` option in PyTorch.\n",
        "\n",
        "Some examples include exponential learning rates and ReduceLROnPlateau.\n",
        "\n",
        "You can learn about the different types of optimizers and learning rate schedulers available in PyTorch [here](https://https://pytorch.org/docs/stable/optim.html)."
      ],
      "metadata": {
        "id": "NujhYjzK7f6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TASK 1: Experiment with different optimization algorithms, learning rates and schedulers to see how it affects **training time and accuracy**. Compare the **convergence time** to the runs above."
      ],
      "metadata": {
        "id": "R2zRz0P17xrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify your train function to use a scheduler"
      ],
      "metadata": {
        "id": "OT0gZr1I0IJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer_scheduler(network, optimizer, device, flatten_data=True):\n",
        "    \"\"\"\n",
        "    Trains the given network for one epoch.\n",
        "\n",
        "    Args:\n",
        "        network (nn.Module): The neural network model to train.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer to apply during backpropagation.\n",
        "        device (str): The device type ('cpu' or 'cuda') to use for training.\n",
        "        flatten_data (bool): Whether to flatten the input images into vectors (True for fully connected networks).\n",
        "\n",
        "    Returns:\n",
        "        (float, float): A tuple (loss_ratio, correct_ratio) representing the average loss\n",
        "                        per example and overall accuracy after one epoch of training.\n",
        "    \"\"\"\n",
        "    network.train()  # Set the model to training mode\n",
        "\n",
        "    # Initialize accumulators for loss and accuracy calculations\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_items = 0\n",
        "\n",
        "    # TODO: Create scheduler\n",
        "\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # Transfer the data and target to the specified device (GPU/CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Flatten the data if using fully connected layers\n",
        "        if flatten_data:\n",
        "            data = flatten(data)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        output = network(data)\n",
        "\n",
        "        # Compute batch loss (CrossEntropy is well-suited for multi-class classification)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Reset gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagation: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Parameter update step\n",
        "        optimizer.step()\n",
        "\n",
        "        # TODO: Scheduler step\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss += loss\n",
        "        total_correct += torch.sum(torch.argmax(output, dim=1) == target)\n",
        "        total_items += len(target)\n",
        "\n",
        "        # Calculate average loss and accuracy so far\n",
        "        loss_ratio = (total_loss / total_items).item()\n",
        "        correct_ratio = (total_correct / total_items).item()\n",
        "\n",
        "        # Note: .item() converts a scalar tensor to a Python float (and moves it to CPU if on GPU)\n",
        "\n",
        "    return loss_ratio, correct_ratio\n"
      ],
      "metadata": {
        "id": "1DYvhXqez9mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record convergence time\n",
        "%%time\n",
        "\n",
        "plotlosses = PlotLosses()\n",
        "\n",
        "learning_rate = 0.01 # TODO: Change me!\n",
        "n_epochs = 50\n",
        "fc_network = FullyConnectedNetwork(hidden_layers = 3, hidden_nodes = 250)\n",
        "fc_network.to(device)\n",
        "\n",
        "# Add momentum term\n",
        "momentum = 0.5 # TODO: Change me!\n",
        "\n",
        "# TODO: Try out different optimizers {SGD, Adam, RMSprop, Adadelta}\n",
        "fc_optimizer = optim.SGD(fc_network.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  train_loss, train_acc = trainer_schedule(fc_network, fc_optimizer, device)\n",
        "  val_loss, val_acc = tester(fc_network, val_loader, device)\n",
        "\n",
        "  plotlosses.update({'acc': train_acc, 'val_acc': val_acc, 'loss': train_loss, 'val_loss': val_loss})\n",
        "  plotlosses.draw()  # draw, update logs, etc\n"
      ],
      "metadata": {
        "id": "Y-ueKu752clY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = tester(fc_network, test_loader, device)\n",
        "print(\"Test loss = {loss}, Test Accuracy = {acc}\".format(loss=round(test_loss, 4), acc=round(test_acc, 4)))"
      ],
      "metadata": {
        "id": "JDk4HEzHAENt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}